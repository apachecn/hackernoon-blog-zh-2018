<html>
<head>
<title>dThe 3 Tricks That Made AlphaGo Zero Work</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让AlphaGo Zero成功的3个诀窍</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef?source=collection_archive---------1-----------------------#2018-01-07">https://medium.com/hackernoon/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef?source=collection_archive---------1-----------------------#2018-01-07</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="1bc1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">2017年，<a class="ae jp" href="https://hackernoon.com/tagged/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>和人工智能取得了许多进展<a class="ae jp" href="http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/" rel="noopener ugc nofollow" target="_blank">，但很少有人像DeepMind的</a><a class="ae jp" href="https://deepmind.com/blog/alphago-zero-learning-scratch/" rel="noopener ugc nofollow" target="_blank"> AlphaGo Zero </a>一样引起如此多的关注和兴趣。这个程序确实是一个令人震惊的突破:它不仅以100比0击败了AlphaGo的早期版本——该程序仅在一年半前击败了17次世界冠军Lee Sedol，而且它在没有任何真实人类游戏数据的情况下进行了训练。Xavier Amatrain 在《机器学习<a class="ae jp" href="https://hackernoon.com/tagged/machine-learning" rel="noopener ugc nofollow" target="_blank"/>中称之为<a class="ae jp" href="https://www.quora.com/What-is-the-significance-of-AlphaGo-Zero-in-AI-research/answer/Xavier-Amatriain" rel="noopener ugc nofollow" target="_blank">“比过去5年中的任何事情都更加【重要】”。</a></p><p id="dbce" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">那么DeepMind是怎么做到的呢？在这篇文章中，我将尝试给出AlphaGo Zero使用的技术的直观想法，是什么使它们工作，以及对未来人工智能研究的影响。先说AlphaGo和AlphaGo Zero下围棋的一般方法。</p><h1 id="4466" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">DeepMind的一般方法</h1><p id="8b75" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">AlphaGo和AlphaGo Zero都评估了围棋棋盘，并结合使用了两种方法来选择走法:</p><ol class=""><li id="c3ce" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated">执行<em class="lc"> </em> " <strong class="it hv">前瞻</strong>"搜索:通过模拟游戏来前瞻几个走法，从而看到当前的哪一个走法最有可能导致将来的“好”位置。</li><li id="068b" class="kt ku hu it b iu ld iy le jc lf jg lg jk lh jo ky kz la lb dt translated">根据“直觉”<strong class="it hv"/>评估头寸，判断头寸是“好”还是“坏”——也就是说，可能会导致盈利或亏损。</li></ol><p id="cc11" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">AlphaGo和AlphaGo Zero都是通过巧妙地结合这两种方法来工作的。让我们依次看看每一项:</p><h2 id="9535" class="li jr hu bd js lj lk ll jw lm ln lo ka jc lp lq ke jg lr ls ki jk lt lu km lv dt translated">go-play方法#1:“前瞻”</h2><p id="cb3e" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">围棋是一个足够复杂的游戏，计算机不能简单地使用蛮力方法搜索所有可能的走法来找到最佳走法(事实上，<a class="ae jp" href="https://en.wikipedia.org/wiki/Go_and_mathematics" rel="noopener ugc nofollow" target="_blank">它们甚至不能接近</a>)。</p><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="fe ff lw"><img src="../Images/bc4d68bd678e29dc9e3fcb8b9ce9e031.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qpzAxoUR9POLYl__zJhU5g.png"/></div></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">The tree of possible positions in Go. <a class="ae jp" href="https://www.quora.com/What-does-it-mean-that-AlphaGo-relied-on-Monte-Carlo-tree-search/answer/Kostis-Gourgoulias" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="0e38" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">AlphaGo之前最好的围棋程序通过使用“<a class="ae jp" href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search" rel="noopener ugc nofollow" target="_blank">蒙特卡罗树搜索</a>或MCTS克服了这个问题。在高层次上，这种方法包括最初探索棋盘上许多可能的棋步，然后随着时间的推移专注于这种探索，因为某些棋步被发现比其他棋步更有可能导致胜利。</p><p id="3d88" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">AlphaGo和AlphaGo Zero都使用相对简单的MCTS版本进行“前瞻”，只是使用了在<a class="ae jp" href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search" rel="noopener ugc nofollow" target="_blank">蒙特卡罗树搜索维基百科页面</a>中列出的许多最佳实践，以正确管理探索新的移动序列或更深入地探索已经探索过的序列之间的权衡(有关更多信息，请参见发表在自然杂志上的AlphaGo原始论文<a class="ae jp" href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank">中“方法”下的“搜索”部分)。</a></p><p id="a0dc" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">尽管在AlphaGo之前，MCTS一直是所有成功围棋程序的核心，但DeepMind将这种技术与基于神经网络的“直觉”巧妙结合，使其超越了人类的表现。</p><h2 id="788e" class="li jr hu bd js lj lk ll jw lm ln lo ka jc lp lq ke jg lr ls ki jk lt lu km lv dt translated">围棋打法<em class="mm"> #2:“直觉”</em></h2><p id="28c1" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">DeepMind与AlphaGo的重大创新是使用深度神经网络来理解游戏的状态，然后使用这种理解来智能地指导MCTS的搜索。更具体地说:他们训练网络来观察</p><ol class=""><li id="8e13" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated">当前的董事会职位</li><li id="a3f5" class="kt ku hu it b iu ld iy le jc lf jg lg jk lh jo ky kz la lb dt translated">哪个玩家在玩，</li><li id="3b89" class="kt ku hu it b iu ld iy le jc lf jg lg jk lh jo ky kz la lb dt translated">最近移动的顺序(排除某些非法移动的必要条件)</li></ol><p id="f1e1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">给定这些信息，神经网络可以推荐:</p><ol class=""><li id="357b" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated">应该下哪一步棋</li><li id="8590" class="kt ku hu it b iu ld iy le jc lf jg lg jk lh jo ky kz la lb dt translated">当前玩家是否有可能赢。</li></ol><p id="ae35" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">DeepMind是如何训练神经网络做到这一点的？在这里，AlphaGo和AlphaGo Zero使用了非常不同的方法；我们先从AlphaGo的开始:</p><h2 id="ce95" class="li jr hu bd js lj lk ll jw lm ln lo ka jc lp lq ke jg lr ls ki jk lt lu km lv dt translated"><strong class="ak"> AlphaGo的“直觉”:政策网和价值网</strong></h2><p id="c39f" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">AlphaGo有两个单独训练的神经网络。</p><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div class="fe ff mn"><img src="../Images/b27321156dece145478e86afe3c52622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*rnmOlqtr_bTSpP6i7GuJ4w.png"/></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">The two neural networks at the core of AlphaGo. <a class="ae jp" href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><ol class=""><li id="ccd9" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated">第一个神经网络(随机初始化)被训练为模拟人类专家的游戏，向其展示来自大型真实游戏数据库的3000万个动作。解决这个问题是一个困难但简单明了的模式识别问题，深度神经网络在这方面更胜一筹。事实上，一旦经过训练，这个网络确实学会了推荐类似于它观察人类专家在游戏中玩的那些动作。</li><li id="b23c" class="kt ku hu it b iu ld iy le jc lf jg lg jk lh jo ky kz la lb dt translated">DeepMind不仅希望AlphaGo模仿人类棋手:他们还希望它赢。为了学会玩更有可能导致胜利而不是失败的游戏，网络——被训练得像人类专家一样玩——与自己玩游戏。然后从这些“自我游戏”中随机抽取动作样本；如果一个给定的移动发生在一场游戏中，当前玩家最终获胜，网络被训练成更有可能在未来进行类似的移动，反之亦然。</li></ol><p id="515d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">DeepMind然后以一种非常聪明的方式将这两个神经网络与MCTS结合起来——即程序的“直觉”与其强力“前瞻”搜索相结合:它使用经过训练的网络来<em class="lc">预测移动</em>到<em class="lc">引导博弈树的哪个分支</em>进行搜索，并使用经过训练的网络来<em class="lc">预测某个位置是否“获胜”</em>到<em class="lc">评估</em>它在搜索过程中遇到的位置。这使得AlphaGo能够智能地搜索即将到来的移动，并最终击败Lee Sedol。</p><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="fe ff mo"><img src="../Images/9cac2ea6ce9c10e5882202575f7a49b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s2kMOSdl2AaUwo5QVjpTHA.png"/></div></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">Monte Carlo Tree Search in AlphaGo, guided by neural networks. <a class="ae jp" href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="b3ef" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然而，AlphaGo Zero将这一点提升到了一个全新的水平。</p><h1 id="a3dc" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">让AlphaGo Zero成功的三招</h1><p id="6938" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">在高层次上，AlphaGo Zero的工作方式与AlphaGo相同:具体来说，它通过使用基于MCTS的前瞻搜索来玩围棋，并由神经网络智能指导。</p><p id="583b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然而，AlphaGo Zero的神经网络——它的“直觉”——与AlphaGo的训练完全不同:</p><h2 id="5a8d" class="li jr hu bd js lj lk ll jw lm ln lo ka jc lp lq ke jg lr ls ki jk lt lu km lv dt translated">技巧1:如何训练你的AlphaGo Zero，第1部分</h2><p id="3f79" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">假设你有一个试图“理解”围棋的神经网络:也就是说，对于每一个棋盘位置，它都使用一个深度神经网络来生成对最佳走法的评估。DeepMind意识到的是，无论这个神经网络有多智能——无论它是完全无知还是围棋高手— <em class="lc">它的评估总是可以由MCTS </em>做得更好。</p><p id="b0d2" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">从根本上说，MCTS执行的是一种前瞻搜索，如果给它足够的时间，我们可以想象一个人类大师会执行这种搜索:它智能地猜测哪些变化——未来移动的序列——最有希望，模拟这些变化，评估它们<em class="lc">实际上</em>有多好，并相应地更新其对当前<em class="lc">最佳移动的评估。</em></p><p id="a5f3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">下面是一个例子。假设我们有一个神经网络，它正在读取棋盘，并确定给定的移动会导致游戏变得公平，评估值为0.0。然后，网络智能地预测一些移动，并找到可以从当前位置强制的移动序列，最终导致0.5的评估。然后，它可以更新其对<em class="lc">当前</em>板位置的评估，以反映它会导致更有利的位置。</p><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="fe ff mp"><img src="../Images/b0f48d986c35ce1598197245dffd53c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hBzorPuADtitET2SZaLN2A.png"/></div></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">How MCTS can always continually improve programs’ evaluations.</figcaption></figure><p id="7de4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，这种前瞻搜索总是能够给我们提供关于神经网络正在评估的<em class="lc">当前</em>位置中的各种移动有多好的改进数据。无论我们的神经网络是处于业余水平还是专家水平，这都是真实的:我们可以通过向前看，看看它当前的哪些选项实际上会导致更好的位置，来<em class="lc">总是</em>为它生成改进的评估。</p><h2 id="5fe8" class="li jr hu bd js lj lk ll jw lm ln lo ka jc lp lq ke jg lr ls ki jk lt lu km lv dt translated">技巧1(续):如何训练你的AlphaGo Zero，第2部分</h2><p id="b31d" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">此外，就像在AlphaGo中一样，我们也希望我们的神经网络能够学习哪些步骤可能会导致胜利。因此，也像以前一样，我们的代理人——利用其MCTS改进的评估和其神经网络的当前状态——可以与自己玩游戏，赢一些输一些。</p><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="fe ff mq"><img src="../Images/40051b5c956378b2769609e4af5b8051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DB99saQWkvVwPleKaWj-1A.png"/></div></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">Self-play in AlphaGo Zero. Diagram courtesy of <a class="ae jp" href="https://deepmind.com" rel="noopener ugc nofollow" target="_blank">DeepMind</a>.</figcaption></figure><p id="f2d9" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这些数据完全通过前瞻和自玩生成，是DeepMind用来训练AlphaGo Zero的。更具体地说:</p><ol class=""><li id="f5e3" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated">神经网络被训练来玩反映执行“前瞻”搜索的改进评估的动作。</li><li id="8f03" class="kt ku hu it b iu ld iy le jc lf jg lg jk lh jo ky kz la lb dt translated">对神经网络进行了调整，以便它更有可能在自我游戏中采取类似于导致胜利的行动，而不太可能采取类似于导致失败的行动。</li></ol><p id="51a9" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">很大程度上是因为没有人与人之间的游戏被用来训练AlphaGo Zero，而这第一个“技巧”就是为什么:对于围棋智能体的给定状态，通过执行基于MCTS的前瞻，并使用前瞻的结果来改进智能体，它总是可以变得更聪明。这就是AlphaGo Zero能够不断改进的原因，从它还是一个业余选手一直到它比最好的人类棋手都强。</p><p id="056a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">第二个技巧是一个新颖的神经网络结构，我称之为“双头怪物”。</p><h2 id="7cf4" class="li jr hu bd js lj lk ll jw lm ln lo ka jc lp lq ke jg lr ls ki jk lt lu km lv dt translated"><strong class="ak">第二招:双头怪</strong></h2><p id="4645" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">AlphaGo Zero的是它的神经网络架构，一种“双头”架构。它的前20层左右是现代神经网络结构中常见的层“块”。这些层之后是<strong class="it hv"> <em class="lc">两个“头”</em> </strong>:一个头获取前20层的输出，并产生围棋智能体进行某些移动的概率，另一个头获取前20层的输出，并输出当前棋手获胜的概率。</p><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div class="fe ff mr"><img src="../Images/f4de79307360ad7470988ff8dc54ba76.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*96DnPFNDD8YyN-GK737bBQ.png"/></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">AlphaGo Zero’s two headed neural network architecture. Diagram courtesy of <a class="ae jp" href="https://deepmind.com" rel="noopener ugc nofollow" target="_blank">DeepMind</a>.</figcaption></figure><p id="bf76" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这很不寻常。在<a class="ae jp" href="https://towardsdatascience.com/the-5-deep-learning-breakthroughs-you-should-know-about-df27674ccdf2" rel="noopener" target="_blank">几乎所有的应用中，</a>神经网络输出一个单一的、固定的输出——比如一幅图像包含一只狗的概率，或者一个包含一幅图像包含10种对象之一的概率的向量。如果一个网接收到两组信号:一组是它对棋盘的评价有多好，另一组是它选择的特定走法有多好，它怎么能知道呢？</p><p id="5a17" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">答案很简单:记住神经网络从根本上来说只是数学函数，有一堆参数决定了它们所做的预测；我们通过反复向他们展示“正确答案”并让他们更新参数来“教”他们，这样他们产生的答案就更接近这些正确答案。</p><p id="9cd4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，当我们使用双头神经网络使用头#1进行预测时，我们只需更新导致进行该预测的参数，即“主体”和“头#1”中的参数。类似地，当我们使用Head #2进行预测时，我们更新“Body”和“Head #2”中的参数。</p><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="fe ff ms"><img src="../Images/9a58d11a736738a97bb4b9caf20a25cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kxeOANM2_4aqGXwGAXsusQ.png"/></div></div></figure><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="fe ff mp"><img src="../Images/d4024eb46e054ffc882f0e65bd34cee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bqt3g_0RlcAJNn6KEICNkQ.png"/></div></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">Training the two headed neural network, one head at a time.</figcaption></figure><p id="ef7e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这就是DeepMind如何训练它的<em class="lc">单“双头”神经网络</em>的，它用来在搜索过程中指导MCTS，就像AlphaGo使用两个独立的神经网络一样。这一招占了AlphaGo Zero比AlphaGo提升的对局实力的一半。</p><p id="ffdf" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">(这个技巧在技术上被称为具有硬参数共享的多任务学习。<a class="ae jp" href="http://ruder.io/multi-task/index.html#introduction" rel="noopener ugc nofollow" target="_blank"> Sebastian Ruder在这里有一个很好的概述</a>。</p><p id="0b5f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">游戏强度增加的另一半仅仅来自于使神经网络架构与该领域的最新进展保持同步:</p><h2 id="797b" class="li jr hu bd js lj lk ll jw lm ln lo ka jc lp lq ke jg lr ls ki jk lt lu km lv dt translated"><strong class="ak">绝招三:“残”网</strong></h2><p id="2f8a" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">AlphaGo Zero使用了比AlphaGo更“前沿”的神经网络架构。具体来说，他们使用了一种“残差”神经网络架构，而不是纯粹的“卷积”架构。剩余网络是微软研究院在2015年末首创的<a class="ae jp" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">，大约在AlphaGo第一版工作结束的时候，所以DeepMind没有在最初的AlphaGo程序中使用它们是可以理解的。</a></p><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="fe ff mt"><img src="../Images/287d975b0d07d6ab597840bb787e8397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aJCekYFA3jG0NDBmBEYYPA.png"/></div></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">Diagram comparing residual to convolutional architectures, from the original “ResNet” paper. <a class="ae jp" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="0ebb" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">有趣的是，如下图所示，这两种与神经网络相关的技巧中的每一种——从卷积架构切换到残差架构，以及使用“双头怪物”神经网络架构而不是单独的神经网络——都会导致游戏强度增加约一半，这是两者结合时实现的。</p><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="fe ff mu"><img src="../Images/f5eacb23d7dc15a87d5680a7856b9213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Yl6HAo-3YVwdbKpSVZY_A.png"/></div></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">Improvmenet of AlphaGo Zero — which used a “Dual-Residual” neural network architecture — over AlphaGo, which used a “Separate-Convolutional” architecture. Chart courtesy of <a class="ae jp" href="https://deepmind.com" rel="noopener ugc nofollow" target="_blank">DeepMind</a>.</figcaption></figure><h2 id="68df" class="li jr hu bd js lj lk ll jw lm ln lo ka jc lp lq ke jg lr ls ki jk lt lu km lv dt translated"><strong class="ak">招数总结</strong></h2><p id="dc2c" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">这三个技巧使AlphaGo Zero实现了令人难以置信的性能，甚至让AlphaGo都望尘莫及:</p><ol class=""><li id="6763" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated">使用蒙特卡罗树搜索提供的评估——“智能前瞻”——来不断改进神经网络对棋盘位置的评估，而不是使用人类游戏。</li><li id="16bc" class="kt ku hu it b iu ld iy le jc lf jg lg jk lh jo ky kz la lb dt translated">使用一个神经网络——同时学习两个动作的“双头怪物”“智能前瞻”将推荐<em class="lc">和</em>哪些动作可能导致胜利——而不是两个独立的神经网络。</li><li id="6c78" class="kt ku hu it b iu ld iy le jc lf jg lg jk lh jo ky kz la lb dt translated">使用更先进的神经网络架构——“残差”架构，而不是“卷积”架构。</li></ol><h2 id="78d0" class="li jr hu bd js lj lk ll jw lm ln lo ka jc lp lq ke jg lr ls ki jk lt lu km lv dt translated"><strong class="ak">一条评论</strong></h2><p id="17ac" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">值得注意的是，AlphaGo没有使用任何经典甚至是“前沿”的强化学习概念——没有深度Q学习、异步行动者-批评家代理或任何其他我们通常与强化学习联系在一起的东西。它只是使用模拟来为其神经网络生成训练数据，然后在监督下进行学习。<a class="ae jp" href="https://twitter.com/dennybritz" rel="noopener ugc nofollow" target="_blank"> Denny Britz </a>在AlphaGo Zero论文发布后不久的这条推文中很好地总结了这个想法:</p><figure class="lx ly lz ma fq mb"><div class="bz el l di"><div class="mv mw l"/></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">Follow Denny <a class="ae jp" href="https://twitter.com/dennybritz" rel="noopener ugc nofollow" target="_blank">here</a>.</figcaption></figure><h2 id="cf81" class="li jr hu bd js lj lk ll jw lm ln lo ka jc lp lq ke jg lr ls ki jk lt lu km lv dt translated"><strong class="ak">数字:训练AlphaGo零分，循序渐进</strong></h2><p id="5eb9" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">以下是AlphaGo Zero如何训练的“一步一步”时间表:</p><ol class=""><li id="aea2" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated">初始化神经网络。</li><li id="5316" class="kt ku hu it b iu ld iy le jc lf jg lg jk lh jo ky kz la lb dt translated">玩自我游戏，每一步使用<strong class="it hv">1600</strong>MCTS模拟(耗时约0.4秒)。</li></ol><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="fe ff mx"><img src="../Images/cdc48082ddc6302f99b7c83496250f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IRbL8abD_fN4tafR6cfMlg.png"/></div></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">One MCTS iteration in AlphaGo Zero. Diagram courtesy of <a class="ae jp" href="https://deepmind.com" rel="noopener ugc nofollow" target="_blank">DeepMind</a>.</figcaption></figure><p id="69ca" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">3.随着这些自玩游戏的进行，从最近的<strong class="it hv"> 500，000 </strong>游戏中取样<strong class="it hv"> 2，048 </strong> <em class="lc">位置</em>，以及游戏是赢还是输。对于每一步棋，记录A)MCTS对这些位置的评估结果——这些位置的各种棋步基于前瞻的“好”程度——以及B)当前玩家是赢了还是输了这场游戏。</p><p id="f820" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">4.使用A)由MCTS前瞻搜索产生的移动评估和B)当前玩家是赢还是输来训练神经网络。</p><p id="1604" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">5.最后，每重复1000次步骤3-4，对照以前的最佳版本评估当前的神经网络；如果它赢得了至少55%的游戏，开始使用<em class="lc"> it </em>来生成自己玩的游戏，而不是以前的版本。</p><p id="9756" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">重复步骤3-4<strong class="it hv">700，000 </strong>次，同时继续玩自我游戏——三天后，您将拥有自己的AlphaGo Zero！</p><h1 id="b259" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated"><strong class="ak">对AI其余部分的影响</strong></h1><p id="463d" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">DeepMind令人难以置信的成就对人工智能研究的未来有许多影响。以下是几个关键的例子:</p><p id="01b5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">首先，从模拟中生成的自我游戏数据“足够好”，能够训练网络，这一事实表明<strong class="it hv">模拟的自我游戏数据<em class="lc">可以</em>训练代理在极其复杂的任务中超越人类的表现，甚至完全从零开始</strong>——从人类专家那里生成的数据可能并不需要。</p><p id="b1de" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">第二，<strong class="it hv">“双头怪物”技巧似乎极大地帮助代理学习在许多领域执行几个相关的任务</strong>，因为它似乎可以防止代理将其行为过度适应于任何单独的任务。DeepMind似乎真的很喜欢这个把戏，并使用它和它的更高级版本来构建代理，这些代理可以在<a class="ae jp" href="https://arxiv.org/pdf/1707.04175.pdf" rel="noopener ugc nofollow" target="_blank">几个</a> <a class="ae jp" href="https://arxiv.org/pdf/1701.08734.pdf" rel="noopener ugc nofollow" target="_blank">不同的</a> <a class="ae jp" href="https://arxiv.org/pdf/1708.07860.pdf" rel="noopener ugc nofollow" target="_blank">域</a>中学习多项任务。</p><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="fe ff my"><img src="../Images/cb59da79d15e99aa6b19e01757c221fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZFqEYHfcP-8mLAYi2wz8gw.png"/></div></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">DeepMind’s AI learning to solve mazes, using the “Distral” framework for multitask reinforcement learning. <a class="ae jp" href="https://deepmind.com/research/publications/distral-robust-multitask-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="3f9c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">机器人领域的许多项目，特别是新兴的使用模拟来教机器人代理使用他们的肢体来完成任务的领域，正在使用这两个技巧取得巨大的效果。<a class="ae jp" href="https://www.youtube.com/watch?v=TyOooJC_bLY" rel="noopener ugc nofollow" target="_blank"> Pieter Abbeel最近的NIPS主题演讲</a>强调了许多令人印象深刻的新成果，这些成果使用了这些技巧<em class="lc">以及</em>许多前沿强化学习技术。事实上，运动似乎是“双头怪物”技巧的一个完美用例:例如，机器人代理可以同时被训练使用球棒击球和击打移动目标，因为这两项任务需要学习一些共同的技能(例如，平衡、躯干旋转)。</p><figure class="lx ly lz ma fq mb fe ff paragraph-image"><div class="fe ff mz"><img src="../Images/f8ac53e2983d473b4399b2e353cf3408.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*la16q_VbyN_l3iejXSJ3og.jpeg"/></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">The tricks DeepMind used to train AlphaGo Zero have already been applied to locomotion. <a class="ae jp" href="https://www.youtube.com/watch?v=hx_bgoTF7bs" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="5967" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">DeepMind的AlphaGo Zero是2017年人工智能和深度学习领域最有趣的进步之一。我迫不及待地想看看2018年带来了什么！</p><figure class="lx ly lz ma fq mb"><div class="bz el l di"><div class="na mw l"/></div></figure></div></div>    
</body>
</html>