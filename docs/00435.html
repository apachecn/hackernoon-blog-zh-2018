<html>
<head>
<title>Linear Regression in 2 Minutes (using PyTorch)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2分钟内线性回归(使用PyTorch)</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/linear-regression-in-x-minutes-using-pytorch-8eec49f6a0e2?source=collection_archive---------5-----------------------#2018-01-14">https://medium.com/hackernoon/linear-regression-in-x-minutes-using-pytorch-8eec49f6a0e2?source=collection_archive---------5-----------------------#2018-01-14</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff hs"><img src="../Images/de89fe61c263da88452679d96b39d6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jcZLpgh3gppeFFgcpFSP0w.jpeg"/></div></div></figure><div class=""/><blockquote class="jc jd je"><p id="c7d4" class="jf jg jh ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><a class="ae ke" href="https://github.com/init27Lab/DL-Toolkit" rel="noopener ugc nofollow" target="_blank"> <em class="if">你可以在这个Github回购</em> </a>中找到所有附带的代码</p></blockquote><p id="62b1" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">这是PyTorch <a class="ae ke" rel="noopener" href="/init27-labs/pytorch-primer-series-0-e2e5df9b31c6?source=collection_home---4------1----------------">初级系列</a>的第二部分。</p><p id="4b6f" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated"><strong class="ji ig">线性回归</strong>是建模输入和预测之间关系的线性方法</p><figure class="kj kk kl km fq hw fe ff paragraph-image"><div class="fe ff ki"><img src="../Images/c566145379762972f3ba6c12eaac612f.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*eLfDr8eXnYKY-VELQXmnSg.png"/></div><figcaption class="kn ko fg fe ff kp kq bd b be z ek">Source: Wikipedia</figcaption></figure><p id="f46e" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">我们找到了数据的“线性拟合”。</p><p id="9e2a" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">拟合:我们试图通过拟合数据的曲线来预测变量y。线性回归中的曲线遵循标量(x)和<a class="ae ke" href="https://hackernoon.com/tagged/dependent" rel="noopener ugc nofollow" target="_blank">因变量</a>之间的线性关系。</p><h1 id="adb0" class="kr ks if bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo dt translated"><strong class="ak">在PyTorch中创建模型</strong></h1><ol class=""><li id="f8b4" class="lp lq if ji b jj lr jn ls kf lt kg lu kh lv kd lw lx ly lz dt translated">创建一个类</li><li id="5cb3" class="lp lq if ji b jj ma jn mb kf mc kg md kh me kd lw lx ly lz dt translated">宣布你的向前传球</li><li id="56eb" class="lp lq if ji b jj ma jn mb kf mc kg md kh me kd lw lx ly lz dt translated">调整超参数</li></ol><pre class="kj kk kl km fq mf mg mh mi aw mj dt"><span id="4400" class="mk ks if mg b fv ml mm l mn mo"><strong class="mg ig">class</strong> <strong class="mg ig">LinearRegressionModel</strong>(nn.Module):<br/><br/>    <strong class="mg ig">def</strong> __init__(self, input_dim, output_dim):<br/><br/>        super(LinearRegressionModel, self).__init__() <br/>        <em class="jh"># Calling Super Class's constructor</em><br/>        self.linear = nn.Linear(input_dim, output_dim)<br/>        <em class="jh"># nn.linear is defined in nn.Module</em><br/><br/>    <strong class="mg ig">def</strong> forward(self, x):<br/>        <em class="jh"># Here the forward pass is simply a linear function</em><br/><br/>        out = self.linear(x)<br/>        <strong class="mg ig">return</strong> out<br/><br/>input_dim = 1<br/>output_dim = 1</span></pre><h2 id="b73f" class="mk ks if bd kt mp mq mr kx ms mt mu lb kf mv mw lf kg mx my lj kh mz na ln nb dt translated">步伐</h2><ol class=""><li id="2327" class="lp lq if ji b jj lr jn ls kf lt kg lu kh lv kd lw lx ly lz dt translated">创建模型的实例</li><li id="5dd9" class="lp lq if ji b jj ma jn mb kf mc kg md kh me kd lw lx ly lz dt translated">选择损失标准</li><li id="f21b" class="lp lq if ji b jj ma jn mb kf mc kg md kh me kd lw lx ly lz dt translated">选择超级参数</li></ol><pre class="kj kk kl km fq mf mg mh mi aw mj dt"><span id="2987" class="mk ks if mg b fv ml mm l mn mo">model = LinearRegressionModel(input_dim,output_dim)<br/><br/>criterion = nn.MSELoss()<em class="jh"># Mean Squared Loss</em><br/>l_rate = 0.01<br/>optimiser = torch.optim.SGD(model.parameters(), lr = l_rate) <em class="jh">#Stochastic Gradient Descent</em><br/><br/>epochs = 2000</span></pre><p id="c5f9" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated"><strong class="ji ig">训练模型</strong></p><pre class="kj kk kl km fq mf mg mh mi aw mj dt"><span id="2a4a" class="mk ks if mg b fv ml mm l mn mo"><strong class="mg ig">for</strong> epoch <strong class="mg ig">in</strong> range(epochs):<br/><br/>    epoch +=1<br/>    #increase the number of epochs by 1 every time</span><span id="8154" class="mk ks if mg b fv nc mm l mn mo">    inputs = Variable(torch.from_numpy(x_train))<br/>    labels = Variable(torch.from_numpy(y_correct))<br/><br/>    <em class="jh">#clear grads as discussed in prev post</em></span><span id="d4ff" class="mk ks if mg b fv nc mm l mn mo">    optimiser.zero_grad()</span><span id="08ee" class="mk ks if mg b fv nc mm l mn mo">    <em class="jh">#forward to get predicted values</em></span><span id="e100" class="mk ks if mg b fv nc mm l mn mo">    outputs = model.forward(inputs)<br/>    loss = criterion(outputs, labels)<br/>    loss.backward()<em class="jh"># back props</em><br/>    optimiser.step()<em class="jh"># update the parameters</em><br/>    print('epoch <strong class="mg ig">{}</strong>, loss <strong class="mg ig">{}</strong>'.format(epoch,loss.data[0]))</span></pre><p id="9362" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated"><strong class="ji ig">最后，打印预测值</strong></p><pre class="kj kk kl km fq mf mg mh mi aw mj dt"><span id="d44c" class="mk ks if mg b fv ml mm l mn mo">predicted =model.forward(Variable(torch.from_numpy(x_train))).data.numpy()<br/><br/>plt.plot(x_train, y_correct, 'go', label = 'from data', alpha = .5)<br/>plt.plot(x_train, predicted, label = 'prediction', alpha = 0.5)<br/>plt.legend()<br/>plt.show()<br/>print(model.state_dict())</span></pre><p id="b229" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated"><a class="ae ke" rel="noopener" href="/@init_27/a-self-driving-new-year-2-d1bbc5a83570?source=user_profile---------2----------------">如果你想阅读《我的自驾游》第二周的内容，这里有一篇博文</a></p><p id="cef0" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">本系列的下一部分将讨论线性回归。</p><blockquote class="jc jd je"><p id="13b4" class="jf jg jh ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><a class="ae ke" href="http://twitter.com/bhutanisanyam1" rel="noopener ugc nofollow" target="_blank">你可以在推特@bhutanisanyam1 </a>上找到我，在<a class="ae ke" href="https://www.linkedin.com/in/sanyambhutani/" rel="noopener ugc nofollow" target="_blank"> Linkedin上联系我</a></p><p id="57cd" class="jf jg jh ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><a class="ae ke" href="http://tinyletter.com/sanyambhutani/" rel="noopener ugc nofollow" target="_blank">订阅我的时事通讯，获取深度学习和计算机视觉阅读的每周精选列表</a></p></blockquote><figure class="kj kk kl km fq hw"><div class="bz el l di"><div class="nd ne l"/></div></figure></div></div>    
</body>
</html>