<html>
<head>
<title>PyTorch Basics in 4 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch基础4分钟</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/pytorch-basics-9c1c627cd0d2?source=collection_archive---------6-----------------------#2018-01-14">https://medium.com/hackernoon/pytorch-basics-9c1c627cd0d2?source=collection_archive---------6-----------------------#2018-01-14</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff hs"><img src="../Images/de89fe61c263da88452679d96b39d6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jcZLpgh3gppeFFgcpFSP0w.jpeg"/></div></div></figure><div class=""/><blockquote class="jc jd je"><p id="82d7" class="jf jg jh ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><a class="ae ke" href="https://github.com/init27Lab/DL-Toolkit" rel="noopener ugc nofollow" target="_blank">你可以在这个Github repo </a>中找到所有附带的代码</p></blockquote><p id="7854" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">这是PyTorch <a class="ae ke" rel="noopener" href="/init27-labs/pytorch-primer-series-0-e2e5df9b31c6?source=collection_home---4------1----------------">初级系列</a>的第一部分。</p><p id="e1d4" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">讨论的主题:</p><ul class=""><li id="a359" class="ki kj if ji b jj jk jn jo kf kk kg kl kh km kd kn ko kp kq dt translated">张量</li><li id="178b" class="ki kj if ji b jj kr jn ks kf kt kg ku kh kv kd kn ko kp kq dt translated">基本操作。(内嵌、张量索引、切片)</li><li id="c8b9" class="ki kj if ji b jj kr jn ks kf kt kg ku kh kv kd kn ko kp kq dt translated">Numpy-PyTorch桥</li><li id="3e00" class="ki kj if ji b jj kr jn ks kf kt kg ku kh kv kd kn ko kp kq dt translated">皮托赫-努皮桥</li><li id="c2e9" class="ki kj if ji b jj kr jn ks kf kt kg ku kh kv kd kn ko kp kq dt translated">可变的</li><li id="c663" class="ki kj if ji b jj kr jn ks kf kt kg ku kh kv kd kn ko kp kq dt translated">梯度</li></ul><h1 id="4f11" class="kw kx if bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt dt translated">PyTorch是什么？</h1><p id="2a2b" class="pw-post-body-paragraph jf jg if ji b jj lu jl jm jn lv jp jq kf lw jt ju kg lx jx jy kh ly kb kc kd hn dt translated">这是一个基于<a class="ae ke" href="https://hackernoon.com/tagged/python" rel="noopener ugc nofollow" target="_blank"> Python </a>的包，用于替代Numpy，并作为深度<a class="ae ke" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>开发平台提供灵活性。</p><h1 id="4e8b" class="kw kx if bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt dt translated"><strong class="ak">为什么选择PyTorch？</strong></h1><p id="5200" class="pw-post-body-paragraph jf jg if ji b jj lu jl jm jn lv jp jq kf lw jt ju kg lx jx jy kh ly kb kc kd hn dt translated">我鼓励你去看看Fast AI的博客文章<a class="ae ke" href="http://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/" rel="noopener ugc nofollow" target="_blank">关于课程切换到PyTorch的原因。</a></p><p id="3c23" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">或者简单地说:</p><ul class=""><li id="8101" class="ki kj if ji b jj jk jn jo kf kk kg kl kh km kd kn ko kp kq dt translated">动态图表</li><li id="a761" class="ki kj if ji b jj kr jn ks kf kt kg ku kh kv kd kn ko kp kq dt translated">比TF(个人观点)更直观</li></ul><h1 id="c656" class="kw kx if bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt dt translated">张量</h1><p id="90a6" class="pw-post-body-paragraph jf jg if ji b jj lu jl jm jn lv jp jq kf lw jt ju kg lx jx jy kh ly kb kc kd hn dt translated">张量类似于numpy的ndarrays，另外，张量也可以用于GPU上以加速计算。</p><p id="c39c" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">张量是多维矩阵。</p><pre class="lz ma mb mc fq md me mf mg aw mh dt"><span id="77dd" class="mi kx if me b fv mj mk l ml mm">torch.Tensor(x, y)</span></pre><p id="6839" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">这将创建一个已经用随机值实例化的X乘Y维张量。</p><p id="dd71" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">为了创建具有从-1和1之间的均匀分布中随机选择的值的5×3张量，</p><pre class="lz ma mb mc fq md me mf mg aw mh dt"><span id="c3c3" class="mi kx if me b fv mj mk l ml mm">torch.Tensor(5, 3).uniform_(-1, 1)</span></pre><p id="31d7" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">张量有一个大小属性，可以调用它来检查它们的大小</p><pre class="lz ma mb mc fq md me mf mg aw mh dt"><span id="804c" class="mi kx if me b fv mj mk l ml mm">print(x.size())</span></pre><h1 id="5eff" class="kw kx if bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt dt translated">操作</h1><p id="3be6" class="pw-post-body-paragraph jf jg if ji b jj lu jl jm jn lv jp jq kf lw jt ju kg lx jx jy kh ly kb kc kd hn dt translated">PyTorch支持不同语法的各种张量函数:</p><p id="f5e4" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">考虑添加:</p><ul class=""><li id="ab2f" class="ki kj if ji b jj jk jn jo kf kk kg kl kh km kd kn ko kp kq dt translated">正常添加</li></ul><pre class="lz ma mb mc fq md me mf mg aw mh dt"><span id="e834" class="mi kx if me b fv mj mk l ml mm">y = torch.rand(5, 3)<br/>print(x + y)</span></pre><ul class=""><li id="9cc7" class="ki kj if ji b jj jk jn jo kf kk kg kl kh km kd kn ko kp kq dt translated">在张量中得到结果</li></ul><pre class="lz ma mb mc fq md me mf mg aw mh dt"><span id="3274" class="mi kx if me b fv mj mk l ml mm">result = torch.Tensor(5, 3)<br/>torch.add(x, y, out=result)</span></pre><ul class=""><li id="63d4" class="ki kj if ji b jj jk jn jo kf kk kg kl kh km kd kn ko kp kq dt translated">（与…）成一直线</li></ul><pre class="lz ma mb mc fq md me mf mg aw mh dt"><span id="0270" class="mi kx if me b fv mj mk l ml mm">y.add_(x)</span></pre><p id="2781" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">内联函数在其名称后用下划线表示。注意:它们有更快的执行时间(以更高的内存复杂度为代价)</p><p id="1394" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">支持所有数字索引、广播和整形功能</p><p id="01be" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">注意:PyTorch不支持负跳，所以[::-1]会导致错误</p><pre class="lz ma mb mc fq md me mf mg aw mh dt"><span id="16e1" class="mi kx if me b fv mj mk l ml mm">print(x[:, 1])</span><span id="4000" class="mi kx if me b fv mn mk l ml mm">y = torch.randn(5, 10, 15)<br/>print(y.size())<br/>print(y.view(-1, 15).size())</span></pre><h1 id="e558" class="kw kx if bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt dt translated"><strong class="ak">张量的类型</strong></h1><p id="d657" class="pw-post-body-paragraph jf jg if ji b jj lu jl jm jn lv jp jq kf lw jt ju kg lx jx jy kh ly kb kc kd hn dt translated">PyTorch支持各种类型的张量:</p><p id="ae0a" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">注意:在处理不同的张量类型时要小心，以避免类型错误</p><p id="22ae" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">支持的类型:</p><ul class=""><li id="c814" class="ki kj if ji b jj jk jn jo kf kk kg kl kh km kd kn ko kp kq dt translated">32位(浮点+整数)</li><li id="7a39" class="ki kj if ji b jj kr jn ks kf kt kg ku kh kv kd kn ko kp kq dt translated">64位(浮点+整数)</li><li id="4565" class="ki kj if ji b jj kr jn ks kf kt kg ku kh kv kd kn ko kp kq dt translated">16位(浮点+整数)</li><li id="3a31" class="ki kj if ji b jj kr jn ks kf kt kg ku kh kv kd kn ko kp kq dt translated">8位(有符号+无符号)</li></ul><h1 id="0883" class="kw kx if bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt dt translated">Numpy桥</h1><p id="5350" class="pw-post-body-paragraph jf jg if ji b jj lu jl jm jn lv jp jq kf lw jt ju kg lx jx jy kh ly kb kc kd hn dt translated">将torch张量转换为numpy数组，反之亦然。</p><p id="1c57" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">注意:torch张量和numpy数组将共享它们的底层内存位置，改变一个将改变另一个。</p><pre class="lz ma mb mc fq md me mf mg aw mh dt"><span id="7b48" class="mi kx if me b fv mj mk l ml mm">a = torch.ones(5)<br/>b = a.numpy()</span></pre><h1 id="a818" class="kw kx if bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt dt translated">CUDA Tensors</h1><p id="b22c" class="pw-post-body-paragraph jf jg if ji b jj lu jl jm jn lv jp jq kf lw jt ju kg lx jx jy kh ly kb kc kd hn dt translated">将张量移动到GPU可以通过以下方式完成:</p><pre class="lz ma mb mc fq md me mf mg aw mh dt"><span id="8bd1" class="mi kx if me b fv mj mk l ml mm"><strong class="me ig">if</strong> torch.cuda.is_available():<br/>    x = x.cuda()<br/>    y = y.cuda()<br/>    x + y</span></pre><h1 id="4583" class="kw kx if bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt dt translated">亲笔签名:自动微分</h1><figure class="lz ma mb mc fq hw fe ff paragraph-image"><div class="fe ff mo"><img src="../Images/af64c8a0e9e9e5836441a01b8eb39cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*COBTDf1oef8r7lbPxcxQcw.png"/></div></figure><p id="2f38" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">PyTorch中所有神经网络的核心是<code class="eh mp mq mr me b">autograd</code>包。让我们先简单地参观一下，然后我们将开始训练我们的第一个神经网络。</p><p id="1bf7" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated"><code class="eh mp mq mr me b">autograd</code>包为张量上的所有操作提供了自动微分。它是一个由运行定义的框架，这意味着您的反向传播是由您的代码如何运行来定义的，并且每一次迭代都可能是不同的。</p><p id="e10f" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">让我们用一些简单的例子来看这个问题。</p><h1 id="75cf" class="kw kx if bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt dt translated">可变的</h1><p id="ec64" class="pw-post-body-paragraph jf jg if ji b jj lu jl jm jn lv jp jq kf lw jt ju kg lx jx jy kh ly kb kc kd hn dt translated"><code class="eh mp mq mr me b">autograd.Variable</code>是包的中心类。它包装了一个张量，并支持几乎所有在张量上定义的操作。一旦你完成计算，你可以调用<code class="eh mp mq mr me b">.backward()</code>并自动计算所有的梯度。</p><p id="4fcd" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">你可以通过<code class="eh mp mq mr me b">.data</code>属性访问原始张量，而梯度w.r.t .这个变量被累积到<code class="eh mp mq mr me b">.grad</code>中。</p><figure class="lz ma mb mc fq hw fe ff paragraph-image"><div class="fe ff mo"><img src="../Images/9d15372afc636b1785dbeb347f8829c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/0*4NtOmdyorhdH9DGl.png"/></div><figcaption class="ms mt fg fe ff mu mv bd b be z ek">Source: PyTorch Docs</figcaption></figure><pre class="lz ma mb mc fq md me mf mg aw mh dt"><span id="7da0" class="mi kx if me b fv mj mk l ml mm">x_data = [1.0, 2.0, 3.0]<br/>y_data = [2.0, 4.0, 6.0]<br/><br/>w = Variable(torch.Tensor([1.0]),  requires_grad=<strong class="me ig">True</strong>)</span></pre><p id="72a5" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">调用后退函数</p><pre class="lz ma mb mc fq md me mf mg aw mh dt"><span id="df5b" class="mi kx if me b fv mj mk l ml mm">l = loss(x_val, y_val)<br/>l.backward()</span></pre><h1 id="6198" class="kw kx if bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt dt translated"><strong class="ak">常见陷阱</strong></h1><p id="97c1" class="pw-post-body-paragraph jf jg if ji b jj lu jl jm jn lv jp jq kf lw jt ju kg lx jx jy kh ly kb kc kd hn dt translated">正如我在快速人工智能社区的朋友和导师拉德克的博客文章所解释的</p><ul class=""><li id="ddf7" class="ki kj if ji b jj jk jn jo kf kk kg kl kh km kd kn ko kp kq dt translated">默认情况下，每次调用渐变时，渐变都会累积，请确保调用zero.gradient()来避免这种情况</li><li id="abc3" class="ki kj if ji b jj kr jn ks kf kt kg ku kh kv kd kn ko kp kq dt translated">数据类型，如张量部分所述，PyTorch支持各种张量类型。请务必检查类型，以避免类型兼容性错误。</li></ul><p id="0202" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">欢迎提问以下任何问题。<br/>也给我们留下你想看的教程的评论，我会尽快写出来。</p><p id="4dca" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated"><a class="ae ke" rel="noopener" href="/@init_27/a-self-driving-new-year-2-d1bbc5a83570?source=user_profile---------2----------------">如果你想阅读《我的自驾游》第二周的内容，这里有一篇博文</a></p><p id="cef0" class="pw-post-body-paragraph jf jg if ji b jj jk jl jm jn jo jp jq kf js jt ju kg jw jx jy kh ka kb kc kd hn dt translated">本系列的下一部分将讨论线性回归。</p><blockquote class="jc jd je"><p id="13b4" class="jf jg jh ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><a class="ae ke" href="http://twitter.com/bhutanisanyam1" rel="noopener ugc nofollow" target="_blank">你可以在Twitter @bhutanisanyam1 </a>上找到我，在<a class="ae ke" href="https://www.linkedin.com/in/sanyambhutani/" rel="noopener ugc nofollow" target="_blank"> Linkedin上联系我</a></p><p id="57cd" class="jf jg jh ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><a class="ae ke" href="http://tinyletter.com/sanyambhutani/" rel="noopener ugc nofollow" target="_blank">订阅我的时事通讯，获取深度学习和计算机视觉阅读的每周精选列表</a></p></blockquote><figure class="lz ma mb mc fq hw"><div class="bz el l di"><div class="mw mx l"/></div></figure></div></div>    
</body>
</html>