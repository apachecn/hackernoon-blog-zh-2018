<html>
<head>
<title>Supervised Machine Learning — Dimensional Reduction and Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督机器学习——降维与主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/supervised-machine-learning-dimensional-reduction-and-principal-component-analysis-614dec1f6b4c?source=collection_archive---------4-----------------------#2018-01-15">https://medium.com/hackernoon/supervised-machine-learning-dimensional-reduction-and-principal-component-analysis-614dec1f6b4c?source=collection_archive---------4-----------------------#2018-01-15</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><h2 id="1108" class="hs ht hu bd b gc hv hw hx hy hz ia ek ib translated" aria-label="kicker paragraph">第二部分</h2><div class=""/><figure class="fi fk jb jc jd je fe ff paragraph-image"><div role="button" tabindex="0" class="jf jg di jh bf ji"><div class="fe ff ja"><img src="../Images/36c972ca80edffa48367b6652f6f41ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j_9nchRe4TrHA_1jS1DeRA.jpeg"/></div></div><figcaption class="jl jm fg fe ff jn jo bd b be z ek">Special thanks to a dear friend of mine, Katharine, for inspiring this series. May the lessons learned in this series guide even the warmest of hearts.</figcaption></figure><p id="4259" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">本文是系列文章的一部分。 <a class="ae ko" href="https://hackernoon.com/supervised-machine-learning-linear-regression-in-python-541a5d8141ce" rel="noopener ugc nofollow" target="_blank"> <em class="kn">此处检出Part 1</em></a><em class="kn">。</em></p><h1 id="e456" class="kp kq hu bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm dt translated">介绍</h1><p id="33c8" class="pw-post-body-paragraph jp jq hu jr b js ln ju jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km hn dt translated"><em class="kn">本系列的动机是让任何对机器学习领域感兴趣的人都能够</em> <a class="ae ko" href="https://hackernoon.com/tagged/develop" rel="noopener ugc nofollow" target="_blank"> <em class="kn">开发</em> </a> <em class="kn">，理解并实现自己的机器学习算法。</em></p><p id="be60" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">在<a class="ae ko" href="https://hackernoon.com/tagged/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>问题中，每个训练实例通常涉及成千上万个特征。这可能是一个问题，因为它使我们的训练极其缓慢，容易出现<a class="ae ko" href="https://hackernoon.com/supervised-machine-learning-linear-regression-in-python-541a5d8141ce" rel="noopener ugc nofollow" target="_blank"><em class="kn"/></a><em class="kn">(参见“过拟合”部分)</em>。这个问题通常被称为<strong class="jr ie"/><em class="kn"/><strong class="jr ie">维度之祸</strong> <em class="kn">。</em></p><p id="c66c" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">由于与<strong class="jr ie"/><strong class="jr ie">维数灾难</strong>相关的问题，有必要大幅减少特征/维度的数量，以帮助提高我们模型的性能，并使我们能够为我们的机器学习模型达成最佳解决方案。幸运的是，在大多数现实生活问题中，通常可以减少训练集的维数，而不会损失太多数据中的方差。</p><p id="5e19" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">例如，我们的一些数据点在解释我们想要的目标变量时可能完全没有意义。因此，我们可能更愿意把它们从我们的分析中去掉。而且，经常是两个数据点可能高度相关；因此，通过将它们合并成一个数据点，您不会丢失太多信息。</p><blockquote class="ls"><p id="31ce" class="lt lu hu bd lv lw lx ly lz ma mb km ek translated">通过减少训练集的维度，我们可以提高训练的速度，并将数据集减少到二维或三维，从而更容易执行数据可视化(聚类、模式)。</p></blockquote><p id="ade8" class="pw-post-body-paragraph jp jq hu jr b js mc ju jv jw md jy jz ka me kc kd ke mf kg kh ki mg kk kl km hn dt translated">在本章中，我们将探讨与<strong class="jr ie"><em class="kn">维数灾难</em> </strong> <em class="kn">、</em>执行维数约简的各种方法(<strong class="jr ie"> <em class="kn">投影&amp;流形学习</em> </strong>)以及一种称为<strong class="jr ie"> <em class="kn">主成分分析</em> </strong>的维数约简算法。</p></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="075c" class="kp kq hu bd kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ms lk ll lm dt translated">维度的诅咒</h1><p id="cc42" class="pw-post-body-paragraph jp jq hu jr b js ln ju jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km hn dt translated">一个傻瓜曾经说过“救命救命！我好像看不到任何人！”智者回答说“因为你被困在了四维空间”，明白了吗？我也没有</p><figure class="mu mv mw mx fq je fe ff paragraph-image"><div role="button" tabindex="0" class="jf jg di jh bf ji"><div class="fe ff mt"><img src="../Images/924ffc3804e7980784bbf96c8b39bc66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2KGuS5cVr8hHHFj1b77d3g.png"/></div></div><figcaption class="jl jm fg fe ff jn jo bd b be z ek">Source: <a class="ae ko" href="https://simple.wikipedia.org/wiki/Dimension" rel="noopener ugc nofollow" target="_blank">Wikipedia</a></figcaption></figure><p id="1868" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">事实证明，在高维空间中，会出现各种在低维空间中不会出现的现象。</p><blockquote class="ls"><p id="5008" class="lt lu hu bd lv lw lx ly lz ma mb km ek translated">最值得注意的是，随着数据集维度的增加，空间的体积增长如此之快，以至于可用的数据变得极其稀疏。</p></blockquote><p id="393a" class="pw-post-body-paragraph jp jq hu jr b js mc ju jv jw md jy jz ka me kc kd ke mf kg kh ki mg kk kl km hn dt translated">正如<a class="ae ko" href="https://github.com/ageron/handson-ml" rel="noopener ugc nofollow" target="_blank">奥雷连·杰龙</a>所指出的，如果你要在一个单位空间(1 x 1平方)中选取一个随机点，它只有0.4%的机会位于距离边界不到0.001的位置(换句话说，随机点极不可能“在任何维度上都是极端的”)。但是在一个10000维的单位超立方体(一个1 x 1 x … x 1的立方体，有一万个1)中，这个概率大于99.9999%。</p><figure class="mu mv mw mx fq je fe ff paragraph-image"><div role="button" tabindex="0" class="jf jg di jh bf ji"><div class="fe ff my"><img src="../Images/02e777c4903b3d5e56d7e7e55ddb0cbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vZH5t0bgRN1FCTihTqdRbA.png"/></div></div><figcaption class="jl jm fg fe ff jn jo bd b be z ek">Source: <a class="ae ko" href="http://cleverowl.uk/2016/02/06/curse-of-dimensionality-explained/" rel="noopener ugc nofollow" target="_blank">Clever Owl</a></figcaption></figure><p id="1c40" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">为了进一步强调这个问题，想象一下，如果你在一个单位正方形中随机选取两个点，这两个点之间的平均距离大约是0.52。如果你被要求随机选择两个点，但是这次是在一个单位3D立方体中，平均距离大约是0.66。对于1，000，000维的超立方体，这个距离大约是408.25——这要归功于奥丽恩·杰龙<a class="ae ko" href="https://github.com/ageron/handson-ml/blob/master/08_dimensionality_reduction.ipynb" rel="noopener ugc nofollow" target="_blank">。因此，高维数据集通常存在极其稀疏的风险，这使得我们的模型容易过度拟合，并且比低维数据更不可靠。从而对我们的机器学习模型的性能产生负面影响。</a></p><blockquote class="ls"><p id="1622" class="lt lu hu bd lv lw lx ly lz ma mb km ek translated">简而言之，训练集的维度越多，过度拟合模型的风险就越大。</p></blockquote><pre class="mz na nb nc nd ne nf ng nh aw ni dt"><span id="d2a0" class="nj kq hu nf b fv nk nl l nm nn"><strong class="nf ie">Note:</strong> One solution to the problem of<em class="kn"> the curse of</em> <em class="kn">dimensionality</em> could be to increase the size of the training set such that it reaches a sufficient density of training instances. However, this solution is extremely impractical as the number of training instances required to reach a given density grows exponentially with the number of dimensions.</span></pre></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="99eb" class="kp kq hu bd kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ms lk ll lm dt translated">降维方法</h1><p id="dd51" class="pw-post-body-paragraph jp jq hu jr b js ln ju jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km hn dt translated">有两种主要的降维方法，<em class="kn">投影</em>和<em class="kn">流形学习</em>。我们将在下面深入探讨这两种方法。</p><h1 id="c7bb" class="kp kq hu bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm dt translated">推断</h1><p id="0b0c" class="pw-post-body-paragraph jp jq hu jr b js ln ju jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km hn dt translated"><em class="kn">抓起一张纸，放在两个物体之间。就是这样！你学会投影了！</em></p><p id="5487" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">值得庆幸的是，在大多数现实生活问题中，我们的训练实例并没有在所有维度上统一分离。我们观察到的大多数特征都遵循某种模式，这通常导致我们的特征要么是恒定的，要么是高度相关的。因此，所有训练实例实际上都可以映射到高维度的低得多的维度子空间。以这个三维空间为例。</p><figure class="mu mv mw mx fq je fe ff paragraph-image"><div role="button" tabindex="0" class="jf jg di jh bf ji"><div class="fe ff no"><img src="../Images/3ffea07c02d3ef2f60f5686607f2bf85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*Wcn_sTqL05a7vHpmxf24Dw.png"/></div></div><figcaption class="jl jm fg fe ff jn jo bd b be z ek">Source: <a class="ae ko" href="https://github.com/ageron/handson-ml" rel="noopener ugc nofollow" target="_blank">handson-ml</a></figcaption></figure><p id="ec7b" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">正如您所看到的，几乎所有的训练实例都位于一个平面附近:这个平面是高维(3D)空间的低维(2D)子空间。现在，如果我们将<strong class="jr ie"> <em class="kn">项目</em> </strong> <em class="kn"> </em>的每个训练实例垂直投影到该子空间上(如从小线条所见)，我们可以得到新的2D数据集。就是这样！我们刚刚将一个高维(3D)空间缩减为一个较低的二维空间。轴的投影现在将对应于新特征，通常表示为<em class="kn"> z </em> 1和<em class="kn"> z </em> 2(平面投影的坐标)。</p><figure class="mu mv mw mx fq je fe ff paragraph-image"><div class="fe ff np"><img src="../Images/10bac4a3fc15eea401f0737e48b7146b.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*1dtvuEeb3lKyXd2fpIz6kw.png"/></div><figcaption class="jl jm fg fe ff jn jo bd b be z ek">Source: <a class="ae ko" href="https://github.com/ageron/handson-ml" rel="noopener ugc nofollow" target="_blank">handson-ml</a></figcaption></figure><pre class="mu mv mw mx fq ne nf ng nh aw ni dt"><span id="60a8" class="nj kq hu nf b fv nk nl l nm nn"><strong class="nf ie">Projection</strong></span><span id="3b85" class="nj kq hu nf b fv nq nl l nm nn"><strong class="nf ie">Original Dimensions</strong> --&gt; [x1, x2, x3] (<em class="kn">3-Dimensions</em>)</span><span id="a67b" class="nj kq hu nf b fv nq nl l nm nn"><strong class="nf ie">Projection Dimensions</strong> --&gt; [z1, z2] (<em class="kn">2-Dimensions</em>)</span><span id="f02b" class="nj kq hu nf b fv nq nl l nm nn"><strong class="nf ie">Note: </strong>This various depending on the dimensions of your data set. For our example, we have used a 3D data set and reduced the dimensions to 2D, as it is easier to visualise. However, if you have more dimensions in your training and after reduction, you have k-dimensions. Your coordinates would be denoted as (z1, z2...zn) for all n dimensions in k</span></pre><h2 id="28fc" class="nj kq hu bd kr nr ns nt kv nu nv nw kz ka nx ny ld ke nz oa lh ki ob oc ll ia dt translated">弱点</h2><p id="d1dc" class="pw-post-body-paragraph jp jq hu jr b js ln ju jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km hn dt translated">而投影法在进行降维时简单有效。如果我们在我们的维度子空间中有重叠的实例，简单地将我们的实例投射到一个超平面可能会导致重要信息的丢失。就拿这个从Sci-kit Learn引进的<a class="ae ko" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_swiss_roll.html" rel="noopener ugc nofollow" target="_blank">瑞士卷</a>来说吧。</p><figure class="mu mv mw mx fq je fe ff paragraph-image"><div class="fe ff no"><img src="../Images/d06b38bcf56c2c0d5b19bf038fc341a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*QjA8csAhKvWxxBl-RRE1SQ.png"/></div></figure><p id="13dc" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">如果我们只是将一个2D平面投影到这个3D空间上(<em class="kn">例如，通过放下x3 </em>)，它将简单地将各个层挤压在一起，并丢失我们想要捕捉的所有信息。相反，我们想要的是展开瑞士卷，以获得2D数据集，如果我们能够捕捉几乎所有的方差，而没有太多的信息损失。这种技术被称为流形学习。</p><h1 id="9b4a" class="kp kq hu bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm dt translated">流形学习</h1><p id="128a" class="pw-post-body-paragraph jp jq hu jr b js ln ju jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km hn dt translated">一个人曾经说过“我如何在一天内游遍一个城市？”导演回答说“通过雇佣一个CGI人员，并让他展开城市”——因此多面学习诞生了。—哈哈…… </p><p id="ec91" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">简而言之，流形学习是指我们弯曲和扭曲高维空间，使其可以映射到低维空间，其中高维空间可以在低维空间中局部相似。这种技术通常是机器学习中最常用的方法，因为它使算法能够找到减少数据集维度的最佳流形。例如，在我们的瑞士卷的例子中，算法的目标是学习展开我们的瑞士卷的最佳方式，使我们能够相对于它的原始维度空间捕捉尽可能多的信息。</p><figure class="mu mv mw mx fq je fe ff paragraph-image"><div class="fe ff od"><img src="../Images/9b8b22d975f3bdfe0a72ed63331b191e.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*8r7-RR9SuwgChBi3nNKJvQ.png"/></div></figure><figure class="mu mv mw mx fq je fe ff paragraph-image"><div role="button" tabindex="0" class="jf jg di jh bf ji"><div class="fe ff oe"><img src="../Images/dd612c3fafe069eb0d0c145c68b6e873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mV6MGRR7gflMs_SpCcbvxg.png"/></div></div><figcaption class="jl jm fg fe ff jn jo bd b be z ek">Source: <a class="ae ko" href="https://github.com/ageron/handson-ml" rel="noopener ugc nofollow" target="_blank">handson-ml</a></figcaption></figure><h2 id="a956" class="nj kq hu bd kr nr ns nt kv nu nv nw kz ka nx ny ld ke nz oa lh ki ob oc ll ia dt translated">弱点</h2><p id="3dbc" class="pw-post-body-paragraph jp jq hu jr b js ln ju jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km hn dt translated">流形学习未能提高我们模型的准确性的地方是当手头的任务在低维子空间中表达时并不简单。这样，如果我们要减少训练集的维数，数据集的解释能力相对于它的原始维数会减少。</p><blockquote class="ls"><p id="8adb" class="lt lu hu bd lv lw lx ly lz ma mb km ek translated">一般来说，如果你有一个超过3维的数据集，通常情况下，降维算法将有利于提高模型的性能。</p></blockquote></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="7964" class="kp kq hu bd kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ms lk ll lm dt translated">主成分分析</h1><p id="ffec" class="pw-post-body-paragraph jp jq hu jr b js ln ju jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km hn dt translated">主成分分析<strong class="jr ie"> (PCA) </strong>数据科学领域最重要的算法之一，也是迄今为止当今最流行的降维方法。主成分分析的目标很简单，识别一个最接近数据点的超平面，并将数据投影到其上。</p><figure class="mu mv mw mx fq je fe ff paragraph-image"><div role="button" tabindex="0" class="jf jg di jh bf ji"><div class="fe ff of"><img src="../Images/43518817d7ff40b0882c8665db9484a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*XGaA7KWUlhWZLIezYEBIHA.gif"/></div></div><figcaption class="jl jm fg fe ff jn jo bd b be z ek">Source: <a class="ae ko" href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/principal_component_analysis.html" rel="noopener ugc nofollow" target="_blank">leonardoaraujosantos</a></figcaption></figure><h1 id="6750" class="kp kq hu bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm dt translated">保持我们的差异</h1><p id="dd14" class="pw-post-body-paragraph jp jq hu jr b js ln ju jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km hn dt translated">当在低维超平面中映射我们的原始数据集时，我们首先必须确定正确的超平面。在主成分分析中，超平面在大多数情况下由穿过我们数据集的轴来确定，这保持了最大的方差。</p><figure class="mu mv mw mx fq je fe ff paragraph-image"><div class="fe ff og"><img src="../Images/3b49e8919a618fdd3375a766ecbedab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*4g773zGtB6As7iId_Zu_7g.png"/></div><figcaption class="jl jm fg fe ff jn jo bd b be z ek">Source: <a class="ae ko" href="https://prachimjoshi.wordpress.com/2015/07/23/principal-component-analysis-part-1/" rel="noopener ugc nofollow" target="_blank">prachimjoshi</a></figcaption></figure><p id="69fa" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">我们希望选择在数据集中保留最大方差的轴，因为与考虑其他投影相比，它很可能会丢失较少的信息。在这种情况下，我们选择的轴被称为<strong class="jr ie">主成分</strong>，也被称为<strong class="jr ie"> PCA1 </strong>，因为它捕捉了我们数据集内的最大方差，第二个主成分与第一个主成分正交，因为它占了我们数据集中最大的剩余方差，表示为<strong class="jr ie"> PCA2 </strong>。</p><pre class="mu mv mw mx fq ne nf ng nh aw ni dt"><span id="b705" class="nj kq hu nf b fv nk nl l nm nn"><strong class="nf ie">Note: </strong>In real life we are more likely to work with higher dimensional data sets. As a result, we may need to include addition Principal Components in order to capture the more of the variance in our data set. Thereby, increasing the performance of our Machine Learning algorithm. However, for the purpose of visualisation, only necessary to take two Principle Components for visualisation; and in some cases three.</span><span id="b8f6" class="nj kq hu nf b fv nq nl l nm nn"><strong class="nf ie">Data Visualisation:</strong> [PCA1, PCA2 and/or PCA3] 2 or 3 Dimensions</span><span id="1d4a" class="nj kq hu nf b fv nq nl l nm nn"><strong class="nf ie">Machine Learning:</strong> It is preferred in most machine learning problems to capture at least 95% of the training set's variance. Thus, it is not necessary to stick with 2 or 3 Principal Components.</span></pre></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="7306" class="kp kq hu bd kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ms lk ll lm dt translated">主成分</h1><p id="2bb9" class="pw-post-body-paragraph jp jq hu jr b js ln ju jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km hn dt translated">有各种各样的技术可以用来寻找我们的主要组成部分。然而，寻找我们的主成分的最常见的方法被称为<strong class="jr ie">奇异值分解(SVD) </strong>。事实证明，如果您执行一种称为奇异值分解(<strong class="jr ie"> SVD </strong>)的基本矩阵分解技术，将训练集矩阵X分解为三个矩阵的点积，我们的第三个矩阵V*实际上包含了我们正在寻找的所有主分量。</p><figure class="mu mv mw mx fq je fe ff paragraph-image"><div class="fe ff oh"><img src="../Images/a16da75273b02568e699855c06bcb491.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/1*aWJsuoMEqcuIbTfycDqoQA.gif"/></div></figure><pre class="mu mv mw mx fq ne nf ng nh aw ni dt"><span id="24dc" class="nj kq hu nf b fv nk nl l nm nn">Where:</span><span id="9f1d" class="nj kq hu nf b fv nq nl l nm nn"><strong class="nf ie">M =</strong> m × n matrix whose entries come from a field, where either field consists of real numbers or complex numbers.<br/><strong class="nf ie">U  = </strong>a <em class="kn">m × m </em>unitary matrix. (left singular vector)<br/><strong class="nf ie">Σ  =</strong> <em class="kn">m</em> × <em class="kn">n </em>diagonal matrix with non-negative real numbers.<br/><strong class="nf ie">V  = </strong><em class="kn">n</em> × <em class="kn">n </em>unitary matrix<em class="kn"> </em>. ( right singular vector)<br/><strong class="nf ie">V*  = </strong>conjugate transpose of the <em class="kn">n</em> × <em class="kn">n</em> unitary matrix.</span><span id="0711" class="nj kq hu nf b fv nq nl l nm nn">For those whom are interested in the full mathematics in Singular Value Decomposition(SVD). <a class="ae ko" href="https://www.youtube.com/watch?v=mBcLRGuAFUk" rel="noopener ugc nofollow" target="_blank">Here is a good link</a></span></pre><h1 id="1307" class="kp kq hu bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm dt translated">履行</h1><p id="5afb" class="pw-post-body-paragraph jp jq hu jr b js ln ju jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km hn dt translated">对我们的数据集进行主成分分析非常简单。我们只需调用Sci-Kit Learn中的函数<strong class="jr ie"> PCA </strong>。</p><pre class="mu mv mw mx fq ne nf ng nh aw ni dt"><span id="2e9a" class="nj kq hu nf b fv nk nl l nm nn"><strong class="nf ie">from</strong> sklearn.decomposition <strong class="nf ie">import</strong> PCA<br/><br/>pca = PCA(n_components = 2)<br/>X2D = <!-- -->pca.fit(X).transform(X)</span></pre><p id="68e7" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">在拟合数据集之后，我们可以将数据集转换成一个<strong class="jr ie"> Pandas DataFrame对象</strong>，并带有各自的标签<strong class="jr ie"> (PCA1，PCA2)。</strong></p><pre class="mu mv mw mx fq ne nf ng nh aw ni dt"><span id="10d4" class="nj kq hu nf b fv nk nl l nm nn">df = pd.DataFrame(X2D,columns = ['PCA1','PCA2'])</span></pre><p id="2922" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">然后我们可以把主要成分绘制到散点图上。为了简单起见，我使用了一个名为<strong class="jr ie"> Seaborn </strong>的库。</p><figure class="mu mv mw mx fq je fe ff paragraph-image"><div class="fe ff oi"><img src="../Images/6efc26bcd38eed68fa057e84e550b704.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*Y09knVcUg0O6m9cUCZO1nQ.png"/></div></figure><p id="8059" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">从散点图中，很明显，我们可以看到我们的主成分聚类。这表明我们的数据集中存在可以解释我们的目标变量的方差。最值得注意的是，如果我们想要检查每个主成分的解释方差，Sci-Kit Learn有一个方便的函数，名为<strong class="jr ie">explained _ variance _ ratio</strong>，它为我们提供了每个主成分的解释方差的信息。让我们调用它！</p><pre class="mu mv mw mx fq ne nf ng nh aw ni dt"><span id="5c15" class="nj kq hu nf b fv nk nl l nm nn">pca.explained_variance_ratio<br/>array([0.82,0.15])</span></pre><blockquote class="oj ok ol"><p id="ed7f" class="jp jq kn jr b js jt ju jv jw jx jy jz om kb kc kd on kf kg kh oo kj kk kl km hn dt translated">这告诉我们82%的方差位于第一个轴上，</p><p id="e5f1" class="jp jq kn jr b js jt ju jv jw jx jy jz om kb kc kd on kf kg kh oo kj kk kl km hn dt translated">12%位于第二轴上，</p><p id="6f6e" class="jp jq kn jr b js jt ju jv jw jx jy jz om kb kc kd on kf kg kh oo kj kk kl km hn dt translated">其余的方差在其他主成分中获得。</p></blockquote><p id="587d" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">当我们希望在高维数据集中执行数据可视化任务时，这种方法非常有效。然而，当我们建模时，我们经常希望在我们的训练集中捕获一定比例的方差。</p><blockquote class="ls"><p id="348e" class="lt lu hu bd lv lw lx ly lz ma mb km ek translated">作为一般规则:我们通常倾向于在我们的训练集中捕获至少95%的方差。</p></blockquote><p id="dc71" class="pw-post-body-paragraph jp jq hu jr b js mc ju jv jw md jy jz ka me kc kd ke mf kg kh ki mg kk kl km hn dt translated">为此，我们只需将n_components参数从一个整数更改为一个介于0.0到1.0之间的浮点数，表示我们希望捕获的方差比率:</p><pre class="mu mv mw mx fq ne nf ng nh aw ni dt"><span id="b6e1" class="nj kq hu nf b fv nk nl l nm nn">pca = PCA(n_components = 0.95)<br/>X = pca.fit_transform(X_train)</span></pre></div><div class="ab cl mh mi hc mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hn ho hp hq hr"><h1 id="9fd8" class="kp kq hu bd kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ms lk ll lm dt translated">恭喜你！您已经涵盖了降维和主成分分析的所有基本原理！！</h1><blockquote class="oj ok ol"><p id="4f67" class="jp jq kn jr b js jt ju jv jw jx jy jz om kb kc kd on kf kg kh oo kj kk kl km hn dt translated">哇，一天要学的东西太多了！我们刚刚讨论了机器学习的一些重要概念。我希望你对降维有了更多的了解，并在你的下一次冒险中看到它的力量。</p></blockquote><p id="f336" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">以下是我们所学内容的简要总结:</p><ul class=""><li id="0561" class="op oq hu jr b js jt jw jx ka or ke os ki ot km ou ov ow ox dt translated">什么是<strong class="jr ie"/><strong class="jr ie">维度诅咒</strong></li><li id="283c" class="op oq hu jr b js oy jw oz ka pa ke pb ki pc km ou ov ow ox dt translated">我们可以用来<strong class="jr ie">减少维度<strong class="jr ie">诅咒</strong>影响</strong>的方法。值得注意的是:<strong class="jr ie">投影&amp;流形学习</strong></li><li id="fd04" class="op oq hu jr b js oy jw oz ka pa ke pb ki pc km ou ov ow ox dt translated">各种<strong class="jr ie">降维方法</strong>的<strong class="jr ie">弱点</strong></li><li id="994c" class="op oq hu jr b js oy jw oz ka pa ke pb ki pc km ou ov ow ox dt translated">介绍<strong class="jr ie">主成分分析</strong>和<strong class="jr ie">的工作原理</strong></li><li id="aeb0" class="op oq hu jr b js oy jw oz ka pa ke pb ki pc km ou ov ow ox dt translated">如何<strong class="jr ie">实现主成分分析</strong></li></ul><h2 id="b97d" class="nj kq hu bd kr nr ns nt kv nu nv nw kz ka nx ny ld ke nz oa lh ki ob oc ll ia dt translated">如果你喜欢这篇文章，请👏并分享给你的朋友。记住，你最多可以鼓掌50次——这对我真的很重要。</h2><figure class="mu mv mw mx fq je"><div class="bz el l di"><div class="pd pe l"/></div></figure></div></div>    
</body>
</html>