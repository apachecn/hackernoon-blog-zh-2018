<html>
<head>
<title>MIT 6.S094: Deep Learning for Self-Driving Cars 2018 Lecture 3 Notes: Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">麻省理工6。S094:自动驾驶汽车的深度学习2018讲座3笔记:深度强化学习</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-3-notes-deep-reinforcement-learning-fe9a8592e14a?source=collection_archive---------7-----------------------#2018-01-25">https://medium.com/hackernoon/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-3-notes-deep-reinforcement-learning-fe9a8592e14a?source=collection_archive---------7-----------------------#2018-01-25</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><blockquote class="ir is it"><p id="6568" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="http://twitter.com/bhutanisanyam1" rel="noopener ugc nofollow" target="_blank">你可以在Twitter @bhutanisanyam1 </a>找到我，在<a class="ae jt" href="https://www.linkedin.com/in/sanyambhutani/" rel="noopener ugc nofollow" target="_blank"> Linkedin这里</a> <br/>这里<a class="ae jt" href="https://becominghuman.ai/a-self-driving-new-year-33284e592f35" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae jt" href="https://hackernoon.com/a-self-driving-new-year-2-d1bbc5a83570" rel="noopener ugc nofollow" target="_blank">这里</a>是两篇关于我<a class="ae jt" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>通往<a class="ae jt" href="https://hackernoon.com/tagged/self-driving-cars" rel="noopener ugc nofollow" target="_blank">自动驾驶汽车</a>的文章</p><p id="76d3" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="https://github.com/init27/MIT-6.S094-Deep-Learning-for-Self-Driving-Cars" rel="noopener ugc nofollow" target="_blank">你可以在这里找到降价文件</a></p><p id="810e" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" rel="noopener" href="/init27-labs/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-1-notes-807be1a50893">你可以在这里找到讲座1的笔记</a> <br/> <a class="ae jt" href="https://hackernoon.com/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-2-notes-e283b9ec10a0" rel="noopener ugc nofollow" target="_blank">讲座2的笔记可以在这里找到</a> <br/> <a class="ae jt" href="https://hackernoon.com/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-4-notes-computer-vision-f591f14b3b99" rel="noopener ugc nofollow" target="_blank">讲座4的笔记可以在这里找到</a> <br/> <a class="ae jt" rel="noopener" href="/@init_27/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-5-notes-deep-learning-for-human-5cb0f53e4f15">讲座5的笔记可以在这里找到</a></p><p id="821b" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated">这些是麻省理工学院六年级第三讲的笔记。S094:自动驾驶汽车深度学习课程(2018)，由<a class="ae jt" href="https://twitter.com/lexfridman" rel="noopener ugc nofollow" target="_blank">莱克斯·弗里德曼</a>授课。</p></blockquote><p id="f8f3" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">所有图片均来自讲座幻灯片。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff jx"><img src="../Images/8e1620da3e0fe55b2147b2b7e2cbb139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZ0L88ILnqi5zt1cfYU7BQ.png"/></div></div></figure><p id="9514" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">如果你想简单了解强化学习，这里有一个快速入门</p><h1 id="9c70" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated"><strong class="ak">我们能在多大程度上教会系统从数据中感知和行动这个世界？</strong></h1><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff lh"><img src="../Images/9da0636b13c6d158344e151c4c5264bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rn3l1y5knzwPHQBgCeF4lw.png"/></div></div></figure><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff li"><img src="../Images/e53fa874bd7366a7ec3196413dde6476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GyXFHXeJI4XTFPWO_yOIDQ.png"/></div></div></figure><p id="2c61" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">人工智能系统需要执行的任务堆栈</p><ul class=""><li id="0de0" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">环境:系统运行的环境。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ls"><img src="../Images/89f9eb9f8322ee26390e677d96c9dcb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Thr9Qza9KmG-Pk9hHOsW2w.png"/></div></div></figure><ul class=""><li id="56fb" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">传感器:通过传感器感知世界，并将其转换为机器可以感知的原始数据。世界上运行的机器人的输入。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff li"><img src="../Images/fb3e3117180e5ed6eadc4909ca2161ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rXwkP4y_Pf1VWK2aSsM3nQ.png"/></div></div></figure><ul class=""><li id="dd9f" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">传感器数据:由传感器提取的原始数据。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff jx"><img src="../Images/2d664a8545e2a8309a7bc1e1e85fbd2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMJWu_YkBAsu_n1fDcCCRA.png"/></div></div></figure><ul class=""><li id="dd0f" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">特征提取:从传感器数据中提取特征。<a class="ae jt" href="https://hackernoon.com/tagged/structure" rel="noopener ugc nofollow" target="_blank">结构</a>是从数据中提取出来的，这样你就可以输入、辨别、分离和理解数据。<br/>原始的感官数据经过多重高阶抽象处理。深度学习自动化了这项先前由人类专家执行的任务。</li><li id="2b96" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">我们形成更高阶的表示，基于这些表示可以应用ML技术。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ly"><img src="../Images/79ecf3588cee1c65653216a82b888663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kJHk4nkAt6V3d1_ilqAr3Q.png"/></div></div></figure><ul class=""><li id="f0f8" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">一旦ML技术将数据转换成简单的可操作信息，我们就将这些信息聚集成知识。深度学习网络能够执行监督学习任务、生成任务和非监督技术。知识，是简单而干净的有用的价值。这些可以是单一值、语音、图像等等。</li><li id="53bb" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">我们建立了一个分类法，一个知识库。我们把想法联系起来。</li><li id="8976" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">基于分类的代理推理:连接过去的数据和感知世界，根据目标定义计划。(目标，这里可能是奖励函数)。<br/>规划:融合传感器信息，做出更有利于DL方法的行动。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff lz"><img src="../Images/0458a07cfb0de7ca237e526c33228bed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a3-804TyKQVRMPRUJ2aP9w.png"/></div></div></figure><ul class=""><li id="820b" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">既然它在现实世界中表演，它就必须有可以在现实世界中表演的效应器。</li></ul><p id="0048" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">人工智能堆栈中有多少是可以“学习”的？</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff li"><img src="../Images/e53fa874bd7366a7ec3196413dde6476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GyXFHXeJI4XTFPWO_yOIDQ.png"/></div></div></figure><p id="2118" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">我们可以学习表象和知识。神经网络将数据映射为信息，核方法在这里也是有效的。</p><p id="4032" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">将正确的传感器数据映射为知识是DL的亮点。</p><p id="7f4f" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">开放式问题:我们能否将此扩展到端到端的推理和可操作信息？</p><p id="241c" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">Q2，我们能把这个扩展到SDC和机器人的真实世界吗？</p><h1 id="50ae" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated"><strong class="ak">深度学习的类型</strong></h1><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ma"><img src="../Images/c28ee162eb8a5f916e28d7072c1d2e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b84VsVEpdKIGgtArSuOF6g.png"/></div></div></figure><ul class=""><li id="2151" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">监督:每个数据点都由人类标记。</li><li id="f53c" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">无监督:数据未标记。</li><li id="fd5d" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">半监督学习:一些数据由人类标注。</li><li id="bca4" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">强化学习:RL是半监督学习的一个子类。<br/>目标:从稀疏的奖励/监督数据中学习，并利用从一个状态到另一个状态遵循时间动态的事实，这可以通过时间传播，以基于先前的数据推断关于现实的知识。我们可以概括现实世界中稀疏的学习信息。</li></ul><h1 id="896a" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">强化学习的哲学动机</h1><p id="2aa7" class="pw-post-body-paragraph iu iv hu ix b iy mb ja jb jc mc je jf ju md ji jj jv me jm jn jw mf jq jr js hn dt translated">监督学习:记忆基础事实数据，以形成从基础事实中概括出来的表述。</p><p id="58ec" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">强化学习:蛮力通过时间传播稀疏信息，以将质量奖励分配给没有直接奖励的状态。当数据/回报很少，但通过时间联系在一起时，理解这个世界。这相当于推理。</p><h2 id="deda" class="mg kk hu bd kl mh mi mj kp mk ml mm kt ju mn mo kx jv mp mq lb jw mr ms lf mt dt translated">代理和环境</h2><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mu"><img src="../Images/6c930dcaeedc3716ed0a3990d3cd5f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VByNF7339P0GYmlMGiXTtQ.png"/></div></div></figure><p id="2904" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">通过时间的连接被建模为:</p><p id="2854" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">有一个代理，在环境中执行一个动作，接收一个新的状态和一个奖励。这个过程一遍又一遍地继续。</p><p id="47c5" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">示例:</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mv"><img src="../Images/622c028e3fdc12d2f4808e00c66d6f6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DtE4WbvUCsDTeWiC0c9qSw.png"/></div></div></figure><ul class=""><li id="7067" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">雅达利突围:代理人是桨。主体采取的每一个行动都对环境的演变产生影响。成功是通过总奖励机制来衡量的。这里的分数是由游戏给出的。该方案必须以系统可解释的方式规范化。目标是最大化目标。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mw"><img src="../Images/c3278b0da8c8ea89cf0553827fda00cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5lCEHO-Fp85hzsTjJx7VsQ.png"/></div></div></figure><ul class=""><li id="ef55" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">大车-杆子平衡:<br/>目标:移动大车顶部平衡杆连续问题。<br/>状态:角度，角速度，大车的水平速度。<br/>动作:对推车施加水平力。<br/>奖励:1在每一个时间步如果杆子是直立的。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mx"><img src="../Images/a9bde53f378855cdf237343dcb9b1d6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SyZ9IG9tbOB-hltI2CUedg.png"/></div></div></figure><ul class=""><li id="89f2" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">所有第一人称射击游戏<br/>毁灭战士:目标:消灭所有对手。<br/>状态:来自游戏的原始像素</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff my"><img src="../Images/ce45b48f40615f8813e20eca0158272b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i2rfbDy435VDJWlW7UWLSw.png"/></div></div></figure><ul class=""><li id="3588" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">工业机器人:<br/>用机器人装箱。<br/>目标:挑选一个盒子，放入容器中。<br/>状态:世界的原始像素。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mz"><img src="../Images/4df819943d69a6071a066111141336eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tsvam2ye-8t9e2X8mAAm3w.png"/></div></div></figure><ul class=""><li id="6093" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">马尔可夫决策过程:行动-回报-状态，直到收到终端状态。</li></ul><p id="b68f" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">RL代理的主要组件</strong></p><p id="d157" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">以下一项或多项:</p><ul class=""><li id="41d0" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">政策:在每个州执行何种行动的计划。</li><li id="08ed" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">价值功能:意识到什么是好的状态，什么是可以执行的好的行动。</li><li id="f9fa" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">模型:代理对世界的表示。</li></ul><p id="c6b5" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">房间里的机器人</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff na"><img src="../Images/2ab43d8e4e7a4e6b523a8897c9f05016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gs2Tx8aNuSapySri7WfDaA.png"/></div></div></figure><p id="e5cb" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">确定性方法:选择最短路径。</p><p id="5e23" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">非确定性</p><p id="2669" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">关键观察:空间中的每一个状态都必须有一个控制非确定性环境的计划。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nb"><img src="../Images/0dcbd815df8b63fd0cbbcb77843f7696.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sHq4fBxCtDOSMs9IhiAh8w.png"/></div></div></figure><p id="4354" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">如果奖励函数被设计成每一步都受到惩罚，那么在这种情况下的最优策略将是选择最短的路径。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nc"><img src="../Images/8aa8040a8e42e36c4a87fa05903e4c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hjtowb0b4LBxNe3uzYqY6g.png"/></div></div></figure><p id="2184" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">如果我们减少惩罚，运动的随机性是允许的。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nd"><img src="../Images/6e9b582a45030857b3221817c8bd7cf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EH8TvriyuLKWxJd6dx6h_g.png"/></div></div></figure><p id="5a66" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">如果我们把奖励从+ve变成了运动，那就有更多的激励让我们留在板上不完成。</p><p id="4cef" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">价值函数:</strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ne"><img src="../Images/6cecd8bf46af30b1afcfbf94d4c1a61e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bsv-YeLFkBnGyZ8SElCJPg.png"/></div></div></figure><p id="44f8" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">环境状态的价值是我们将来可能得到的回报。这是通过贴现未来的贴现来确定的。</p><p id="771e" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">伽玛:降低未来目标的重要性。</p><p id="1f0f" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">一个好的策略是最大化贴现的未来目标的总和。</p><p id="905e" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">Q-学习:</strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nf"><img src="../Images/c63d1a3601fd7eb45b4f34074eaa5bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iRHNLmHdthueArHD80CBLg.png"/></div></div></figure><p id="c4ed" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">我们使用任何政策来评估最大化未来回报的状态。</p><p id="5f8f" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">这允许我们考虑更大的状态空间和动作空间。<br/>我们模拟采取行动，并更新我们对行动效果的评估。</p><p id="c8cb" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">勘探与开采:</strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ng"><img src="../Images/14e63260820125c489e759989476fb3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3FvtH-PKmRbjywx-EZSW8A.png"/></div></div></figure><p id="c716" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">随着对Q函数形成更好的估计，我们对可以执行的更好的动作形成更好的感觉。这还不完美，所以探索是有价值的。随着评估的改进，勘探的价值越低。</p><p id="01e5" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">因此，最初我们希望进行更多探索，并随着时间的推移，随着我们的估计变得更加准确，减少探索。</p><p id="5584" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">因此，最终系统应该按照Q函数贪婪地运行。</p><p id="655f" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">用于Q函数的表格表示。</p><p id="acdb" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">y轴是状态，X轴是动作。</p><p id="2bef" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">该表以随机方式初始化，并使用贝尔曼方程进行更新，随着时间的推移，近似值成为合适的表。</p><p id="d7d6" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">问题:当Q表呈指数级增长时。使用现实世界/游戏中的像素输入。潜在的状态空间和可能的组合状态比系统内存所能容纳的要大，比用贝尔曼方程所能估计的要大。</p><p id="64b5" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">深RL: </strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff lz"><img src="../Images/6a7c2dd06d89a5f532ce08248b1c9f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y8R6KdPIx0unvK1igN2rAQ.png"/></div></div></figure><p id="06a5" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">神经网络真的很擅长估计。</p><p id="de17" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">DL:与ML相比，它允许我们在更大的状态空间上逼近数值。这使我们能够处理原始值的感官数据，它更有能力处理现实世界的应用；这是可以概括的。</p><p id="b78d" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">这种理解来自于将原始感官信息转化为简单有用的信息，并在此基础上采取行动。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nh"><img src="../Images/c2669305fba5a1c576bbc1b466bd3155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UIAk_UoTaLMEGilFNamBlQ.png"/></div></div></figure><p id="fe42" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">代替Q函数，我们插入了一个神经网络。<br/>输入:状态空间。<br/>输出:各状态可具有的函数值。<br/> DQN:深度Q网络。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ni"><img src="../Images/b0dd40e81553efd94a53cf21f1670865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M8RWevLxhus56RABFEGYYQ.png"/></div></div></figure><p id="4fb9" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">DQN是如何训练的？</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nj"><img src="../Images/7e30341c2cbfdc5fc9acd4da048654ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GbA9LVGJnCxj4AfJ3hujpw.png"/></div></div></figure><p id="b4c7" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">bellman方程输入奖励并对未来的奖励进行折扣。</p><p id="2067" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">神经网络的损失函数:获取当前状态下收到的奖励，通过神经网络向前传递，计算未来状态的值，并从当前动作状态的向前传递中减去该值。</p><p id="41c4" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">我们取Q函数估计器(NN)所估计的未来价值和基于可能行为的可能价值之间的差值。</p><p id="36d7" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">算法:</strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div class="fe ff nk"><img src="../Images/4230543af041a77c9e3c2457f9848f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*67QUgxIzHWUBrM6aahkgZw.png"/></div></figure><p id="ccba" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">输入:运行状态</p><p id="f603" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">输出:Q-每个动作的值。</p><p id="3bd4" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">给定一个转换S，一个动作a产生一个奖励r’并改变到状态S’。</p><p id="9949" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">更新是对当前状态的网络进行前馈传递，对下一状态的所有可能动作进行前馈传递，并使用反向传播更新权重。</p><h1 id="671c" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated"><strong class="ak"> DQN小窍门:</strong></h1><p id="2a91" class="pw-post-body-paragraph iu iv hu ix b iy mb ja jb jc mc je jf ju md ji jj jv me jm jn jw mf jq jr js hn dt translated"><strong class="ix hv">经验重演:</strong> <br/>游戏通过模拟进行，观察结果被收集到经验库中，通过分批随机抽取先前的经验进行训练。这样，系统就不会对模拟的特定发展过程进行过度建模。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nl"><img src="../Images/7994309ce444954e1948b6d6c628ad7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ndWCzEenoL0kuSqchwL25w.png"/></div></div></figure><p id="4779" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">固定目标网络:</strong> <br/>我们用一个NN来估计动作对中当前状态和下一个状态的值，从而多次使用。当我们运行网络时，我们也在更新网络。所以损失函数中的目标函数改变了，这就导致了稳定性的问题。所以我们修复了网络，只是每1000步更新一次。<br/>当我们训练网络时，用于估计目标函数的网络保持固定，从而产生稳定的损失函数。</p><p id="5e3e" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">奖励剪裁:</strong> <br/>对以一般化方式操作的系统来说是真的。这些简化了奖励函数，无论是正面的还是负面的。</p><p id="c576" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">跳帧:</strong> <br/>每4帧执行1个动作。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nm"><img src="../Images/c7b0cae274e4a0b1bd9d436846f410c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r5kgt5x_ogv_TScH_Vy8ew.png"/></div></div></figure><p id="1379" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">圆:使用技巧时。<br/>交叉:未使用的技术。<br/>数字越高，获得的奖励越多。</p><p id="fd63" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">结论:重放目标能显著提高奖励。</p><h1 id="b27c" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated"><strong class="ak">深度Q学习算法:</strong></h1><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ls"><img src="../Images/19c558b028d30b04a4e3558e7e02ac9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PzKsw47WTPW2w_YOE22wLQ.png"/></div></div></figure><p id="f625" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">注意:循环不是训练的一部分，它是将观察、状态、动作、奖励和下一个状态保存到重放记忆中的部分。</p><p id="f8e3" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">接下来，我们从内存中随机取样，根据损失函数训练网络。概率:ε，是探索的概率——随着时间的推移而降低。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ls"><img src="../Images/732e4092f28e35f0859fdca95302fd21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XPyCYkvLW6CJjmiG0_QEAg.png"/></div></div></figure><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff lz"><img src="../Images/7b01cf55e3f5093cf056b4a39bb64bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dq2cwADdezBMGNcqDRV1LA.png"/></div></div></figure><p id="f256" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">2015:雅达利突围<br/> DQN在许多雅达利游戏上表现优异。</p><p id="49b2" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">AlphaGo (2016):</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff li"><img src="../Images/659e77567d87b24a2c4ca2980d8d1426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fDilXXjJaQxmnmAlmDmS6g.png"/></div></div></figure><p id="da2e" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">注意:任何时候可能的法定董事会条件= 2.8x10^(170)</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nn"><img src="../Images/e787af10db9a75e09908fc3fb5739ef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ncH_ok4p7Dpk_pQD5eKJA.png"/></div></div></figure><p id="9623" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">使用人类专家的立场发挥，以监督的方式播种，RL方法击败人类专家。</p><p id="6911" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">(偏颇)观点:人工智能十年成就，AlphaGo Zero (2017):</p><ul class=""><li id="dab9" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">它是在没有任何训练数据的情况下开发的。</li><li id="dc4f" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">击败AlphaGo。</li></ul><p id="01d6" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv"> AlphaGo方法:</strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ly"><img src="../Images/347767de44d8a30b1f9bec128c9c86cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hR5TqQ9hyZE6F9tH3z2WIQ.png"/></div></div></figure><p id="44e9" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">使用MTCS:蒙特卡罗树搜索。</p><p id="61f4" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">给定一个大的状态空间。我们从一个棋盘开始，在探索和利用的平衡中选择棋步，直到得出一些结论。这个信息是反向传播的，然后我们知道棋盘位置的价值。</p><p id="ad9c" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">AlphGo使用神经网络的“直觉”来估计状态的质量。</p><p id="d1e1" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">窍门:</p><ul class=""><li id="e106" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">使用基于神经网络预测的MCTS来估计未来状态有多好。它执行简单的前瞻动作，进行目标校正以产生损失函数。</li><li id="7084" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">多任务学习:网络是“双头的”，</li></ul><ol class=""><li id="364d" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js no lp lq lr dt translated">它输出最优移动的概率。</li><li id="f557" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js no lp lq lr dt translated">它还估计了获胜的概率。</li></ol><ul class=""><li id="e72a" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">我们希望在短期内结合最佳举措，并以高概率获胜。</li><li id="7fe5" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">更新的架构:Resnet(ImageNet的赢家)</li></ul><h1 id="0c79" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">深层交通</h1><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ly"><img src="../Images/7f4f06724d309183e86450ae0043e01c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5FVeIRYvXQTVKSgGzxbtng.png"/></div></div></figure><p id="8a0b" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><a class="ae jt" href="https://selfdrivingcars.mit.edu/deeptraffic-about" rel="noopener ugc nofollow" target="_blank">点击这里查看官方教程</a></p><p id="f301" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">v2功能:</p><ul class=""><li id="7fea" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">我们可以进行多代理培训(最多10辆车)</li><li id="1009" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">(很酷的附加功能)定制汽车图片。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff np"><img src="../Images/1d598f27fd012703c4751bd5f933dc85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ydC7HV2zzqqLWBIgqy1A5A.png"/></div></div></figure><p id="7f5d" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">目标:随着时间的推移达到最高的平均速度。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nq"><img src="../Images/72d15b8f53892a5fa1b11daee5ae6cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iSxqnngqVaupwLvSOsMeRg.png"/></div></div></figure><p id="c16e" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">道路:网格空间，一个占用网格:当空的时候，它被设置为ab-网格值是可以达到的任何速度。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nr"><img src="../Images/18391751c0fc0b3aa0e874e6440679a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vRRvAi0pN_8rloWvWTE7rw.png"/></div></div></figure><p id="b03a" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">网格中的其他汽车:网格中的值是速度较慢的汽车的速度。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ns"><img src="../Images/3af4ce919023bc5b83a67a5e3d6efc7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4JEJsFzks-2pa1uqw8eTQg.png"/></div></div></figure><p id="f074" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">我们可以决定将哪一部分用作网络的输入。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nt"><img src="../Images/388309c78efee23cb466105c2a6d3952.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QLle7OYVSWEhH5o_bklfyA.png"/></div></div></figure><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nu"><img src="../Images/047be055d7d8545a7d5ea1cc83560689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U5LTqhcU8mqfTIncQcGTzQ.png"/></div></div></figure><p id="cd86" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">安全系统可以被认为是基本MPC的等价物:允许防止碰撞的基本传感器。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nv"><img src="../Images/ec9a9ef3f510e6af51c9a220f34e7f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EHgf1XQloiUJpdFyHWTJtg.png"/></div></div></figure><p id="542a" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">任务:在安全系统的约束下在空间内移动。红色:不可到达的空间。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nw"><img src="../Images/64d534ace02ebd87e444042a4e4c3b44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jpbqlQmvHuhYxtA3RLG9VA.png"/></div></div></figure><p id="c0cf" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">目标:不堵车。</p><p id="6362" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">输入:状态空间。<br/>输出:不同动作的值。<br/>基于ε值并通过训练、推理评估:我们选择勘探程度。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nx"><img src="../Images/8ec697008d3814432dd2f8c73ba616e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5IAQ82uZTC76gFbAwguGwg.png"/></div></div></figure><p id="daa8" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">5行动空间</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ny"><img src="../Images/d282dfb43be82d02c34cc722c31a42a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gOR332WbiztYZixBoQtqHw.png"/></div></div></figure><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nz"><img src="../Images/efd6937e66321886aee7365401985b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EMlCdoo6jMOTEU3wD7gjgg.png"/></div></div></figure><p id="f423" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">“大脑”将状态作为输入。奖励执行向前传递并计算奖励。“大脑”是包含用于训练和评估的神经网络的地方。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff oa"><img src="../Images/e5d861308802522740faed0f9f4cb68c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s1hDW1-VGlMYq12LTOkT7A.png"/></div></div></figure><p id="d82b" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">新增加:可以被NN控制的代理数量，范围从1到10。评估以相同的方式进行。<br/>注意:代理不知道其他代理的优先权。每个个体代理的动作都是贪婪的，并且不是以优化的分布式方式。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ob"><img src="../Images/20279c620b36f72e7ab1754dcc8b6237.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mec0xIx8fSh93tQcQbC4EA.png"/></div></div></figure><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff oc"><img src="../Images/61ce0071bd5283794ea7cdc497fb815c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aMmM4XKlNL5T2tZLXPXqeQ.png"/></div></div></figure><p id="c043" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">评估:</p><ul class=""><li id="005c" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">每次运行收集45秒的模拟时间。</li><li id="384c" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">计算500次运行的中值。</li><li id="b2ad" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">服务器端评估。</li><li id="6554" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">随机性已经大大降低。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff od"><img src="../Images/f19d62b1bc6c0db252a62870a5db41dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YbOY8DZNhTGnCFxaCEEYew.png"/></div></div></figure><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ls"><img src="../Images/755e067d9ca030bc6a832a87b1ab6cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7RvVKatALzJIRL6o8g3OXQ.png"/></div></div></figure><h1 id="900d" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated"><strong class="ak">RL方法适用于人类在回路中的学习吗？</strong></h1><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff oe"><img src="../Images/958d3984503c4628e5b96aaa6d157264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lj_Z1unarF6Dr1PJ5aItyw.png"/></div></div></figure><ul class=""><li id="512a" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">我们可以探索驾驶员数据。</li><li id="022c" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">真实世界测试是不可行的。</li></ul><h1 id="a312" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated"><strong class="ak">大多数成功的RL都不涉及深度RL: </strong></h1><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff of"><img src="../Images/3425fde2d6633296d1171a9568d61139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K0d8NghTEH5_ElxSm3CM-g.png"/></div></div></figure><ul class=""><li id="25f5" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">波士顿机器人公司</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff og"><img src="../Images/ea2ead575eaf994b552ac7cff88c76d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3Lri9Fc0G-e8wNrIBn9Ag.png"/></div></div></figure><ul class=""><li id="44ba" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">Waymo: <br/> DL用于感知。<br/>大部分工作都是用传感器完成的。<br/>使用基于模型的方法。</li></ul><h1 id="82f0" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated"><strong class="ak">回复:意想不到的本地高额奖励</strong></h1><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff oh"><img src="../Images/37eae1de0818e0354e17336f19e28b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*thWYrauRC1pUZ-PvbLO9yg.png"/></div></div></figure><p id="c7ab" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">当应用到现实世界时，这些在所有的例子中都出现了。</p><h1 id="e78d" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">人工智能安全:</h1><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff of"><img src="../Images/9765ed9818855144276e19ad6133a365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RM8j0vswscitBYePTVWVXQ.png"/></div></div></figure><ul class=""><li id="084e" class="lj lk hu ix b iy iz jc jd ju ll jv lm jw ln js lo lp lq lr dt translated">探索RL规划算法如何以意想不到的方式发展。</li><li id="a632" class="lj lk hu ix b iy lt jc lu ju lv jv lw jw lx js lo lp lq lr dt translated">怎么才能约束他们呢？以强制它们以安全的方式运行。</li></ul><blockquote class="ir is it"><p id="ff7b" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="http://twitter.com/bhutanisanyam1" rel="noopener ugc nofollow" target="_blank">你可以在Twitter @bhutanisanyam1 </a>上找到我，在<a class="ae jt" href="https://www.linkedin.com/in/sanyambhutani/" rel="noopener ugc nofollow" target="_blank"> Linkedin上联系我这里</a> <br/> <a class="ae jt" href="https://becominghuman.ai/a-self-driving-new-year-33284e592f35" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae jt" href="https://hackernoon.com/a-self-driving-new-year-2-d1bbc5a83570" rel="noopener ugc nofollow" target="_blank">这里</a>是我学习自动驾驶汽车的两篇文章</p><p id="f2a9" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="https://tinyletter.com/sanyambhutani" rel="noopener ugc nofollow" target="_blank">订阅我的时事通讯，获取深度学习、计算机视觉文章的每周精选列表</a></p></blockquote><figure class="jy jz ka kb fq kc"><div class="bz el l di"><div class="oi oj l"/></div></figure></div></div>    
</body>
</html>