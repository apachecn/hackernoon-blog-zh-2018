<html>
<head>
<title>Kubernetes for dev infrastructure</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Kubernetes开发基础设施</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/kubernetes-for-dev-infrastructure-40b9175cb8c0?source=collection_archive---------6-----------------------#2018-01-26">https://medium.com/hackernoon/kubernetes-for-dev-infrastructure-40b9175cb8c0?source=collection_archive---------6-----------------------#2018-01-26</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="b564" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">Kubernetes是目前最热门的开源项目之一。这是一个生产级容器编排系统，灵感来自谷歌自己的<a class="ae jp" href="http://blog.kubernetes.io/2015/04/borg-predecessor-to-kubernetes.html" rel="noopener ugc nofollow" target="_blank"> Borg </a>，于2014年发布。从那时起，成千上万的开发人员加入了这个项目，现在它正在成为运行容器化应用程序的行业标准。Kubernetes旨在大规模运行生产工作负载，但它的能力远不止于此。在这篇文章中，我将讲述我在<a class="ae jp" href="https://thoughtspot.com" rel="noopener ugc nofollow" target="_blank"> ThoughtSpot </a>工作时建立Kubernetes集群作为开发基础设施的核心组件的经历。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff jq"><img src="../Images/1188d52c976e6984521b3bc1254d8ef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_NuVjWpLrt-G98e2-oIGvA.png"/></div></div></figure><h2 id="25ad" class="kc kd hu bd ke kf kg kh ki kj kk kl km jc kn ko kp jg kq kr ks jk kt ku kv kw dt translated">语境</h2><p id="2c99" class="pw-post-body-paragraph ir is hu it b iu kx iw ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo hn dt translated"><a class="ae jp" href="https://thoughtspot.com" rel="noopener ugc nofollow" target="_blank"> ThoughtSpot </a>正在为大型企业开发一个复杂的<strong class="it hv"> BI </strong>系统，该系统运行在另一个受Borg启发的编排系统Orion之上。它是内部设计的，当时Docker和Kubernetes都没有公开发布。</p><p id="8b43" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">与本文相关的一些关于ThoughtSpot的知识是:</p><ul class=""><li id="f1a8" class="lc ld hu it b iu iv iy iz jc le jg lf jk lg jo lh li lj lk dt translated">该系统由几十个服务组成，运行它们的开销相当大。数据非常少的空闲系统需要<strong class="it hv">20–30Gb</strong>的RAM、<strong class="it hv"> 4个CPU </strong>内核和<strong class="it hv">2–3分钟</strong>的启动时间。</li><li id="5883" class="lc ld hu it b iu ll iy lm jc ln jg lo jk lp jo lh li lj lk dt translated">ThoughtSpot出售自己的设备，通常每个集群至少有<strong class="it hv"> 1TB </strong> RAM，因此20-30Gb的开销对该产品来说不成问题。然而，对于开发基础设施来说，这是一个相当大的问题。</li><li id="4e40" class="lc ld hu it b iu ll iy lm jc ln jg lo jk lp jo lh li lj lk dt translated">办公室里有很多退役的硬件可供开发人员使用。</li></ul><h2 id="cf88" class="kc kd hu bd ke kf kg kh ki kj kk kl km jc kn ko kp jg kq kr ks jk kt ku kv kw dt translated">动机</h2><p id="67a2" class="pw-post-body-paragraph ir is hu it b iu kx iw ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo hn dt translated">我最初被分配去解决一个听起来简单的问题:让集成测试更快。有数百个基于Selenium的工作流，它们按顺序运行，需要10个小时才能完成。显而易见的解决方案是将它们并行化。问题是它们没有被设计成并发运行，因此我们必须要么重构所有测试，要么为每个线程提供一个ThoughtSpot系统的独立副本(<em class="lq">a</em>T18】test back end)。重新设计测试可能看起来像一个更干净的解决方案，但这需要整个工程团队付出巨大的努力，并且需要对产品进行大量与测试相关的更改，因此这是不可行的。我们已经决定采用第二种方法，这就把任务留给了我，我最终在Docker和Kubernetes的帮助下解决了这个问题:<em class="lq">使快速(在2-3分钟内)用预加载的测试数据旋转几十个测试后端成为可能，运行测试，拆除它们，重复。</em></p><h2 id="1fcc" class="kc kd hu bd ke kf kg kh ki kj kk kl km jc kn ko kp jg kq kr ks jk kt ku kv kw dt translated">小路</h2><p id="33ca" class="pw-post-body-paragraph ir is hu it b iu kx iw ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo hn dt translated">带着这个任务，我开始寻找选择。实际上，一些基础架构已经就位:我们有一个运行在四台服务器上的VMware集群。当前的集成测试已经使用它来提供测试后端，但是存在一些问题:</p><ul class=""><li id="261b" class="lc ld hu it b iu iv iy iz jc le jg lf jk lg jo lh li lj lk dt translated">它只能维持大约一百个虚拟机，之后我们将不得不购买更多昂贵的专有硬件。该公司的其他工作流已经利用了大约80%的资源。</li><li id="d0ce" class="lc ld hu it b iu ll iy lm jc ln jg lo jk lp jo lh li lj lk dt translated">并行克隆10个或更多虚拟机会导致IO崩溃。它必须移动~ <strong class="it hv"> 500Gb </strong>的磁盘快照，而且要花很长时间。</li><li id="5c60" class="lc ld hu it b iu ll iy lm jc ln jg lo jk lp jo lh li lj lk dt translated">虚拟机的启动时间远远超过2-3分钟。</li></ul><p id="5dd5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">虚拟化对我们来说不是一个可行的选择，所以我们转向容器。在2016年初，我们正在寻找两个主要选项:<a class="ae jp" href="https://linuxcontainers.org/lxd/" rel="noopener ugc nofollow" target="_blank"><strong class="it hv">【LXC】/LXD</strong></a>和<a class="ae jp" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="it hv"> Docker </strong> </a>。Docker已经是公认的领导者，LXD 2.0只会和Ubuntu 16.04一起发布。然而，Docker对小的单进程容器有强烈的偏见，这些容器必须通过网络相互对话，并以这种方式形成一个完整的系统。另一方面，LXD提供了一些东西，看起来更像熟悉的虚拟机，具有<strong class="it hv"> init </strong>系统和在单个容器中运行多个服务的能力。有了Docker，我们不得不以“LXD方式”在整洁和使用上妥协，或者重新调整整个系统，使其在Docker上运行，这是不可行的。另一方面，对于LXD，我们不能依赖Docker所拥有的详尽的社区知识和文档。尽管如此，我们还是决定试一试。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff lr"><img src="../Images/a3a8970ec13bee98fa4dac0c17589273.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KzmUY16PknJY4SFXyDV9UA.png"/></div></div></figure><h2 id="b7e4" class="kc kd hu bd ke kf kg kh ki kj kk kl km jc kn ko kp jg kq kr ks jk kt ku kv kw dt translated">LXC/LXD</h2><p id="fe7f" class="pw-post-body-paragraph ir is hu it b iu kx iw ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo hn dt translated">我拿了四台机器，每台都有<strong class="it hv"> 256Gb </strong> RAM、<strong class="it hv"> 40个CPU </strong>内核、2个固态硬盘和4个硬盘，安装了LXD，并在每个节点上配置了一个ZFS池。然后，我设置了一个Jenkins作业，该作业将构建项目，将其安装在其中一台机器上的LXD容器中，导出一个映像并将其推送到其他三个节点。每个集成测试工作将只做<code class="eh ls lt lu lv b">lxd clone current-master-snapshot &lt;backend-$i&gt;</code>，运行测试并在完成后销毁容器。由于ZFS的<a class="ae jp" href="https://en.wikipedia.org/wiki/Copy-on-write" rel="noopener ugc nofollow" target="_blank">写时拷贝</a>特性，克隆操作现在是瞬时的。每个节点能够处理大约十个测试后端，直到事情开始崩溃。这是一个很好的结果，比VMware给我们的好得多，但有一个主要缺点:它不灵活，也不可扩展。每个测试作业都需要准确地知道在哪个LXD节点上创建后端，如果需要超过10个后端，它们就不合适。换句话说，没有编排系统，它是不可伸缩的。对于LXD，当时我们只有两个选择:使用OpenStack或编写自己的调度程序(这是我们不想写的)。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff lw"><img src="../Images/06cec6db38420c2e9956ca948520acf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*YyRYRNyizFDvVQZfMf2maw.png"/></div></figure><p id="f23c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">OpenStack支持LXD作为计算后端，但在2016年，这一切都很新鲜，几乎没有记录，几乎没有工作。我花了大约一周的时间尝试配置OpenStack集群，然后放弃了。幸运的是，我们还有另一条未探索的道路:<em class="lq"> Docker和Kubernetes </em>。</p><h2 id="0a2c" class="kc kd hu bd ke kf kg kh ki kj kk kl km jc kn ko kp jg kq kr ks jk kt ku kv kw dt translated">多克-库伯内特公司</h2><p id="0e3d" class="pw-post-body-paragraph ir is hu it b iu kx iw ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo hn dt translated">在第一次浏览文档之后，很明显Docker和Kubernetes哲学都不适合我们的用例。Docker明确表示“<a class="ae jp" href="https://blog.docker.com/2016/03/containers-are-not-vms/" rel="noopener ugc nofollow" target="_blank">容器不是虚拟机</a>”，Kubernetes是为运行一个(或几个)应用程序而设计的，由许多小型容器服务组成，而不是许多庞大的单容器应用程序。另一方面，我们认为Kubernetes背后的运动是强大的。这是一个顶级的开源产品，有一个活跃的社区，它可以(应该)最终取代我们自己开发的产品中的编排系统。因此，我们在使Kubernetes适应开发基础设施需求时获得的所有知识，我们可以在以后将主要产品迁移到Kubernetes时重用。考虑到这一点，我们开始建设新的基础设施。</p><p id="1c4e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们无法在产品中摆脱对Systemd的依赖，所以我们最终将所有东西打包到一个基于CentOS 7的容器中，并将Systemd作为顶层流程。这是为我们工作的基本映像<a class="ae jp" href="https://gist.github.com/bsod90/3796481608d79ceb332f1260806f439b" rel="noopener ugc nofollow" target="_blank"> Dockerfile </a>。我们制作了一个非常重的Docker映像(最初是<strong class="it hv"> 20Gb </strong>，经过一些优化后是<strong class="it hv"> 5 </strong>，它封装了Orion (ThoughtSpots自己的容器引擎)，然后它在cgroup容器中运行20多个ThoughtSpot服务，这大致对应于单个节点生产设置。这很麻烦，但却是从一无所有到有用的最快方法。</p><p id="0121" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">之后，我使用了一些其他的物理机器，并在它们上面创建了我们的第一个Kubernetes集群。在所有的Kubernetes抽象中，只有<a class="ae jp" href="https://kubernetes.io/docs/concepts/workloads/pods/pod/" rel="noopener ugc nofollow" target="_blank"> <em class="lq"> Pod </em> </a>与我们的问题相关，因为它实际上只是一个在某处运行的容器。对于我们的大多数测试用例，我们需要创建多个<em class="lq"> Pods </em>，并且能够根据工作负载对它们进行分组会很有帮助。也许<a class="ae jp" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/" rel="noopener ugc nofollow" target="_blank"> <em class="lq">标签</em> </a>更适合这个目的，但是我们决定开发一个<em class="lq">复制控制器</em>。<a class="ae jp" href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/" rel="noopener ugc nofollow" target="_blank"> <em class="lq">复制控制器</em> </a>是一个抽象，它将创建许多<em class="lq">pod</em>(根据一个<em class="lq">复制因子</em>)，确保它们始终处于活动状态，另一端，接收来自<a class="ae jp" href="https://kubernetes.io/docs/concepts/services-networking/service/" rel="noopener ugc nofollow" target="_blank"> <em class="lq">服务</em> </a>的流量，并在<em class="lq">pod</em>之间重新分配流量。<em class="lq">复制控制器</em>假设每个<em class="lq"> Pod </em>都是平等无状态的，这样每个新的<em class="lq">服务</em>连接都可以被路由到一个随机的<em class="lq"> Pod </em>。在我们的例子中，我们没有创建<em class="lq">服务</em>，只是使用<em class="lq">复制控制器</em>作为分组<em class="lq">容器</em>的一种方式，并确保它们在任何东西死亡时自动重新创建。然后，每个测试作业都会为自己创建一个<em class="lq">复制控制器</em>，并直接使用底层的<em class="lq">pod</em>。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff lx"><img src="../Images/f5cf1da5611f8ade2995f33cbdeb5047.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*HRRkdlA4W3CAXfmK7A29bA.png"/></div></figure><h2 id="86cb" class="kc kd hu bd ke kf kg kh ki kj kk kl km jc kn ko kp jg kq kr ks jk kt ku kv kw dt translated">豆荚网络黑客</h2><p id="7b9c" class="pw-post-body-paragraph ir is hu it b iu kx iw ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo hn dt translated">我们依靠<em class="lq">pod</em>在API方面表现得像真正的虚拟机。特别是，我们需要对每个<em class="lq"> Pod </em>的SSH访问，以及与动态分配的端口对话的能力。此外，每个<em class="lq"> Pod </em>显然都是有状态的，因为图像封装了存储在其中的状态。这实际上意味着，我们不得不直接进入<a class="ae jp" href="https://kubernetes.io/docs/concepts/cluster-administration/networking/" rel="noopener ugc nofollow" target="_blank"> <em class="lq"> pod-network </em> </a>，而不是通过<a class="ae jp" href="https://kubernetes.io/docs/reference/generated/kube-proxy/" rel="noopener ugc nofollow" target="_blank"> <em class="lq"> kube-proxy </em> </a>使用<em class="lq">服务</em>和负载平衡。我们已经通过在Kubernetes主节点上启用<a class="ae jp" href="https://unix.stackexchange.com/questions/14056/what-is-kernel-ip-forwarding" rel="noopener ugc nofollow" target="_blank"> <em class="lq"> ip转发</em> </a>(将它变成一个路由器)并重新配置所有办公室路由器来路由<code class="eh ls lt lu lv b">172.18.128.0/16 </code>(我们的<em class="lq"> pod网络</em>)通过Kubernetes主节点。这是一个<strong class="it hv">可怕的</strong>黑客行为，永远不应该在生产环境中进行，但它允许我们快速启动开发基础设施，解决眼前的问题，并开始寻找如何在未来使我们的产品Kubernetes就绪的方法。</p></div><div class="ab cl ly lz hc ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="hn ho hp hq hr"><p id="810d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">快进近两年，以下是ThoughtSpot的开发基础设施的现状:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff mf"><img src="../Images/f8e218ac559766c9e7981dbd45798974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QUC2L260d2oHr-gXp1IjnA.png"/></div></div></figure><ul class=""><li id="801d" class="lc ld hu it b iu iv iy iz jc le jg lf jk lg jo lh li lj lk dt translated">Kubernetes集群运行在20台物理机器上，总共提供<strong class="it hv">7tb</strong>RAM和<strong class="it hv">928</strong>T29】CPU内核。</li><li id="3297" class="lc ld hu it b iu ll iy lm jc ln jg lo jk lp jo lh li lj lk dt translated">每个主机节点都运行CentOS 7和4.4-lt Linux内核。</li><li id="8cc2" class="lc ld hu it b iu ll iy lm jc ln jg lo jk lp jo lh li lj lk dt translated">我们用<a class="ae jp" href="https://github.com/weaveworks/weave" rel="noopener ugc nofollow" target="_blank"> <em class="lq">编织</em> </a>作为覆盖网，路由hack还在。</li><li id="2792" class="lc ld hu it b iu ll iy lm jc ln jg lo jk lp jo lh li lj lk dt translated">我们运行一个内部Docker注册中心，每次主或发布分支构建成功时，CI pipeline都会将一个产品映像上传到该注册中心。</li><li id="8f96" class="lc ld hu it b iu ll iy lm jc ln jg lo jk lp jo lh li lj lk dt translated">我们使用<a class="ae jp" href="https://plugins.jenkins.io/kubernetes" rel="noopener ugc nofollow" target="_blank"> Jenkins Kubernetes </a>插件在Kubernetes上动态提供Jenkins奴隶。</li><li id="eba3" class="lc ld hu it b iu ll iy lm jc ln jg lo jk lp jo lh li lj lk dt translated">我们最近在几个节点上部署了<a class="ae jp" href="https://www.gluster.org/" rel="noopener ugc nofollow" target="_blank"> Glusterfs </a>，并开始尝试持久性有状态服务。<a class="ae jp" href="https://blog.lwolf.org/post/how-i-deployed-glusterfs-cluster-to-kubernetes/" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae jp" href="https://github.com/heketi/heketi/blob/master/doc/admin/install-kubernetes.md" rel="noopener ugc nofollow" target="_blank">这个</a>是基本教程。</li></ul><h2 id="1080" class="kc kd hu bd ke kf kg kh ki kj kk kl km jc kn ko kp jg kq kr ks jk kt ku kv kw dt translated">马斯河</h2><p id="899b" class="pw-post-body-paragraph ir is hu it b iu kx iw ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo hn dt translated">在这个项目中，我们发现了另一个伟大的开源工具，它在管理物理硬件方面帮了我们很多。它叫做<a class="ae jp" href="https://maas.io" rel="noopener ugc nofollow" target="_blank"> MAAS </a>，翻译过来就是“<em class="lq">金属即服务</em>”<br/>这是一个工具，它利用<a class="ae jp" href="https://en.wikipedia.org/wiki/Preboot_Execution_Environment" rel="noopener ugc nofollow" target="_blank"> PXE引导</a>和<a class="ae jp" href="https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface" rel="noopener ugc nofollow" target="_blank">远程节点控制</a>，允许使用任意操作系统映像进行动态节点重新映像。在用户端，它提供了一个REST API和一个漂亮的UI，这样您就可以以AWS的方式配置物理机器，而无需实际接触硬件。最初设置它需要一些努力，但在它存在之后，整个物理基础架构变得几乎像云一样灵活。<br/>目前，我们通过MAAS配置普通CentOS 7节点，然后运行一个<a class="ae jp" href="https://www.ansible.com/" rel="noopener ugc nofollow" target="_blank"> Ansible </a>脚本，该脚本升级内核，安装所有附加软件，并将节点添加到Kubernetes集群。(<a class="ae jp" href="https://gist.github.com/bsod90/b5dda35413a4d4f1d2eaf72fd3b7b46d" rel="noopener ugc nofollow" target="_blank">链接到要点</a>)</p><h2 id="67f1" class="kc kd hu bd ke kf kg kh ki kj kk kl km jc kn ko kp jg kq kr ks jk kt ku kv kw dt translated">星云</h2><p id="3bf8" class="pw-post-body-paragraph ir is hu it b iu kx iw ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo hn dt translated">大多数开发人员或CI职位不直接与MAAS或Kubernetes交互。在此之上，我们还有另一个自定义层，它将所有可用的资源聚合在一起，并提供一个API和UI来分配它们。它被称为Nebula，它可以在Kubernetes以及旧的VMware基础架构、AWS或物理硬件(通过MAAS)上创建和销毁测试后端。它还实现了<em class="lq"> lease </em>的概念:在一定时间内，将提供的每个资源分配给一个人或一个CI作业。租约到期时，资源会被自动回收或清理。</p><h2 id="a416" class="kc kd hu bd ke kf kg kh ki kj kk kl km jc kn ko kp jg kq kr ks jk kt ku kv kw dt translated">LXCFS</h2><p id="2967" class="pw-post-body-paragraph ir is hu it b iu kx iw ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo hn dt translated">默认情况下，Docker从主机挂载<code class="eh ls lt lu lv b">/proc/</code>文件系统，因此<code class="eh ls lt lu lv b">/proc/stat</code> (meminfo、cpuinfo等)不反映特定于容器的信息。特别是，它们不反映在cgroup上设置的任何资源配额。我们的产品和CI管道中的一些流程检查可用的总RAM，并相应地分配它自己的内存。如果进程不检查cgroup的限制，它很容易分配超过容器配额所允许的内存，然后被OOM杀手杀死。特别是，这发生在很多JS丑化器上，我们把它作为产品构建的一部分运行。这个问题在这里<a class="ae jp" href="https://github.com/moby/moby/issues/20688" rel="noopener ugc nofollow" target="_blank">描述和讨论，解决的方法之一就是使用</a><a class="ae jp" href="https://github.com/lxc/lxcfs" rel="noopener ugc nofollow" target="_blank"> LXCFS </a>。</p><blockquote class="mg mh mi"><p id="9bcc" class="ir is lq it b iu iv iw ix iy iz ja jb mj jd je jf mk jh ji jj ml jl jm jn jo hn dt translated">LXCFS是一个小型的FUSE文件系统，其目的是让Linux容器看起来更像一个虚拟机。它最初是LXC的一个附带项目，但是可以在任何运行时使用。</p><p id="f4c2" class="ir is lq it b iu iv iw ix iy iz ja jb mj jd je jf mk jh ji jj ml jl jm jn jo hn dt translated">LXCFS将注意procfs中关键文件提供的信息，例如:</p><p id="edae" class="ir is lq it b iu iv iw ix iy iz ja jb mj jd je jf mk jh ji jj ml jl jm jn jo hn dt translated"><code class="eh ls lt lu lv b">/proc/cpuinfo<br/>/proc/diskstats<br/>/proc/meminfo<br/>/proc/stat<br/>/proc/swaps<br/>/proc/uptime</code> <br/>是容器感知的，因此显示的值(例如在/proc/uptime中)真正反映了容器运行的时间，而不是主机运行的时间。</p></blockquote><h2 id="43ca" class="kc kd hu bd ke kf kg kh ki kj kk kl km jc kn ko kp jg kq kr ks jk kt ku kv kw dt translated">结论</h2><p id="857f" class="pw-post-body-paragraph ir is hu it b iu kx iw ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo hn dt translated">我们花了相当多的时间才弄清楚所有的事情。当我们刚开始使用Kubernetes 1.4时，文档和社区知识非常缺乏。我们从网上搜集信息，通过调试来学习。我们还对我们的产品做了几十处改动，重新设计了CI渠道，并尝试了许多本文中没有提到的其他东西。然而，最终一切都很顺利，Kubernetes成为了ThoughtSpot中开发基础设施的基石，提供了急需的灵活性，并允许利用办公室中现有的所有硬件。我在9月份离开了公司，但是这个项目被移交给了其他开发人员，并且一直在发展。我知道许多人正试图为他们的公司建立类似的东西，所以我很乐意回答下面评论中的任何问题。</p><h2 id="4e03" class="kc kd hu bd ke kf kg kh ki kj kk kl km jc kn ko kp jg kq kr ks jk kt ku kv kw dt translated">链接</h2><p id="af8b" class="pw-post-body-paragraph ir is hu it b iu kx iw ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo hn dt translated"><a class="ae jp" href="https://hackernoon.com/another-reason-why-your-docker-containers-may-be-slow-d37207dec27f" rel="noopener ugc nofollow" target="_blank">调试Docker容器缓慢的故事</a></p><p id="896b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><a class="ae jp" href="https://hackernoon.com/my-engineering-journey-to-date-8250d69fd079" rel="noopener ugc nofollow" target="_blank">我的工程之旅</a></p><p id="3b65" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><a class="ae jp" href="https://onebar.io" rel="noopener ugc nofollow" target="_blank">你的懈怠知识库</a></p></div></div>    
</body>
</html>