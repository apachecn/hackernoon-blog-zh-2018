<html>
<head>
<title>Introduction to Recommender System. Part 1 (Collaborative Filtering, Singular Value Decomposition)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推荐系统介绍。第1部分(协同过滤，奇异值分解)</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/introduction-to-recommender-system-part-1-collaborative-filtering-singular-value-decomposition-44c9659c5e75?source=collection_archive---------0-----------------------#2018-01-28">https://medium.com/hackernoon/introduction-to-recommender-system-part-1-collaborative-filtering-singular-value-decomposition-44c9659c5e75?source=collection_archive---------0-----------------------#2018-01-28</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/3cf0e3d1eb7aadffeb2c9a7891711064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PsI17WdbeL1OUyhD5H6JMQ.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek"><a class="ae jg" href="https://goo.gl/ihju1k" rel="noopener ugc nofollow" target="_blank">https://goo.gl/ihju1k</a></figcaption></figure><h1 id="905e" class="jh ji hu bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dt translated">1.介绍</h1><p id="24e1" class="pw-post-body-paragraph kf kg hu kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc hn dt translated">推荐系统是指能够预测用户对一组项目的未来偏好并推荐最佳项目的系统。现代社会需要推荐系统的一个重要原因是，由于互联网的普及，人们有太多的选择。过去，人们习惯在实体店购物，实体店的商品有限。例如，可以放在大片商店的电影数量取决于商店的大小。相比之下，如今，互联网允许人们在线访问丰富的资源。例如，网飞收藏了大量的电影。虽然可获得的信息量增加了，但新的问题出现了，因为人们很难选择他们真正想看的项目。这就是推荐系统的用武之地。本文将简要介绍构建推荐系统的两种典型方法，协同过滤和奇异值分解。</p></div><div class="ab cl ld le hc lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="hn ho hp hq hr"><h1 id="681f" class="jh ji hu bd jj jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke dt translated">2.传统方法</h1><p id="bef2" class="pw-post-body-paragraph kf kg hu kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc hn dt translated">传统上，有两种方法来构建推荐系统:</p><ul class=""><li id="c3a0" class="lp lq hu kh b ki lr km ls kq lt ku lu ky lv lc lw lx ly lz dt translated"><strong class="kh hv">基于内容的推荐</strong></li><li id="bad6" class="lp lq hu kh b ki ma km mb kq mc ku md ky me lc lw lx ly lz dt translated"><strong class="kh hv">协同过滤</strong></li></ul><p id="4114" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">第一个分析每个项目的性质。例如，通过对每个诗人的内容执行自然语言处理来向用户推荐诗人。另一方面，协同过滤不需要关于项目或用户本身的任何信息。它根据用户过去的行为推荐商品。我将在下面的段落中详细阐述协作过滤。</p><h1 id="1efd" class="jh ji hu bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dt translated">3.协同过滤</h1><p id="6099" class="pw-post-body-paragraph kf kg hu kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc hn dt translated">如上所述，协同过滤(CF)是一种基于用户过去行为的推荐方式。CF分为两类:</p><ul class=""><li id="7a62" class="lp lq hu kh b ki lr km ls kq lt ku lu ky lv lc lw lx ly lz dt translated"><strong class="kh hv">基于用户的</strong>:衡量目标用户与其他用户的相似度</li><li id="d460" class="lp lq hu kh b ki ma km mb kq mc ku md ky me lc lw lx ly lz dt translated"><strong class="kh hv">基于项目的</strong>:衡量目标用户评价/交互的项目与其他项目之间的相似性</li></ul><p id="79ec" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">CF背后的关键思想是相似的用户分享相同的兴趣，并且相似的项目被用户喜欢。</p><p id="c91a" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">假设有<em class="mi"> m </em>个用户和<em class="mi"> n </em>个物品，我们用一个大小为<em class="mi"> m*n </em>的矩阵来表示用户过去的行为。矩阵中的每个单元代表用户持有的相关意见。例如，M_{i，j}表示用户I如何喜欢项目j。这样的矩阵被称为<strong class="kh hv">效用矩阵</strong>。CF就像是根据用户或物品之间的相似性，来填充效用矩阵中某个用户之前没有看过/评级过的空白(单元格)。意见有两种，<strong class="kh hv">显性意见</strong>和<strong class="kh hv">隐性意见</strong>和<strong class="kh hv">意见</strong>。前者直接显示用户如何评价该项目(将其视为对一个应用程序或电影的评价)，而后者仅作为一个代理，为我们提供关于用户如何喜欢某个项目的启发式信息(例如，喜欢、点击、访问的数量)。显性观点比隐性观点更直接，因为我们不需要猜测这个数字意味着什么。例如，可能有一首用户非常喜欢的歌曲，但是他只听了一次，因为他在听的时候很忙。没有明确的意见，我们不能确定用户是否不喜欢那个项目。然而，我们从用户那里收集的大多数反馈都是隐性的。因此，正确处理隐式反馈非常重要，但这超出了本文的范围。我将继续讨论CF是如何工作的。</p><h2 id="fb4c" class="mj ji hu bd jj mk ml mm jn mn mo mp jr kq mq mr jv ku ms mt jz ky mu mv kd mw dt translated">基于用户的协同过滤</h2><p id="4c84" class="pw-post-body-paragraph kf kg hu kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc hn dt translated">我们知道，在基于用户的CF中，我们需要计算用户之间的相似性。但是，我们如何度量相似性呢？有两个选项，皮尔逊相关或余弦相似。设u_{i，k}表示用户I和用户k之间的相似度，v_{i，j}表示用户I给项目j的评级，其中v_{i，j} =？如果用户没有对该项进行评级。这两种方法可以表示如下:</p><figure class="my mz na nb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mx"><img src="../Images/bbddd21d2664d9b5283eb61d89fa01c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UAA_F9qVzXesP7rwVSw7_A.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Pearson Correlation (https://goo.gl/y93CsC)</figcaption></figure><figure class="my mz na nb fq iv fe ff paragraph-image"><div class="fe ff nc"><img src="../Images/19e3911d613233d8d66733f912f821b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*9_a6dvYrVevBiX2FQG9-JQ.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Cosine Similarity (https://goo.gl/y93CsC)</figcaption></figure><p id="c785" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">这两种方法都是常用的。不同之处在于，皮尔逊相关对于向所有元素添加常数是不变的。</p><p id="d024" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">现在，我们可以用下面的等式来预测用户对未评级项目的意见:</p><figure class="my mz na nb fq iv fe ff paragraph-image"><div class="fe ff nd"><img src="../Images/2d173c535f1d073007097b95854cd4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*tyq_meHOpldeSXISKkS7jg.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Unrated Item Prediction (https://goo.gl/y93CsC)</figcaption></figure><p id="33c8" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">我用一个具体的<a class="ae jg" href="http://technocalifornia.blogspot.tw/2014/08/introduction-to-recommender-systems-4.html?m=1&amp;from=singlemessage&amp;isappinstalled=0" rel="noopener ugc nofollow" target="_blank">例子</a>来说明一下。在下面的矩阵中，每一行代表一个用户，而每一列对应于不同的电影，除了最后一个记录了该用户与目标用户之间的相似性。每个单元格代表用户对该电影的评价。假设用户E是目标。</p><figure class="my mz na nb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ne"><img src="../Images/867606fed690b585b897eb45bc56f0e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9NBFo4AUQABKfoUOpE3F8Q.png"/></div></div></figure><p id="865b" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">由于用户A和F不与用户E共享任何共同的电影评级，所以他们与用户E的相似性没有在皮尔逊相关中定义。因此，我们只需要考虑用户B、C和d。基于皮尔逊相关性，我们可以计算以下相似性。</p><figure class="my mz na nb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nf"><img src="../Images/46efbab4c3b1a4fa365737a5bca5e4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jZIMJzKM1hKTFftHfcSxRw.png"/></div></div></figure><p id="ee5f" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">从上表中可以看出，用户D与用户E非常不同，因为他们之间的皮尔逊相关是负的。他对你之前的我的评分高于他的平均评分，而用户E的情况正好相反。现在，我们可以开始根据其他用户来填补用户E没有评级的电影的空白。</p><figure class="my mz na nb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ng"><img src="../Images/0b0dee31f736b900913ff976b2e68e71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9TC6BrfxYttJwiATFAIFBg.png"/></div></div></figure><p id="5a33" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">尽管计算基于用户的CF非常简单，但它存在几个问题。一个主要问题是用户的偏好会随着时间而改变。这表明基于它们的相邻用户预先计算矩阵可能导致差的性能。为了解决这个问题，我们可以应用基于项目的CF。</p><h2 id="8f43" class="mj ji hu bd jj mk ml mm jn mn mo mp jr kq mq mr jv ku ms mt jz ky mu mv kd mw dt translated">基于项目的协同过滤</h2><p id="91aa" class="pw-post-body-paragraph kf kg hu kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc hn dt translated">基于项目的CF不是测量用户之间的相似性，而是基于项目与目标用户评价的项目的相似性来推荐项目。同样，相似性可以用皮尔逊相关或余弦相似性来计算。主要区别在于，使用基于项目的协作过滤，我们垂直地填补空白，而不是基于用户的CF的水平方式。下表显示了如何为电影《我在你面前的T2》做到这一点。</p><figure class="my mz na nb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nh"><img src="../Images/269ea0dff5cf58adc5cf1570d3ca1000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LqFnWb-cm92HoMYBL840Ew.png"/></div></div></figure><p id="161d" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">它成功地避免了动态用户偏好带来的问题，因为基于项目的CF更加静态。然而，这种方法存在几个问题。首先，主要问题是可伸缩性。计算随着客户和产品的增长而增长。最坏情况的复杂度是O(mn ),有m个用户和n个项目。此外，稀疏是另一个问题。再看一下上面的表格。虽然只有一个用户对<em class="mi">矩阵</em>和<em class="mi">泰坦尼克号</em>都进行了评级，但是它们之间的相似度是1。在极端情况下，我们可以有数百万用户，两部完全不同的电影之间的相似性可能非常高，仅仅因为它们对于唯一对它们进行排名的用户来说具有相似的排名。</p><h1 id="5e86" class="jh ji hu bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dt translated">4.奇异值分解</h1><p id="de37" class="pw-post-body-paragraph kf kg hu kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc hn dt translated">处理CF产生的可伸缩性和稀疏性问题的一种方法是利用<strong class="kh hv">潜在因素模型</strong>来捕捉用户和项目之间的相似性。本质上，我们想把推荐问题变成一个优化问题。我们可以将其视为我们在预测给用户的项目评级方面有多好。一个常见的度量是<strong class="kh hv">均方根误差</strong> (RMSE)。RMSE越低，性能越好。由于我们不知道看不见的项目的评级，我们将暂时忽略它们。也就是说，我们只是最小化效用矩阵中已知条目的RMSE。为了实现最小RMSE，采用如下公式所示的<strong class="kh hv">奇异值分解</strong> (SVD)。</p><figure class="my mz na nb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ni"><img src="../Images/049237b5967c08c92a3f81f7d512b82c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*haUDjEiQmG0RapR0SHos6Q.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Singular Matrix Decomposition(<a class="ae jg" href="http://www.cs.carleton.edu/cs_comps/0607/recommend/recommender/images/svd2.png" rel="noopener ugc nofollow" target="_blank">http://www.cs.carleton.edu/cs_comps/0607/recommend/recommender/images/svd2.png</a>)</figcaption></figure><p id="43cc" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">x表示效用矩阵，U是左奇异矩阵，代表用户与<strong class="kh hv">潜在因素的关系。</strong> S是描述各潜在因素强度的对角矩阵，V转置是右奇异矩阵，表示项目与潜在因素的相似度。现在，你可能想知道我说的潜在因素是什么意思？这是一个宽泛的概念，描述了用户或项目所具有的属性或概念。例如，对于音乐来说，潜在因素可以指音乐所属的流派。奇异值分解通过提取潜在因子来降低效用矩阵的维数。本质上，我们将每个用户和每个项目映射到一个维度为<em class="mi"> r </em>的潜在空间。因此，它有助于我们更好地理解用户和项目之间的关系，因为它们变得可以直接比较。下图说明了这个想法。</p><figure class="my mz na nb fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nj"><img src="../Images/534a543949d11919a96c26b756448112.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GUw90kG2ltTd2k_iv3Vo0Q.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">SVD Maps Users and Items Into Latent Space (<a class="ae jg" href="https://www.youtube.com/watch?v=E8aMcwmqsTg&amp;list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&amp;index=55" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=E8aMcwmqsTg&amp;list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&amp;index=55</a>)</figcaption></figure><p id="da4c" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">奇异值分解有一个很好的性质，即它具有最小的重构平方和；因此也常用于降维。下面公式用A代替X，用σ代替S。</p><figure class="my mz na nb fq iv fe ff paragraph-image"><div class="fe ff nk"><img src="../Images/aa60722936afddea40c3a80bb30eea55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*8K_Auii__ZC44AV7VxloQA.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Sum of Square Error (<a class="ae jg" href="https://www.youtube.com/watch?v=E8aMcwmqsTg&amp;list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&amp;index=55" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=E8aMcwmqsTg&amp;list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&amp;index=55</a>)</figcaption></figure><p id="f124" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">但是这和我在本节开始提到的RMSE有什么关系呢？事实证明，RMSE和上证指数是单调相关的。这意味着上证指数越低，RMSE越低。利用SVD的便利特性，它可以最小化SSE，我们知道它也可以最小化RMSE。因此，SVD是解决这个优化问题的一个很好的工具。要为用户预测看不见的项目，我们只需将U、σ和t相乘。</p><p id="5ca6" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">Python Scipy有一个很好的稀疏矩阵SVD实现。</p><pre class="my mz na nb fq nl nm nn no aw np dt"><span id="4e01" class="mj ji hu nm b fv nq nr l ns nt"><strong class="nm hv">&gt;&gt;&gt; from</strong> <strong class="nm hv">scipy.sparse</strong> <strong class="nm hv">import</strong> csc_matrix<br/><strong class="nm hv">&gt;&gt;&gt; from</strong> <strong class="nm hv">scipy.sparse.linalg</strong> <strong class="nm hv">import</strong> svds<br/><strong class="nm hv">&gt;&gt;&gt; </strong>A = csc_matrix([[1, 0, 0], [5, 0, 2], [0, -1, 0], [0, 0, 3]], dtype=float)<br/><strong class="nm hv">&gt;&gt;&gt; </strong>u, s, vt = svds(A, k=2) # k is the number of factors<br/><strong class="nm hv">&gt;&gt;&gt; </strong>s<br/>array([ 2.75193379,  5.6059665 ])</span></pre><p id="049a" class="pw-post-body-paragraph kf kg hu kh b ki lr kk kl km ls ko kp kq mf ks kt ku mg kw kx ky mh la lb lc hn dt translated">SVD成功地解决了CF带来的可伸缩性和稀疏性问题。然而，SVD并不是没有缺陷。SVD的主要缺点是没有或很少解释我们向用户推荐一个项目的原因。如果用户急于知道为什么向他们推荐一个特定的项目，这可能是一个巨大的问题。我将在下一篇博文中对此进行更多的讨论。</p><h1 id="5efd" class="jh ji hu bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dt translated">5.结论</h1><p id="219a" class="pw-post-body-paragraph kf kg hu kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc hn dt translated">我已经讨论了构建推荐系统的两种典型方法，协同过滤和奇异值分解。在下一篇博文中，我将继续谈论一些更高级的构建推荐系统的算法。如果你对这篇文章有任何问题，请不要犹豫，在下面留下你的评论或者给我发电子邮件:khuangaf@connect.ust.hk。如果你喜欢这篇博文，请确保你在<a class="ae jg" href="https://twitter.com/steeve__huang" rel="noopener ugc nofollow" target="_blank"> twitter </a>上关注我，以获得更多伟大的深度学习文章！</p></div></div>    
</body>
</html>