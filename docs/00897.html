<html>
<head>
<title>MIT 6.S094: Deep Learning for Self-Driving Cars 2018 Lecture 4 Notes: Computer Vision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">麻省理工6。S094:自动驾驶汽车的深度学习2018第4讲笔记:计算机视觉</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-4-notes-computer-vision-f591f14b3b99?source=collection_archive---------9-----------------------#2018-01-28">https://medium.com/hackernoon/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-4-notes-computer-vision-f591f14b3b99?source=collection_archive---------9-----------------------#2018-01-28</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><blockquote class="ir is it"><p id="6568" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="http://twitter.com/bhutanisanyam1" rel="noopener ugc nofollow" target="_blank">你可以在Twitter @bhutanisanyam1 </a>找到我，在<a class="ae jt" href="https://www.linkedin.com/in/sanyambhutani/" rel="noopener ugc nofollow" target="_blank"> Linkedin这里</a> <br/>这里<a class="ae jt" href="https://becominghuman.ai/a-self-driving-new-year-33284e592f35" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae jt" href="https://hackernoon.com/a-self-driving-new-year-2-d1bbc5a83570" rel="noopener ugc nofollow" target="_blank">这里</a>是我学习<a class="ae jt" href="https://hackernoon.com/tagged/self-driving-cars" rel="noopener ugc nofollow" target="_blank">自动驾驶汽车</a>道路上的两篇文章</p><p id="76d3" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="https://github.com/init27/MIT-6.S094-Deep-Learning-for-Self-Driving-Cars" rel="noopener ugc nofollow" target="_blank">你可以在这里找到降价文件</a></p><p id="810e" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" rel="noopener" href="/init27-labs/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-1-notes-807be1a50893">你可以在这里找到讲座1的笔记</a> <br/> <a class="ae jt" href="https://hackernoon.com/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-2-notes-e283b9ec10a0" rel="noopener ugc nofollow" target="_blank">讲座2的笔记可以在这里找到</a> <br/> <a class="ae jt" href="https://hackernoon.com/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-3-notes-deep-reinforcement-learning-fe9a8592e14a" rel="noopener ugc nofollow" target="_blank">讲座3的笔记可以在这里找到</a> <br/> <a class="ae jt" rel="noopener" href="/@init_27/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-5-notes-deep-learning-for-human-5cb0f53e4f15">讲座5的笔记可以在这里找到</a></p><p id="821b" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated">这些是麻省理工学院六年级第四讲的笔记。S094:自动驾驶汽车深度学习课程(2018)，由<a class="ae jt" href="https://twitter.com/lexfridman" rel="noopener ugc nofollow" target="_blank">莱克斯·弗里德曼</a>教授。</p></blockquote><p id="f8f3" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">所有图片均来自讲座幻灯片。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff jx"><img src="../Images/0fc0144bf8989c9e8c7eb9faa4bd296f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*47f-0NEb8HnX2qR-Bg-DBw.png"/></div></div></figure><h1 id="ed69" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">计算机视觉:教计算机看东西。</h1><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff lh"><img src="../Images/2c920dfd802862fc621fe54520198ed7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YxkzAc1QwaSJHxm6Swi8Fw.png"/></div></div></figure><p id="f1aa" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">计算机视觉，到今天为止是深度<a class="ae jt" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>。我们对图像理解的大部分成功都是利用了神经网络。</p><ul class=""><li id="4b3c" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">受监督的<a class="ae jt" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>:注释数据由人类提供。神经网络通过从原始感官数据到图像类别的数据映射进行分析，并且对于健全性检查，它应该在测试集上表现良好。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff lr"><img src="../Images/4b181c0d564108902f02f03b382adfad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KuSIFsaKvEVAqXjMtidvJQ.png"/></div></div></figure><p id="a669" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">原始感官数据:对于机器来说，图像是数字的形式。</p><p id="4743" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">1通道或3通道数字阵列形式的图像被神经网络作为输入，通过回归或通过将图像分类成各种类别来产生输出。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ls"><img src="../Images/29ac1686ab7987d4f736c42d0c4b5c2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pcpUdNJe0rWIrGg8L_OMuA.png"/></div></div></figure><p id="264c" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">我们必须小心我们对于感知的容易和困难的假设。</p><p id="e29a" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">人类视觉与计算机视觉。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff lt"><img src="../Images/43d4235aa0a96271487b3565969cbdc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U719aymTJg7PVNERWN_P7A.png"/></div></div></figure><ol class=""><li id="a6ad" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js lu lo lp lq dt translated">视觉皮层的结构是分层的。随着信息从我们的眼睛传递到大脑，越来越高级的表征就形成了。这就是Deep NN对于图像背后的启发。通过这些层形成越来越高的表示。<br/>早期图层，获取原始像素，寻找边缘。通过连接这些边进一步发现更多的抽象特征。最后，寻找高阶语义。</li><li id="bd93" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js lu lo lp lq dt translated">深度学习对计算机视觉来说很难:</li></ol><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ma"><img src="../Images/6223e07e1771e67a50285b0155e86511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hVXlFofI5DJIMNSTgbIAjA.png"/></div></div></figure><ul class=""><li id="6bb4" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">照明可变性是驾驶中最大的挑战之一。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mb"><img src="../Images/bc55c9220eaa1021cbf507c7887d04b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PefR1HUW40ckJR-FIV4DYw.png"/></div></div></figure><ul class=""><li id="f113" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">姿态可变性:神经网络不擅长表现姿态。当物体旋转时，2D平面中的物体在颜色和纹理上看起来非常不同。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mc"><img src="../Images/a70a1146ac3ec2eca3d0b987d6e3b738.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i3SGzTySxqFd4tj8G-3n-w.png"/></div></div></figure><ul class=""><li id="c20c" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">类间可变性:对于分类问题，类内可变性很大，类间可变性很小。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff md"><img src="../Images/9576349fc6bc41c895626a5f81653ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1YEhXzcwANTdTfE0uf3PZw.png"/></div></div></figure><ul class=""><li id="7b8f" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">遮挡:对象的一部分被另一个对象遮挡，我们的任务是识别被遮挡的对象。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff me"><img src="../Images/e5d31a3ce239f2818bf5c61544c7362d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W30FXvziH9kcZDkKSMZvuA.png"/></div></div></figure><ul class=""><li id="56c4" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">哲学歧义:图像分类！=理解。</li><li id="78f7" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">无监督学习</li><li id="f08c" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">强化学习。</li></ul><h1 id="b8d8" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">图像分类管道:</h1><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mf"><img src="../Images/aba7350c74274373a999f98227d0f9a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jUu1Dvp-ezhDLgSM4FUAgA.png"/></div></div></figure><p id="3901" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">每个类中都有一个包含不同类别的箱子。这些箱子里都有很多这样的例子。任务:将新图像绑定到其中一个类中。</p><p id="8607" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">著名数据集:</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mg"><img src="../Images/4e5d603d9fa2b8d846ecf781e7946a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NmY46s2lvQraj5IXLJfrAw.png"/></div></div></figure><ul class=""><li id="e941" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">MNIST</li><li id="36cd" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">ImageNet</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mh"><img src="../Images/609cbde0767c84477a525531d21c8a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2nXjWxFUZt__whbQRK14Nw.png"/></div></div></figure><ul class=""><li id="8f8c" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">CIFAR-10 <br/>最简单的一个，包含10个类别，常用于探索CNN。</li></ul><p id="ac19" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">琐碎的例子:</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mi"><img src="../Images/51b81264665bcd4476baa0869cd2f538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-higFyNdjJvp15wTYShpuw.png"/></div></div></figure><ul class=""><li id="6593" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">通过减去像素强度矩阵来比较图像，然后求和每个元素的差异。如果总和高，则图像不同。<br/>如果我们使用这种方法，使用L2差分法我们可以得到35%的准确度，使用L1差分法可以得到38%的准确度，这些都比10%的随机准确度要好。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mj"><img src="../Images/97583ccb166f9de9ed153b917c42b5dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*czz9e3Rd7L0SZTOLoGEL3g.png"/></div></div></figure><ul class=""><li id="99cc" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">k-最近邻:<br/>我们试图找到k-最近邻的图像并将它们归入k个类，而不是找到最接近我们数据集的1个图像。我们从1到5改变k，看看它如何改变问题。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mk"><img src="../Images/8c7fe1fa9a6e7de46e190792693697d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8NVfvEr976S7t4yQ7zIy7A.png"/></div></div></figure><ul class=""><li id="aea3" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">当k=7时，我们达到30%的精度。<br/>人类等级准确率为95%。使用CNN，我们得到了97.75%的准确率。</li></ul><h2 id="4a5b" class="ml kk hu bd kl mm mn mo kp mp mq mr kt ju ms mt kx jv mu mv lb jw mw mx lf my dt translated">神经网络的工作:</h2><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mz"><img src="../Images/2ea8bd4d5378595d59e5637ce50cae5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gSpVjRXsA0fLt3Df9zyx1g.png"/></div></div></figure><ul class=""><li id="305a" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">对每个输入信号进行加权、偏置和相加。</li><li id="4147" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">这些然后被输入到非线性激活函数。</li><li id="8709" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">越来越多的层放在一起，这些就形成了一个很深的NN。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff na"><img src="../Images/21071925c36d1553040645cb830f03f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w-50YTIcul9hgW8OX5BDgA.png"/></div></div></figure><ul class=""><li id="11cd" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">神经网络的训练方式是向前传递，评估实例与基本事实的接近程度，然后惩罚导致错误决策的权重，奖励导致正确决策的权重。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nb"><img src="../Images/bfba013662e9ef72581732ef86b6fe1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZgsCq8Ny18b_EvH2FPBig.png"/></div></div></figure><ul class=""><li id="9ffe" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">例如:MNIST-对于10个案例，输出是10个不同的值。</li><li id="aef5" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">每个神经元基于它所代表的类别而兴奋。</li><li id="8a91" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">输出被分配给显示最高激活值的类。</li></ul><p id="17e0" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><a class="ae jt" href="https://hackernoon.com/convolutional-neural-network-in-5-minutes-8f867eb9ca39" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hv">卷积神经网络</strong> </a></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nc"><img src="../Images/4a8a04f8c14c63624cad0f607597e066.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zl0VrQAfpMz8VHXSFkxmGw.png"/></div></div></figure><p id="c329" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">当神经网络的任务是学习具有大量数据和大量对象的复杂任务时，细胞神经网络有效地工作。</p><p id="b23f" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">技巧-空间不变“:<br/>一个图像左上角的物体和右下角的物体是一样的。所以我们在图像中学习相同的特征。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nd"><img src="../Images/1807f4ce0ec86ec08b17281e1614c47d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WT92r_Cl2mV2NkOkJWeoVQ.png"/></div></div></figure><p id="aa5c" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">卷积运算:代替完全连接的层；这里存在深度的第三维。因此，该模块采用3个输入体积，并产生3D输出体积</p><p id="5277" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">他们截取图像的一部分，即“一个窗口”，并将其滑过图像。它们将相同的权重应用于图像的切片/窗口以生成输出。我们可以制造许多这样的过滤器。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ne"><img src="../Images/101629327754965a1456ced5930514e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxMa-pUYxeBIxN7y8sJZXw.png"/></div></div></figure><p id="f59a" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">每个过滤器上的参数是共享的。(如果一个特性在一个地方有用，那么它在任何地方都有用)这允许参数显著减少。空间特征的再利用。</p><ul class=""><li id="414b" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">深度:过滤器的数量。</li><li id="6ee8" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">步幅:应用滤镜时跳过的像素</li><li id="9cc2" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">填充:在卷积层的输入边界上添加0值。</li></ul><p id="9f9d" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">示例:</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nf"><img src="../Images/450abb22f8f4939cb529272218f4b2ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KinbvXl7QpjSMiDbU1Qtug.png"/></div></div></figure><ul class=""><li id="bc7c" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">左栏:3个输入通道。</li><li id="fe18" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">滤波器W0:滤波器的2个通道，每个通道的尺寸为3x3。</li><li id="509d" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">3x3过滤器将被“学习”</li><li id="b22e" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">这些在图像上滑动以产生输出。</li><li id="b834" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">应用这些操作来产生输出。</li></ul><p id="2ebf" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">卷积</strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ng"><img src="../Images/c20e86ddb2b106d477fd3a992811a1e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y1YSSYRMteyZWvVWav0eiw.png"/></div></div></figure><ul class=""><li id="60be" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">图像被输入。</li><li id="ab1f" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">我们执行身份滤波器来生成卷积图像。</li><li id="40b2" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">我们执行不同的其他过滤器来生成边缘。</li><li id="9051" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">我们可以检测任何类型的模式并产生输出。</li><li id="c70b" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">根据过滤器的不同，您会得到相同数量的输出，每个输出都显示了在哪里找到了模式。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nh"><img src="../Images/37bb832ffbb641231241da88afee21ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjM2QAWmgzbQy1l_6TWusA.png"/></div></div></figure><ul class=""><li id="8589" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">任务:学习分类任务所需的有用模式。</li><li id="be87" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">滤波器具有越来越高的表示阶数。</li><li id="2895" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">从边缘开始，最后到跨越图像的高级语义。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ni"><img src="../Images/fd68f87e08c37e8e56d7a2abb25f2dc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U300iCYAYK6V5Biqa8Q-og.png"/></div></div></figure><ul class=""><li id="d78f" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">跨越图像:池化。<br/>获取卷积运算的输出，并通过压缩信息降低其分辨率，例如考虑最大池中的最大值。</li><li id="73f7" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">注意:当涉及场景分割时,“空间分辨率的降低”具有不利的影响，但是在寻找图像中的高阶表示用于分类方面更好。</li><li id="cd13" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">许多这样的卷积层形成了CNN。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nj"><img src="../Images/8b604e3e624e896501c25ec963447931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zW4fhjLmKABtNeD9IdugWA.png"/></div></div></figure><ul class=""><li id="2547" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">完全连接的层允许我们将其应用于特定的领域。</li></ul><p id="531f" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv"> ImageNet案例研究</strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nk"><img src="../Images/5aa48d23d39d0031fa81c43441f4ef80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xYDmhdcJ9EGstz9R2KDgNg.png"/></div></div></figure><ul class=""><li id="ef29" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">任务:对最大的图像数据集之一进行分类。<br/> 14M+图像<br/> 21k+类别<br/>有许多子类</li><li id="e829" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">这些为检查姿势、类内可变性、照明提供了很好的挑战。</li><li id="964e" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated"><strong class="ix hv">网络:</strong></li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nl"><img src="../Images/30e0316d645642b132c6dc0240adfa21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*akfhZS4bMtBevd3jWshRiQ.png"/></div></div></figure><ul class=""><li id="3014" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">AlexNet 2012:首次重大改进。</li><li id="d560" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">ZFNet 2013</li><li id="6090" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">VGGNet 2014</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nm"><img src="../Images/f37006489aeb1cad1fc8f7ce3fae9de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_x50STFcoXXW2SB-NspMNQ.png"/></div></div></figure><ul class=""><li id="8ed2" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">GoogLeNet 2014 <br/>引入了初始模块。<br/>想法:它使用了不同大小的卷积为<a class="ae jt" href="https://hackernoon.com/tagged/network" rel="noopener ugc nofollow" target="_blank">网络</a>提供不同值的想法，进行不同的卷积并连接。<br/>较小的卷积:纹理上非常局部/高分辨率的特征。<br/>更大的卷积:更高/更抽象的特征。结果:更少的参数和更好的性能。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nn"><img src="../Images/bf1d69d6ba60edc584d4be428214a39a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YpRAepXS2R7Jusb9DV-vlA.png"/></div></div></figure><ul class=""><li id="faa3" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">ResNet 2015 <br/>灵感:(不一定成立)网络深度增加表征力。<br/>“残余块”允许创建更“深”的网络。<br/>残差块:<br/> -重复一个简单的网络块，类似于RNNs。<br/> -传递输入而不进行转换，以及学习权重的能力。<br/> -每一层接受前一层的输入和原始的、未转换的数据，以学习新的东西。</li><li id="f740" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">CUImage 2016</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff no"><img src="../Images/ad894024f7be72aa17d97498a42c92c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ynL9gNuqG2wO4HgXzj6lWA.png"/></div></div></figure><ul class=""><li id="db7f" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">SENet 2017 <br/>挤压和激励网络:<br/> -为卷积块的每个通道添加了一个参数，以便网络可以根据每个特征映射/网络输入自适应地调整每个通道的权重。<br/> -技巧:允许网络学习每个单独信道上的权重。<br/> -注意:这适用于任何架构。因为，它只是根据内容参数化选择哪个过滤器。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff np"><img src="../Images/b4be5a6f736155f512538351ecaff4fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o-IGEnXPgjAaYA_vD5kcmw.png"/></div></div></figure><ul class=""><li id="2654" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">分类的ILSVRC挑战评估<br/>前5名猜测。<br/>人为误差5.1%<br/>2015年超越。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nq"><img src="../Images/d8bd13d010fd9cd971ca1f6a07bec2f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mdhBCpd6CZop7oqYoLj8JA.png"/></div></div></figure><ul class=""><li id="3b13" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">胶囊网络:<br/>——启发:考虑网络做了哪些假设，扔掉了哪些信息。<br/>——CNN，由于其空间不变性——抛开简单与复杂物体之间的等级关系。<br/> -未来的挑战:设计与旋转工作的神经网络。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nr"><img src="../Images/e4c8add9604b8648e85224b155f7e50b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rX34vEwIJj9Y6PcTEN3iig.png"/></div></div></figure><ul class=""><li id="fb97" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">我们用CNN，改变最后的图层来应用它们。</li></ul><h1 id="4ae6" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">目标检测</h1><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ns"><img src="../Images/513fcc7425c13d0f59e60d099c68c59f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DfeDq_nsnk328DzAQDM_Qg.png"/></div></div></figure><p id="7f70" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">注意:CNN基于卷积生成了激活的像素级热图</p><p id="a3f0" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">场景理解</strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nt"><img src="../Images/702279aa9a63e0a42890b360076e8760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mQSoSREvMOnOm--SMW-etw.png"/></div></div></figure><ul class=""><li id="2294" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">目标:为真实的3D世界对2D投影中的每一个像素进行分类。</li><li id="d9f8" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">挑战:在像素级标记边界。</li><li id="1dfe" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">用例:<br/> -医疗、驾驶中物体的精确边界。<br/> -在驾驶中，标出环境的确切界限。融合传感器的数据。因此将语义知识与现实世界中的三维定位相融合。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nu"><img src="../Images/8ac70d8abf598efef53bf4bd51b56e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r5jZl30yzM9b-FRPEBerMg.png"/></div></div></figure><ul class=""><li id="5be1" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">FCN 2014: <br/> -重新调整ImageNet预训练网络的用途<br/> -用解码器取代了完全连接的层，解码器对图像进行上采样以产生热图。<br/> -跳过连接以提高上采样的粗糙度。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ca"><img src="../Images/7a48573f8572184db50ef99c312adc48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yXdBVXGusK46sjcnFiQGVQ.png"/></div></div></figure><ul class=""><li id="20ed" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">SegNet 2015: <br/> -将此应用于驾驶情境。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nv"><img src="../Images/1f83fe2c4bab04d88c2c8af0d0e3260d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qGcv0l0kil6Sujll7-9fFA.png"/></div></div></figure><ul class=""><li id="e9cd" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">扩张卷积2015: <br/> -卷积运算作为汇集运算，显著降低了分辨率。<br/>—“网格化”保持局部高分辨率纹理，同时仍然捕捉必要的空间窗口。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nw"><img src="../Images/f51dd78b615c5eef29d871209a9e0ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k3o-RXCbLa9wD2vFtDFPkg.png"/></div></div></figure><ul class=""><li id="1b44" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">DeepLab v1，v2 2016: <br/> -增加了条件随机场(CRF):通过查看底层图像强度来平滑分割的后处理。</li></ul><p id="02a7" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">细分的关键方面</strong></p><ul class=""><li id="1165" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">全卷积网络。</li><li id="73f1" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">条件随机森林。</li><li id="871b" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">扩张的脑回。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nx"><img src="../Images/f3838de0a25006444c0cdd44b633d92e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cnuqDYlIXZPgc8D0H0FCgA.png"/></div></div></figure><p id="8dfb" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">ResNet-DUC 2017:</p><ul class=""><li id="36ad" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">密集上采样卷积，而不是双线性上采样来学习上缩放特性。</li><li id="7030" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">混合扩张卷积:<br/>卷积是从输入到输出的扩散。</li><li id="6d62" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">技巧:升级特征的参数化。</li></ul><p id="f02c" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">流量网络</strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff ny"><img src="../Images/c22b26c16d3e7fbe75428ca186fcbc03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5CJii3phPuVKAmSrN9kIqg.png"/></div></div></figure><p id="5e79" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">这里讨论的方法忽略了与机器人相关的时间动力学。</p><ul class=""><li id="1715" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">流有助于近似输入图像中的每个像素如何在输出图像中移动。</li><li id="6f81" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">光流产生像素移动的方向和移动的幅度。<br/>这允许我们获取从第一帧检测到的信息，并将其向前传播。</li><li id="678e" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">这对于“涂色书注释”(1张图片90分钟)来说非常慢。</li><li id="3250" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">堆叠两个图像作为输入。</li><li id="4dba" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">单独卷积，与相关层结合。</li></ul><p id="abba" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">挑战:通过时间分割图像。</p><p id="98aa" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">FlowNet 2 2016:</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff nz"><img src="../Images/42a0c8fff0f40da7a24f7c8415091c98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5X8_jL2MNWrbyzYp9v_OYQ.png"/></div></div></figure><ul class=""><li id="ff96" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">将FlowNetC和FlowNetS结合在一起</li><li id="c881" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">产生更平滑的流场。</li><li id="1a6f" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">保留精细的运动细节。</li><li id="10dd" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">以8–140 fps的速度运行。</li><li id="3e2d" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">过程:<br/> -堆叠网络作为一种方法。<br/> -数据集的排序很重要</li></ul><h1 id="1a1d" class="kj kk hu bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">SegFuse</h1><p id="0d1f" class="pw-post-body-paragraph iu iv hu ix b iy oa ja jb jc ob je jf ju oc ji jj jv od jm jn jw oe jq jr js hn dt translated"><strong class="ix hv">数据集:</strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff of"><img src="../Images/1cb2e87ebbdfe2acb708d6ad264483e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y5gJ16VWCp7JjkxVicey7A.png"/></div></div></figure><ul class=""><li id="8745" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">8k 360度视频中1080p的行车原始视频。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff me"><img src="../Images/f1f94cd83a7cc745a17ffa966f7a6a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pGjyjfC06gEOR87FqFSMhA.png"/></div></div></figure><ul class=""><li id="5902" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">训练集的基本事实，每一帧。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff og"><img src="../Images/44bd12d3ccef1ee3a656f7b852b3ac9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZYJLm7XKhrFttvUDlzj6UA.png"/></div></div></figure><ul class=""><li id="6624" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">SOTA分段的输出</li><li id="f95a" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">光流</li></ul><p id="bda5" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated"><strong class="ix hv">任务:</strong></p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff oh"><img src="../Images/36fcbe88c7ca3bf8702198b4809beef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhTeMzitc2k-WR57BEHYlg.png"/></div></div></figure><ul class=""><li id="25cf" class="li lj hu ix b iy iz jc jd ju lk jv ll jw lm js ln lo lp lq dt translated">使用来自SOTA网络的原始视频、地面实况、分段并改进分段。</li><li id="09ab" class="li lj hu ix b iy lv jc lw ju lx jv ly jw lz js ln lo lp lq dt translated">使用网络的输出来帮助更好地传播信息。我们能找到使用时态信息的方法吗？</li></ul><blockquote class="ir is it"><p id="6d6d" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="http://twitter.com/bhutanisanyam1" rel="noopener ugc nofollow" target="_blank">你可以在Twitter @bhutanisanyam1 </a>上找到我，在<a class="ae jt" href="https://www.linkedin.com/in/sanyambhutani/" rel="noopener ugc nofollow" target="_blank"> Linkedin上联系我这里</a> <br/> <a class="ae jt" href="https://becominghuman.ai/a-self-driving-new-year-33284e592f35" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae jt" href="https://hackernoon.com/a-self-driving-new-year-2-d1bbc5a83570" rel="noopener ugc nofollow" target="_blank">这里</a>是我学习自动驾驶汽车的两篇文章</p><p id="f2a9" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="https://tinyletter.com/sanyambhutani" rel="noopener ugc nofollow" target="_blank">订阅我的时事通讯，获取深度学习、计算机视觉文章的每周精选列表</a></p></blockquote><figure class="jy jz ka kb fq kc"><div class="bz el l di"><div class="oi oj l"/></div></figure></div></div>    
</body>
</html>