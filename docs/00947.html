<html>
<head>
<title>Reinforcement Learning and Supervised Learning: A brief comparison</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习和监督学习:一个简单的比较</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/reinforcement-learning-and-supervised-learning-a-brief-comparison-1b6d68c45ffa?source=collection_archive---------5-----------------------#2018-01-30">https://medium.com/hackernoon/reinforcement-learning-and-supervised-learning-a-brief-comparison-1b6d68c45ffa?source=collection_archive---------5-----------------------#2018-01-30</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="83a1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt jp translated"><span class="l jq jr js bm jt ju jv jw jx di"> M </span>机器<a class="ae jy" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习的ost初学者</a>从学习分类、回归等监督学习技术开始。然而，机器学习中最重要的范例之一是<a class="ae jy" href="https://hackernoon.com/tagged/reinforcement" rel="noopener ugc nofollow" target="_blank">强化</a>学习(RL)，它能够处理许多具有挑战性的任务。一个例子是围棋，它是由一个RL代理人玩的，这个代理人设法打败了世界上最好的棋手。</p><p id="08e3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">许多人听说过RL，但实际上并不知道它与监督学习有什么不同。他们对这两种范式以及它们共存的原因感到困惑。这篇文章旨在澄清差异，并介绍深度学习如何融入其中。</p><p id="1a86" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">让我们从最重要的问题开始:我们为什么要关心RL？</p><h2 id="3bf4" class="jz ka hu bd kb kc kd ke kf kg kh ki kj jc kk kl km jg kn ko kp jk kq kr ks kt dt translated">为什么首先是RL？</h2><p id="9cc1" class="pw-post-body-paragraph ir is hu it b iu ku iw ix iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo hn dt translated">监督学习可以解决许多有趣的问题，从分类图像到翻译文本。现在让我们看看像玩游戏或教一个机器人肢体抓取物体这样的问题。为什么我们不能用监督学习来恰当地做到这一点？</p><p id="4628" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">考虑下围棋的情况。假设我们有一个数据集，其中包含了人类下的所有围棋的历史。然后，我们可以使用游戏状态作为输入<em class="kz"> X </em>，并使用该状态的最优移动作为输出标签<em class="kz"> Y </em>。理论上，这听起来不错，但在实践中会出现一些问题。</p><p id="dca3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">1.像这样的数据集并不存在于我们关心的所有领域</p><p id="ac9f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">2.创建这样的数据集可能是昂贵且不可行的</p><p id="2dfe" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">3.该方法学习模仿人类专家，而不是实际学习可能的最佳策略</p><p id="40af" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">RL来拯救我们了。直觉上，RL试图通过<strong class="it hv">试错</strong>来学习动作。我们通过对行动进行取样，然后观察哪一个导致我们想要的结果，来学习最佳策略。与受监督的方法相反，我们不是从一个标签，而是从一个被称为<strong class="it hv">奖励</strong>的延时标签中学习这个最优行动。这个标量值告诉我们无论我们做了什么，结果是好是坏。因此，RL的目标是采取行动以<strong class="it hv">最大化奖励</strong>。</p><h2 id="3a92" class="jz ka hu bd kb kc kd ke kf kg kh ki kj jc kk kl km jg kn ko kp jk kq kr ks kt dt translated">问题的正式定义是什么？</h2><p id="e19b" class="pw-post-body-paragraph ir is hu it b iu ku iw ix iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo hn dt translated">在数学上，RL问题可以被视为<strong class="it hv">马尔可夫决策过程</strong>。这个过程是无记忆的，所以我们所关心的一切都是通过当前状态知道的。RL设置可以如下图所示:</p><figure class="lb lc ld le fq lf fe ff paragraph-image"><div class="fe ff la"><img src="../Images/8b313acfc94a86fb283e343c662c1550.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*FlK1JS3vFhQasvuEgLU3Bg.png"/></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">Source: CS 294 Deep Reinforcement Learning (UC Berkeley)</figcaption></figure><p id="3c8c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在一个环境中有一个代理人采取行动，并反过来接受奖励。让我们简单回顾一下监督学习任务，以澄清两者的区别。</p><p id="4de2" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在监督学习中，给定一堆输入数据<em class="kz"> X </em>和标签<em class="kz"> Y </em>，我们正在学习一个函数<em class="kz"> f: X → Y </em>，该函数将<em class="kz"> X </em>(例如图像)映射到<em class="kz"> Y </em>(例如类别标签)。如果训练过程收敛，该函数将能够根据新的输入数据以一定的精度预测<em class="kz"> Y </em>。</p><p id="f2f5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在让我们继续RL设置，它是由5元组<em class="kz"> (S,A,P,R,𝛾).)定义的</em>我们得到一组状态<em class="kz"> S </em>和一组动作<em class="kz"> A </em>。<em class="kz"> P </em>是状态转移概率。奖励是一个值，它告诉我们在我们想要优化的目标方面做得有多好。它由一个奖励函数<em class="kz"> R: S×A → R </em>给出。我们一会儿会去𝛾。</p><p id="3b64" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">任务是学习一个从状态映射到动作的函数<em class="kz"> π: S → A </em>。该功能称为<strong class="it hv">策略功能</strong>。现在的目标是找到一个最佳的政策，最大化预期的回报。这也叫做<strong class="it hv">控制问题</strong>。</p><p id="1e2a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">围棋游戏可以用这种方法按以下方式建模:</p><p id="bc30" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">状态</strong>:所有部件的位置</p><p id="d072" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">动作</strong>:玩家放下棋子的地方</p><p id="b61f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">奖励</strong>:如果玩家在游戏结束时获胜，奖励1，否则奖励0</p><h2 id="70c8" class="jz ka hu bd kb kc kd ke kf kg kh ki kj jc kk kl km jg kn ko kp jk kq kr ks kt dt translated">我们如何解读奖励？</h2><p id="033b" class="pw-post-body-paragraph ir is hu it b iu ku iw ix iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo hn dt translated">RL中奖励的问题是，我们不知道哪个行为对结果有决定性的影响。是我们之前做的三个动作的移动还是当前的？我们称之为<strong class="it hv">信用分配问题</strong>。</p><p id="35a5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了解决这个问题，引入贴现因子𝛾 ∈ (0，1)来计算最优策略π*。我们的优化问题是最大化<strong class="it hv">折扣</strong>奖励的期望和。因此，可以通过计算该等式的结果来找到最佳策略:</p><figure class="lb lc ld le fq lf fe ff paragraph-image"><div class="fe ff lm"><img src="../Images/de33ac789780a8ead17fc47e726c9f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*Fa3H52TPndagwDn9WI_uIg.png"/></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">The optimal policy function</figcaption></figure><p id="4824" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">直觉上，我们责怪每一个行为，假设它的影响对未来有指数衰减的影响。</p><p id="da02" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了学习最优策略，有不同的方法，例如策略梯度和Q学习。当策略梯度试图直接学习策略时，Q-学习是学习状态-动作对的函数。我将在以后的文章中详细解释这些算法。</p><h2 id="8d0e" class="jz ka hu bd kb kc kd ke kf kg kh ki kj jc kk kl km jg kn ko kp jk kq kr ks kt dt translated">我们为什么要在RL中使用深度学习？</h2><p id="cb37" class="pw-post-body-paragraph ir is hu it b iu ku iw ix iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo hn dt translated">在监督学习中，我们使用深度学习，因为手动设计图像或文本等非结构化数据的特征是不可行的。在RL中，我们使用深度学习很大程度上也是出于同样的原因。有了神经网络，RL问题可以在不需要太多领域知识的情况下解决。</p><p id="8eaf" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了举例说明这一点，考虑乒乓游戏。在传统的学习中，我们需要从游戏位置中提取特征来获得有意义的信息。使用神经网络，我们可以将<strong class="it hv">原始游戏像素</strong>输入算法，并让它创建数据的高级非线性表示。</p><p id="d81d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了做到这一点，我们构建了一个经过端到端训练的策略网络<strong class="it hv">，</strong>，这意味着我们输入我们的游戏状态，然后得出我们可能采取的行动的概率分布。</p><p id="e4dd" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果我们以Pong为例，动作要么向上，要么向下。这是来自<a class="ae jy" href="http://karpathy.github.io/2016/05/31/rl/" rel="noopener ugc nofollow" target="_blank">学习如何玩乒乓</a>的示例设置:</p><figure class="lb lc ld le fq lf fe ff paragraph-image"><div class="fe ff ln"><img src="../Images/7f56b36e45a1d2a07c09c69f057af441.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*IrprEH9sFOrsu06eNM_3Ng.png"/></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">Source: Andrej Karpathy’s blog</figcaption></figure><p id="e2cd" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">乍一看，这可能与典型的监督学习设置方式相同，例如用于图像分类。然而，提醒你自己，我们没有给每个游戏状态的标签，因此我们不能简单地训练这个网络。</p><h2 id="85e3" class="jz ka hu bd kb kc kd ke kf kg kh ki kj jc kk kl km jg kn ko kp jk kq kr ks kt dt translated">结论</h2><p id="21ea" class="pw-post-body-paragraph ir is hu it b iu ku iw ix iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo hn dt translated">我希望这篇文章能让你对监督学习和RL之间的区别有一个更好的直觉。这两种方法都有其适当的位置，并且有许多成功的故事。在未来，我将更深入地解释RL系统是如何训练的。对于任何想了解更多的人，我附上了一些我个人认为有用的资源。</p><h2 id="0216" class="jz ka hu bd kb kc kd ke kf kg kh ki kj jc kk kl km jg kn ko kp jk kq kr ks kt dt translated">进一步阅读</h2><p id="3878" class="pw-post-body-paragraph ir is hu it b iu ku iw ix iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo hn dt translated"><a class="ae jy" href="https://arxiv.org/abs/1701.07274" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1701.07274</a></p><p id="0a0f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><a class="ae jy" href="http://karpathy.github.io/2016/05/31/rl/" rel="noopener ugc nofollow" target="_blank">http://karpathy.github.io/2016/05/31/rl/</a></p><p id="0975" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><a class="ae jy" href="https://www.youtube.com/watch?v=lvoHnicueoE" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=lvoHnicueoE</a></p><p id="7a1d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><a class="ae jy" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="noopener ugc nofollow" target="_blank">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a></p><figure class="lb lc ld le fq lf"><div class="bz el l di"><div class="lo lp l"/></div></figure><figure class="lb lc ld le fq lf"><div class="bz el l di"><div class="lq lp l"/></div></figure></div></div>    
</body>
</html>