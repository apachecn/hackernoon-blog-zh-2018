<html>
<head>
<title>ARKit 101: How to Build Augmented Reality (AR) based resume using Face Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ARKit 101:如何使用人脸识别构建基于增强现实(AR)的简历</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/arkit-101-how-to-build-augmented-reality-ar-based-resume-using-face-recognition-b28941aee2fb?source=collection_archive---------7-----------------------#2018-02-06">https://medium.com/hackernoon/arkit-101-how-to-build-augmented-reality-ar-based-resume-using-face-recognition-b28941aee2fb?source=collection_archive---------7-----------------------#2018-02-06</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="20d6" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">苹果公司宣布，<a class="ae jp" href="https://hackernoon.com/tagged/arkit" rel="noopener ugc nofollow" target="_blank"> ARKit </a>将于6月在2017年WWDC举行的公司活动上在<a class="ae jp" href="https://hackernoon.com/tagged/ios" rel="noopener ugc nofollow" target="_blank"> iOS </a> 11上发布，随着2017年9月19日iOS 11的发布，ARKit成为其中的一部分。用户可以下载包含iOS 11的Xcode版本9.0.1，并开始创建基于增强现实的项目。</p><h1 id="20c0" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">什么是ARKit？</h1><p id="fd52" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">ARKit通过结合设备运动跟踪、场景处理、相机场景捕捉和显示便利性，简化了制作AR体验的任务。增强现实(AR)将2D或3D对象添加到相机视图或实时视图中，以便这些对象看起来像是真实世界的一部分。您可以使用ARKit功能在您的应用程序或游戏中产生AR体验。AR游戏在人群中非常受欢迎，如Pokemon go，僵尸快跑，Ingress等。</p><p id="17d3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">ARKit使用遵循右手惯例的世界和相机坐标，这意味着x轴向右，y轴向上，z轴指向观众。为了追踪世界坐标，ARKit使用了一种称为视觉惯性里程计的技术，这种技术是从iOS设备的运动感应硬件合并的信息与手机摄像头可见场景的视觉分析的结合。世界追踪也分析和理解场景的内容。使用击中测试方法，它可以识别平面水平或垂直的相机图像和跟踪其位置和大小。</p><p id="1e41" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">世界跟踪并不总是给你准确的指标，因为它依赖于设备的物理环境，而这并不总是一致或难以衡量。为了AR体验，在相机视图中映射真实世界时，总会有一定程度的误差。为了构建高质量的AR体验，我们需要考虑以下因素:</p><ul class=""><li id="7496" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated"><strong class="it hv">为可预测的照明条件设计ar体验:</strong>世界追踪需要清晰的图像分析，为了提高AR体验的质量，我们需要设计一款能够更好地分析细节的照明应用。</li><li id="06d5" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated"><strong class="it hv">使用追踪质量信息提供用户反馈:</strong>当设备运动与清晰图像结合时，ARKit可以提供更好的反馈。利用这一点，可以指导用户如何解决低质量的跟踪情况。</li><li id="bcdc" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated"><strong class="it hv">为平面检测留出时间以产生清晰的结果，当您获得所需的结果时，禁用平面检测</strong>。ARKit基于随时间推移的平面检测来改进其位置和范围。飞机第一次检测到位置及其范围可能不准确，但ARKit会随着时间的推移了解飞机何时留在场景中。</li></ul><h1 id="7f5a" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">ARKit术语</h1><ul class=""><li id="dd22" class="kt ku hu it b iu ko iy kp jc lh jg li jk lj jo ky kz la lb dt translated">SceneKit视图:它是interface builder对象库中的一个组件，主要用于3D图形渲染，类似于GLKit视图，允许您快速设置场景的许多定义属性</li><li id="6b76" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">ARSCNView:ARSCNView是一个类，它是一个SceneKit视图，包含一个<a class="ae jp" href="https://developer.apple.com/documentation/arkit/arsession" rel="noopener ugc nofollow" target="_blank"> ARSession </a>对象，该对象管理创建增强现实(AR)体验所需的运动跟踪和图像处理。</li><li id="e496" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">ARSession: ARSession对象管理运动跟踪和图像处理</li><li id="ed1e" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">ARWorldTrackingConfiguration:ARWorldTrackingConfiguration类提供高精度的运动跟踪，并启用一些功能来帮助您相对于现实世界的表面放置虚拟内容。</li><li id="c17c" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">SCNNode:场景图形的结构元素，表示3D坐标空间中的位置和变换，您可以将几何图形、灯光、摄像机或其他可显示的内容附加到它上面。</li><li id="709d" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">SCNPlane:指定宽度和高度的矩形单面平面几何体。</li></ul><h1 id="d385" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">概观</h1><p id="f6c1" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">在这篇博客中，我将解释如何快速开始创建增强现实(AR)应用程序，并使用面部识别建立AR体验。AR应用程序可以识别你的脸，并在相机视图中显示你的3D模拟版本和你的专业信息。应用程序中使用的组件有:</p><ul class=""><li id="dff8" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated">iOS视觉模块</li><li id="eba7" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">IBM Watson视觉识别API</li><li id="e56f" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">iOS 11 ARKit</li><li id="0dca" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">IBM Cloudant数据库来存储信息。</li><li id="a278" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">Xcode版本高于9.0.1</li></ul><h1 id="2894" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">体系结构</h1><figure class="ll lm ln lo fq lp fe ff paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="fe ff lk"><img src="../Images/113bf769c60170a3fae8246009520fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7D8YoTGbPflOb97-.jpg"/></div></div></figure><h1 id="2b50" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">如何</h1><h1 id="f851" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">步骤1:在Xcode中创建项目</h1><p id="1949" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">如下图所示，从Xcode创建一个增强现实应用程序。</p><figure class="ll lm ln lo fq lp fe ff paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="fe ff lw"><img src="../Images/c4e0390c57e790fd00cf17967b0853dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3LIJbexD_qYOHD30.png"/></div></div></figure><h1 id="77d9" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">步骤2:配置和运行AR会话</h1><p id="7ead" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">一旦项目设置完毕，我们需要配置并运行AR会话。已经设置了一个包含ARSession对象的ARSCNView。ARSession对象管理运动跟踪和图像处理。要运行此会话，我们需要向会话添加ARWorldTrackingConfiguration。以下代码使用配置设置会话并运行:</p><pre class="ll lm ln lo fq lx ly lz ma aw mb dt"><span id="215a" class="mc jr hu ly b fv md me l mf mg">@IBOutlet var sceneView: ARSCNView!</span><span id="1451" class="mc jr hu ly b fv mh me l mf mg">override func viewWillAppear(_ animated: Bool) {</span><span id="1aa2" class="mc jr hu ly b fv mh me l mf mg">super.viewWillAppear(animated)</span><span id="320c" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Create a session configuration</em></span><span id="21ac" class="mc jr hu ly b fv mh me l mf mg">let configuration = ARWorldTrackingConfiguration()</span><span id="7acf" class="mc jr hu ly b fv mh me l mf mg">configuration.planeDetection = .horizontal</span><span id="5f15" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Run the view’s session</em></span><span id="9588" class="mc jr hu ly b fv mh me l mf mg">sceneView.session.run(configuration)</span><span id="92bb" class="mc jr hu ly b fv mh me l mf mg">}</span></pre><p id="a694" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">以上代码将平面检测配置添加到水平，并运行会话。</p><p id="d844" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">重要提示</strong>:如果你的应用需要arkit作为其核心功能，使用你的应用的Info.plist文件的<a class="ae jp" href="https://developer.apple.com/library/content/documentation/General/Reference/InfoPlistKeyReference/Articles/iPhoneOSKeys.html#//apple_ref/doc/uid/TP40009252-SW3" rel="noopener ugc nofollow" target="_blank">UIRequiredDeviceCapabilities</a>部分中的ARKit键，使你的应用只在支持ARKit的设备上可用。如果AR是你的应用的次要功能，使用<a class="ae jp" href="https://developer.apple.com/documentation/arkit/arconfiguration/2923553-issupported" rel="noopener ugc nofollow" target="_blank"> isSupported </a>属性来决定是否提供基于AR的功能。</p><h1 id="86f3" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">步骤3:向检测到的平面添加3D内容</h1><p id="35f6" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">一旦设置了ARSession，您就可以使用SceneKit在视图中放置虚拟内容。该项目有一个名为ship.scn的示例文件，您可以将它放在资产目录的视图中。以下代码将3D对象添加到SCNView中:</p><pre class="ll lm ln lo fq lx ly lz ma aw mb dt"><span id="678d" class="mc jr hu ly b fv md me l mf mg"><em class="mi">// Create a new scene<br/></em>let scene =SCNScene(named:”art.scnassets/ship.scn”)!</span><span id="a90f" class="mc jr hu ly b fv mh me l mf mg">Setthe scene to theview</span><span id="c8af" class="mc jr hu ly b fv mh me l mf mg">sceneView.scene= scene</span></pre><p id="806b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">以下代码的输出将为您提供一个3D船对象到真实世界的视图。</p><figure class="ll lm ln lo fq lp fe ff paragraph-image"><div class="fe ff mj"><img src="../Images/fccd12a2114aff6f58e42b6b9d2dfc91.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*yTBmO92J2JVzBHDj.jpg"/></div></figure><h1 id="ddab" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">步骤4:使用视觉API进行人脸检测</h1><p id="0d36" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">一旦您测试了3D在摄像机视图中的工作情况，我们就可以使用vision API设置面部检测了。vision API将检测面部，裁剪面部，并将文件发送到IBM视觉识别API以对面部进行分类。</p><pre class="ll lm ln lo fq lx ly lz ma aw mb dt"><span id="6c26" class="mc jr hu ly b fv md me l mf mg"><em class="mi">// MARK: — Face detections</em></span><span id="1d85" class="mc jr hu ly b fv mh me l mf mg">private func faceObservation() -&gt; Observable&lt;[(observation: VNFaceObservation, image: CIImage, frame: ARFrame)]&gt; {</span><span id="171c" class="mc jr hu ly b fv mh me l mf mg">return Observable&lt;[(observation: VNFaceObservation, image: CIImage, frame: ARFrame)]&gt;.create{ observer in</span><span id="469f" class="mc jr hu ly b fv mh me l mf mg">guard let frame = self.sceneView.session.currentFrame else {</span><span id="7ce7" class="mc jr hu ly b fv mh me l mf mg">print(“No frame available”)</span><span id="9a79" class="mc jr hu ly b fv mh me l mf mg">observer.onCompleted()</span><span id="8c8b" class="mc jr hu ly b fv mh me l mf mg">return Disposables.create()</span><span id="d0d2" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="1b59" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Create and rotate image</em></span><span id="9975" class="mc jr hu ly b fv mh me l mf mg">let image = CIImage.init(cvPixelBuffer: frame.capturedImage).rotate</span><span id="4a76" class="mc jr hu ly b fv mh me l mf mg">let facesRequest = VNDetectFaceRectanglesRequest { request, error in</span><span id="35d6" class="mc jr hu ly b fv mh me l mf mg">guard error == nil else {</span><span id="c8de" class="mc jr hu ly b fv mh me l mf mg">print(“Face request error: \(error!.localizedDescription)”)</span><span id="4f26" class="mc jr hu ly b fv mh me l mf mg">observer.onCompleted()</span><span id="94a5" class="mc jr hu ly b fv mh me l mf mg">return</span><span id="00ba" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="4983" class="mc jr hu ly b fv mh me l mf mg">guard let observations = request.results as? [VNFaceObservation] else {</span><span id="7e42" class="mc jr hu ly b fv mh me l mf mg">print(“No face observations”)</span><span id="690c" class="mc jr hu ly b fv mh me l mf mg">observer.onCompleted()</span><span id="6624" class="mc jr hu ly b fv mh me l mf mg">return</span><span id="2b42" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="8fef" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Map response</em></span><span id="5ee4" class="mc jr hu ly b fv mh me l mf mg">let response = observations.map({ (face) -&gt; (observation: VNFaceObservation, image: CIImage, frame: ARFrame) in</span><span id="21d1" class="mc jr hu ly b fv mh me l mf mg">return (observation: face, image: image, frame: frame)</span><span id="169a" class="mc jr hu ly b fv mh me l mf mg">})</span><span id="2f86" class="mc jr hu ly b fv mh me l mf mg">observer.onNext(response)</span><span id="3e04" class="mc jr hu ly b fv mh me l mf mg">observer.onCompleted()</span><span id="0669" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="53f9" class="mc jr hu ly b fv mh me l mf mg">try? VNImageRequestHandler(ciImage: image).perform([facesRequest])</span><span id="294a" class="mc jr hu ly b fv mh me l mf mg">return Disposables.create()</span><span id="64e0" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="7a7c" class="mc jr hu ly b fv mh me l mf mg">}</span></pre><h1 id="039d" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">步骤5:使用IBM视觉识别API对人脸进行分类</h1><p id="07bb" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">使用IBM视觉识别API，您可以从上面上传裁剪后的人脸，API将进行分类并向您发送一个JSON响应。要使用IBM Watson视觉识别API，您可以注册到IBM Bluemix控制台并创建一个视觉识别服务。然后，您应该能够创建凭证，您可以在调用API时使用这些凭证。您可以在应用程序中使用Watson SDK来使用VisualRecognitionV3 API。在这里的<a class="ae jp" href="https://github.com/watson-developer-cloud/swift-sdk" rel="noopener ugc nofollow" target="_blank">上执行以下指令。</a></p><pre class="ll lm ln lo fq lx ly lz ma aw mb dt"><span id="a966" class="mc jr hu ly b fv md me l mf mg">private func faceClassification(face: VNFaceObservation, image: CIImage, frame: ARFrame) -&gt; Observable&lt;(classes: [ClassifiedImage], position: SCNVector3, frame: ARFrame)&gt; {</span><span id="de01" class="mc jr hu ly b fv mh me l mf mg">return Observable&lt;(classes: [ClassifiedImage], position: SCNVector3, frame: ARFrame)&gt;.create{ observer in</span><span id="1b0e" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Determine position of the face</em></span><span id="dbcc" class="mc jr hu ly b fv mh me l mf mg">let boundingBox = self.transformBoundingBox(face.boundingBox)</span><span id="4267" class="mc jr hu ly b fv mh me l mf mg">guard let worldCoord = self.normalizeWorldCoord(boundingBox) else {</span><span id="56d8" class="mc jr hu ly b fv mh me l mf mg">print(“No feature point found”)</span><span id="fd87" class="mc jr hu ly b fv mh me l mf mg">observer.onCompleted()</span><span id="b5bb" class="mc jr hu ly b fv mh me l mf mg">return Disposables.create()</span><span id="63bf" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="6a94" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Create Classification request</em></span><span id="ad8b" class="mc jr hu ly b fv mh me l mf mg">let fileName = self.randomString(length: 20) + “.png”</span><span id="480a" class="mc jr hu ly b fv mh me l mf mg">let pixel = image.cropImage(toFace: face)</span><span id="2380" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">//convert the cropped image to UI image</em></span><span id="32b9" class="mc jr hu ly b fv mh me l mf mg">let imagePath = FileManager.default.temporaryDirectory.appendingPathComponent(fileName)</span><span id="5994" class="mc jr hu ly b fv mh me l mf mg">let uiImage: UIImage = self.convert(cmage: pixel)</span><span id="ca43" class="mc jr hu ly b fv mh me l mf mg">if let data = UIImagePNGRepresentation(uiImage) {</span><span id="4e0a" class="mc jr hu ly b fv mh me l mf mg">try? data.write(to: imagePath)</span><span id="160f" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="87c7" class="mc jr hu ly b fv mh me l mf mg">let visualRecognition = VisualRecognition.init(apiKey: Credentials.VR_API_KEY, version: Credentials.VERSION)</span><span id="7f1e" class="mc jr hu ly b fv mh me l mf mg">let failure = { (error: Error) in print(error) }</span><span id="0714" class="mc jr hu ly b fv mh me l mf mg">let owners = [“me”]</span><span id="5b83" class="mc jr hu ly b fv mh me l mf mg">visualRecognition.classify(imageFile: imagePath, owners: owners, threshold: 0, failure: failure){ classifiedImages in</span><span id="e726" class="mc jr hu ly b fv mh me l mf mg">print(classifiedImages)</span><span id="0f8d" class="mc jr hu ly b fv mh me l mf mg">observer.onNext((classes: classifiedImages.images, position: worldCoord, frame: frame))</span><span id="689a" class="mc jr hu ly b fv mh me l mf mg">observer.onCompleted()</span><span id="f867" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="84d1" class="mc jr hu ly b fv mh me l mf mg">return Disposables.create()</span><span id="458f" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="e994" class="mc jr hu ly b fv mh me l mf mg">}</span></pre><h1 id="f95f" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">步骤6:更新节点以放置3D内容和文本</h1><p id="cc2d" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">一旦人脸被视觉识别API分类，API的响应就是一个JSON。视觉识别的响应有一个分类id，然后使用它从IBM cloudant数据库中获得更多关于分类的信息。使用分类id检索数据，JSON响应如下所示:</p><pre class="ll lm ln lo fq lx ly lz ma aw mb dt"><span id="1464" class="mc jr hu ly b fv md me l mf mg">{</span><span id="2711" class="mc jr hu ly b fv mh me l mf mg">“_id”: “c2554847ec99e05ffa8122994f1f1cb4”,</span><span id="4cb6" class="mc jr hu ly b fv mh me l mf mg">“_rev”: “3-d69a8b26c103a048b5e366c4a6dbeed7”,</span><span id="eed1" class="mc jr hu ly b fv mh me l mf mg">“classificationId”: “SanjeevGhimire_334732802”,</span><span id="2c4b" class="mc jr hu ly b fv mh me l mf mg">“fullname”: “Sanjeev Ghimire”,</span><span id="f429" class="mc jr hu ly b fv mh me l mf mg">“linkedin”: “https://www.linkedin.com/in/sanjeev-ghimire-8534854/",</span><span id="0aa1" class="mc jr hu ly b fv mh me l mf mg">“twitter”: “https://twitter.com/sanjeevghimire",</span><span id="9a89" class="mc jr hu ly b fv mh me l mf mg">“facebook”: “https://www.facebook.com/sanjeev.ghimire",</span><span id="9bfa" class="mc jr hu ly b fv mh me l mf mg">“phone”: “1–859–684–7931”,</span><span id="a2b2" class="mc jr hu ly b fv mh me l mf mg">“location”: “Austin, TX”</span><span id="9cbc" class="mc jr hu ly b fv mh me l mf mg">}</span></pre><p id="e330" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然后我们可以用这些细节更新SCNNode作为子节点。SCNNode是<code class="eh mk ml mm ly b">A structural element of a scene graph, representing a position and transform in a 3D coordinate space, to which you can attach geometry, lights, cameras, or other displayable content </code>。对于每个子节点，我们需要定义它的字体、对齐方式和材料。材质包括3D内容的属性，如漫反射内容颜色、镜面反射内容颜色、双面等。例如，要显示上述JSON中的全名，可以将数组添加到SCNNode中，如下所示:</p><pre class="ll lm ln lo fq lx ly lz ma aw mb dt"><span id="8f48" class="mc jr hu ly b fv md me l mf mg">let fullName = profile[“fullname”].stringValue</span><span id="e4f5" class="mc jr hu ly b fv mh me l mf mg">let fullNameBubble = SCNText(string: fullName, extrusionDepth: CGFloat(bubbleDepth))</span><span id="1540" class="mc jr hu ly b fv mh me l mf mg">fullNameBubble.font = UIFont(name: “Times New Roman”, size: 0.10)?.withTraits(traits: .traitBold)</span><span id="94ad" class="mc jr hu ly b fv mh me l mf mg">fullNameBubble.alignmentMode = kCAAlignmentCenter</span><span id="8679" class="mc jr hu ly b fv mh me l mf mg">fullNameBubble.firstMaterial?.diffuse.contents = UIColor.black</span><span id="ba93" class="mc jr hu ly b fv mh me l mf mg">fullNameBubble.firstMaterial?.specular.contents = UIColor.white</span><span id="9f1d" class="mc jr hu ly b fv mh me l mf mg">fullNameBubble.firstMaterial?.isDoubleSided = true</span><span id="874f" class="mc jr hu ly b fv mh me l mf mg">fullNameBubble.chamferRadius = CGFloat(bubbleDepth)</span><span id="33c6" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// fullname BUBBLE NODE</em></span><span id="6de0" class="mc jr hu ly b fv mh me l mf mg">let (minBound, maxBound) = fullNameBubble.boundingBox</span><span id="abcf" class="mc jr hu ly b fv mh me l mf mg">let fullNameNode = SCNNode(geometry: fullNameBubble)</span><span id="381d" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Centre Node — to Centre-Bottom point</em></span><span id="f8d8" class="mc jr hu ly b fv mh me l mf mg">fullNameNode.pivot = SCNMatrix4MakeTranslation( (maxBound.x — minBound.x)/2, minBound.y, bubbleDepth/2)</span><span id="560c" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Reduce default text size</em></span><span id="8e50" class="mc jr hu ly b fv mh me l mf mg">fullNameNode.scale = SCNVector3Make(0.1, 0.1, 0.1)</span><span id="e1a4" class="mc jr hu ly b fv mh me l mf mg">fullNameNode.simdPosition = simd_float3.init(x: 0.1, y: 0.06, z: 0)</span></pre><p id="2263" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">要更新SCNNode:</p><pre class="ll lm ln lo fq lx ly lz ma aw mb dt"><span id="9d5f" class="mc jr hu ly b fv md me l mf mg">private func updateNode(classes: [ClassifiedImage], position: SCNVector3, frame: ARFrame) {</span><span id="6ddf" class="mc jr hu ly b fv mh me l mf mg">guard let person = classes.first else {</span><span id="7d9f" class="mc jr hu ly b fv mh me l mf mg">print(“No classification found”)</span><span id="ffe0" class="mc jr hu ly b fv mh me l mf mg">return</span><span id="0014" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="8fcc" class="mc jr hu ly b fv mh me l mf mg">let classifier = person.classifiers.first</span><span id="c38e" class="mc jr hu ly b fv mh me l mf mg">let name = classifier?.name</span><span id="9f5f" class="mc jr hu ly b fv mh me l mf mg">let classifierId = classifier?.classifierID</span><span id="dd0b" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Filter for existent face</em></span><span id="6e55" class="mc jr hu ly b fv mh me l mf mg">let results = self.faces.filter{ $0.name == name &amp;&amp; $0.timestamp != frame.timestamp }</span><span id="0c43" class="mc jr hu ly b fv mh me l mf mg">.sorted{ $0.node.position.distance(toVector: position) &lt; $1.node.position.distance(toVector: position) }</span><span id="1865" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Create new face</em></span><span id="7357" class="mc jr hu ly b fv mh me l mf mg">guard let existentFace = results.first else {</span><span id="79bb" class="mc jr hu ly b fv mh me l mf mg">CloudantRESTCall().getResumeInfo(classificationId: classifierId!) { (resultJSON) in</span><span id="25ca" class="mc jr hu ly b fv mh me l mf mg">let node = SCNNode.init(withJSON: resultJSON[“docs”][0], position: position)</span><span id="616e" class="mc jr hu ly b fv mh me l mf mg">DispatchQueue.main.async {</span><span id="1ae6" class="mc jr hu ly b fv mh me l mf mg">self.sceneView.scene.rootNode.addChildNode(node)</span><span id="d3ed" class="mc jr hu ly b fv mh me l mf mg">node.show()</span><span id="ca76" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="3a95" class="mc jr hu ly b fv mh me l mf mg">let face = Face.init(name: name!, node: node, timestamp: frame.timestamp)</span><span id="ff6b" class="mc jr hu ly b fv mh me l mf mg">self.faces.append(face)</span><span id="df18" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="3e1c" class="mc jr hu ly b fv mh me l mf mg">return</span><span id="bfcd" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="2352" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Update existent face</em></span><span id="159f" class="mc jr hu ly b fv mh me l mf mg">DispatchQueue.main.async {</span><span id="0648" class="mc jr hu ly b fv mh me l mf mg"><em class="mi">// Filter for face that’s already displayed</em></span><span id="aa17" class="mc jr hu ly b fv mh me l mf mg">if let displayFace = results.filter({ !$0.hidden }).first {</span><span id="e0d1" class="mc jr hu ly b fv mh me l mf mg">let distance = displayFace.node.position.distance(toVector: position)</span><span id="9899" class="mc jr hu ly b fv mh me l mf mg">if(distance &gt;= 0.03 ) {</span><span id="2fd0" class="mc jr hu ly b fv mh me l mf mg">displayFace.node.move(position)</span><span id="f024" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="af54" class="mc jr hu ly b fv mh me l mf mg">displayFace.timestamp = frame.timestamp</span><span id="5933" class="mc jr hu ly b fv mh me l mf mg">} else {</span><span id="ebe4" class="mc jr hu ly b fv mh me l mf mg">existentFace.node.position = position</span><span id="3b6e" class="mc jr hu ly b fv mh me l mf mg">existentFace.node.show()</span><span id="0912" class="mc jr hu ly b fv mh me l mf mg">existentFace.timestamp = frame.timestamp</span><span id="07f3" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="da77" class="mc jr hu ly b fv mh me l mf mg">}</span><span id="f7a5" class="mc jr hu ly b fv mh me l mf mg">}</span></pre><h1 id="3df0" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">来源</h1><p id="637f" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">你可以在这里找到GitHub链接<a class="ae jp" href="https://github.com/sanjeevghimire/ARBasedResumeWithFaceRecognition" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="6d7f" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">演示</h1><figure class="ll lm ln lo fq lp"><div class="bz el l di"><div class="mn mo l"/></div></figure><p id="a8b3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这个输出显示了一个模拟的3D人脸和关于这个人的专业细节。</p><h1 id="ecda" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">结论</h1><p id="a423" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">随着ARKit在iOS 11上的发布，有无限的机会来构建将虚拟数据映射到现实世界场景的解决方案。就我个人而言，我认为增强现实是市场上的一项新兴技术，来自不同行业的开发人员正在不同的应用程序上进行试验，如游戏、建筑、航空等。随着时间的推移，增强现实将变得成熟，我认为在可预见的未来，这将是科技行业的另一件事。</p><h1 id="a990" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">油管（国外视频网站）</h1><figure class="ll lm ln lo fq lp"><div class="bz el l di"><div class="mp mo l"/></div></figure><h1 id="a602" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">参考链接</h1><ul class=""><li id="050e" class="kt ku hu it b iu ko iy kp jc lh jg li jk lj jo ky kz la lb dt translated"><a class="ae jp" href="https://github.com/watson-developer-cloud/swift-sdk" rel="noopener ugc nofollow" target="_blank">沃森软件开发套件</a></li><li id="67c5" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated"><a class="ae jp" href="https://github.com/NovaTecConsulting/FaceRecognition-in-ARKit" rel="noopener ugc nofollow" target="_blank">人脸识别模块</a></li><li id="561f" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated"><a class="ae jp" href="https://developer.apple.com/arkit/" rel="noopener ugc nofollow" target="_blank"> ARKit iOS 11 </a></li><li id="a64f" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated"><a class="ae jp" href="https://www.ibm.com/watson/services/visual-recognition/" rel="noopener ugc nofollow" target="_blank"> IBM视觉识别</a></li><li id="30ac" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated"><a class="ae jp" href="https://www.ibm.com/cloud/cloudant" rel="noopener ugc nofollow" target="_blank"> IBM Cloudant数据库</a></li><li id="267f" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated"><a class="ae jp" href="https://github.com/sanjeevghimire/ARBasedResumeWithFaceRecognition" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li></ul><figure class="ll lm ln lo fq lp"><div class="bz el l di"><div class="mq mo l"/></div></figure></div></div>    
</body>
</html>