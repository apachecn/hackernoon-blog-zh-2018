<html>
<head>
<title>Transfer Learning for Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的迁移学习</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/transfer-learning-for-natural-language-processing-bb4669d1c1ff?source=collection_archive---------9-----------------------#2018-02-18">https://medium.com/hackernoon/transfer-learning-for-natural-language-processing-bb4669d1c1ff?source=collection_archive---------9-----------------------#2018-02-18</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="b6b1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">迁移学习</strong>旨在利用<strong class="it hv">源领域</strong>中有价值的知识来帮助<strong class="it hv">目标领域中的绩效建模。</strong></p><h1 id="f1e8" class="jp jq hu bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km dt translated">为什么NLP需要迁移学习？</h1><p id="e95f" class="pw-post-body-paragraph ir is hu it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hn dt translated">在NLP应用中，特别是当我们没有足够大的数据集来解决一个任务(称为<strong class="it hv"> <em class="ks">目标</em>任务T </strong>)时，我们希望从其他<strong class="it hv">任务S </strong>中转移知识，以避免过拟合并提高T的性能。</p><h1 id="0e84" class="jp jq hu bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km dt translated"><strong class="ak">两种场景</strong></h1><p id="5573" class="pw-post-body-paragraph ir is hu it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hn dt translated">将知识转移到语义相似/相同但数据集不同的<strong class="it hv">任务。</strong></p><ul class=""><li id="9f02" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated"><strong class="it hv">源任务</strong>-用于二元情感分类的大型数据集</li><li id="fcac" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated"><strong class="it hv">目标任务(T) </strong> -用于二元情感分类的小数据集</li></ul><p id="d0b9" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">将知识转移到一个任务中，该任务在语义上与不同，但共享相同的神经网络结构，因此可以转移神经参数。</p><ul class=""><li id="5836" class="kt ku hu it b iu iv iy iz jc kv jg kw jk kx jo ky kz la lb dt translated"><strong class="it hv">源任务</strong> -用于二元情感分类的大型数据集</li><li id="2d76" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated"><strong class="it hv">目标任务(T) </strong> -用于6路问题分类的小数据集(例如，位置、时间和数量)</li></ul><h1 id="14ec" class="jp jq hu bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km dt translated">移植法</h1><h2 id="0ce9" class="lh jq hu bd jr li lj lk jv ll lm ln jz jc lo lp kd jg lq lr kh jk ls lt kl lu dt translated"><strong class="ak">参数初始化(INIT) </strong>。</h2><p id="19f0" class="pw-post-body-paragraph ir is hu it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hn dt translated">INIT方法首先<strong class="it hv">在S </strong>上训练网络，然后直接使用调整后的参数<strong class="it hv">为T </strong>初始化网络。转移后，我们可以固定目标域的参数，即<strong class="it hv">微调T. </strong>的参数</p><figure class="lw lx ly lz fq ma fe ff paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="fe ff lv"><img src="../Images/b3bd87b5bd5359f98947e60257bba24a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mH17FEz4LHpaJrPb.png"/></div></div></figure><h2 id="d08e" class="lh jq hu bd jr li lj lk jv ll lm ln jz jc lo lp kd jg lq lr kh jk ls lt kl lu dt translated"><strong class="ak">多任务学习(MULT) </strong></h2><p id="4852" class="pw-post-body-paragraph ir is hu it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hn dt translated">另一方面，MULT在两个领域同时训练<strong class="it hv">样本。</strong></p><figure class="lw lx ly lz fq ma fe ff paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="fe ff mh"><img src="../Images/8f0b119d3ac2fe62551ab63907bf4fb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pb4dDhgeKynpAE74.png"/></div></div><figcaption class="mi mj fg fe ff mk ml bd b be z ek">Multi Task Learning</figcaption></figure><h2 id="9f22" class="lh jq hu bd jr li lj lk jv ll lm ln jz jc lo lp kd jg lq lr kh jk ls lt kl lu dt translated"><strong class="ak">组合(MULT+初始化)</strong></h2><p id="e290" class="pw-post-body-paragraph ir is hu it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hn dt translated">我们首先在源域S上预训练<strong class="it hv">参数初始化</strong>，然后<strong class="it hv">同时训练S和T。</strong></p><h1 id="ecd8" class="jp jq hu bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km dt translated"><strong class="ak">型号性能上</strong></h1><h2 id="e222" class="lh jq hu bd jr li lj lk jv ll lm ln jz jc lo lp kd jg lq lr kh jk ls lt kl lu dt translated"><strong class="ak">参数初始化(INIT)，</strong> MULT和MULT+INIT</h2><ul class=""><li id="c94d" class="kt ku hu it b iu kn iy ko jc mm jg mn jk mo jo ky kz la lb dt translated"><strong class="it hv">语义等价任务</strong>的迁移学习似乎<strong class="it hv">成功</strong>。</li><li id="8102" class="kt ku hu it b iu lc iy ld jc le jg lf jk lg jo ky kz la lb dt translated">对于<strong class="it hv">语义不同的任务<strong class="it hv">没有大的改进</strong>。</strong></li></ul><h1 id="fc4e" class="jp jq hu bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km dt translated"><strong class="ak">结论</strong></h1><p id="9b06" class="pw-post-body-paragraph ir is hu it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hn dt translated">NLP中的神经迁移学习很大程度上取决于源数据集和目标数据集在语义上的相似程度。</p><h1 id="da10" class="jp jq hu bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km dt translated"><strong class="ak">参考</strong></h1><p id="ed3a" class="pw-post-body-paragraph ir is hu it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hn dt translated"><a class="ae mp" href="https://arxiv.org/abs/1603.06111" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1603.06111</a></p></div></div>    
</body>
</html>