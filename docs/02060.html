<html>
<head>
<title>David Silver RL Course: Lecture 1 Notes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大卫·西尔弗RL课程:第一讲笔记</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/david-silver-rl-course-lecture-1-notes-2e650270d626?source=collection_archive---------27-----------------------#2018-03-06">https://medium.com/hackernoon/david-silver-rl-course-lecture-1-notes-2e650270d626?source=collection_archive---------27-----------------------#2018-03-06</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><blockquote class="ir is it"><p id="20ec" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated">你可以在这里找到课堂笔记。请随意贡献并进一步改进它们。 <br/>大卫·西尔弗教授的UCL·伯克利RL课程第一讲笔记。</p></blockquote><p id="8bf0" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">强化<a class="ae jt" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>:它位于<a class="ae jt" href="https://hackernoon.com/tagged/science" rel="noopener ugc nofollow" target="_blank">科学</a>许多领域的交汇点。这是决策的科学，是理解最佳决策的方法。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff jx"><img src="../Images/fc82fee3b1fed7915755cc452966f44d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XYL-gEUZ7_ztK9lOP5Bfng.png"/></div></div></figure><p id="b464" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">例如:</p><ul class=""><li id="4e61" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">工程学:最优控制用不同的术语处理同一问题。</li><li id="1575" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">神经科学:研究表明，基于多巴胺的人脑奖励系统的工作是非常相关的。</li><li id="ebeb" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">心理学:关于动物为什么会做出某些决定，已经做了很多工作。</li><li id="22de" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">数学:运算研究。</li><li id="836b" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">经济学:博弈论、效用理论和有限理性都致力于同一个问题。</li></ul><p id="ae00" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">所以这是很多分支的共同之处，也是解决基于奖励的问题的一般方法。</p><h2 id="ed63" class="kx ky hu bd kz la lb lc ld le lf lg lh ju li lj lk jv ll lm ln jw lo lp lq lr dt translated">RL与OtherLearning:</h2><ul class=""><li id="71c7" class="kj kk hu ix b iy ls jc lt ju lu jv lv jw lw js ko kp kq kr dt translated">没有监督者，它使用试错法工作，使用奖励信号改进。</li><li id="b288" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">奖励信号可以是也可以不是即时的。</li><li id="7719" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">时间是相关的。给定时间的一个事件与下一个事件中的动作非常相关。</li><li id="c45d" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">在RL情况下,“代理”可以影响它看到的数据。它影响环境。</li></ul><p id="5f5f" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">真实生活使用案例:</p><ul class=""><li id="253f" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">直升机特技表演</li><li id="f800" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">双陆棋游戏</li><li id="c454" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">投资管理</li><li id="afad" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">发电站控制</li><li id="8de8" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">教一个人形机器人走路</li><li id="99b3" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">雅达利游戏</li></ul><h1 id="6710" class="lx ky hu bd kz ly lz ma ld mb mc md lh me mf mg lk mh mi mj ln mk ml mm lq mn dt translated">RL问题</h1><ul class=""><li id="5609" class="kj kk hu ix b iy ls jc lt ju lu jv lv jw lw js ko kp kq kr dt translated">奖励:Rt，只是一个标量反馈信号。它定义了“代理人”在时间步长t的绩效。代理人的目标是最大化目标，这是一笔报酬。</li></ul><p id="a180" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">(非正式)奖励假设:所有的目标都可以用期望累积奖励的最大化来描述。</p><p id="2d52" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">所有的奖励都可以根据比较后的行动来衡量。因此，所有的奖励都可以分解成一个比较的尺度，因此最终必须得到一个标量奖励。</p><p id="07ef" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">根据定义，目标可以是中间目标、最终目标或基于时间的目标等。</p><p id="9fd5" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">第一步是理解奖励信号。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff jx"><img src="../Images/47876657d1658db7ab130ca9e56c0463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*agF53c6VEVCU6euqz8LCLg.png"/></div></div></figure><h1 id="9152" class="lx ky hu bd kz ly lz ma ld mb mc md lh me mf mg lk mh mi mj ln mk ml mm lq mn dt translated">顺序决策</h1><ul class=""><li id="7a5a" class="kj kk hu ix b iy ls jc lt ju lu jv lv jw lw js ko kp kq kr dt translated">目标:选择行动以最大化未来奖励。</li><li id="9964" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">行动有长期的后果，所以我们必须提前考虑。贪婪的方法在这里可能没有用。</li><li id="267e" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">奖励可能会延迟(不是立即)</li><li id="b04f" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">为了(更大的)长期回报而牺牲眼前的回报。</li></ul><h1 id="1a6a" class="lx ky hu bd kz ly lz ma ld mb mc md lh me mf mg lk mh mi mj ln mk ml mm lq mn dt translated">形式主义</h1><h2 id="9a95" class="kx ky hu bd kz la lb lc ld le lf lg lh ju li lj lk jv ll lm ln jw lo lp lq lr dt translated">代理人</h2><p id="01d4" class="pw-post-body-paragraph iu iv hu ix b iy ls ja jb jc lt je jf ju mo ji jj jv mp jm jn jw mq jq jr js hn dt translated">我们在这里控制大脑——大脑是代理。</p><ul class=""><li id="cac2" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">代理可以采取措施</li><li id="4d10" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">在每一步，行动都受到给定时间步的观察的影响。</li><li id="2702" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">它接收奖励信号。</li><li id="2ca4" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">我们必须找出基于这些来决定行动的算法</li></ul><h2 id="91c5" class="kx ky hu bd kz la lb lc ld le lf lg lh ju li lj lk jv ll lm ln jw lo lp lq lr dt translated">环境:</h2><p id="b2d4" class="pw-post-body-paragraph iu iv hu ix b iy ls ja jb jc lt je jf ju mo ji jj jv mp jm jn jw mq jq jr js hn dt translated">环境存在于环境之外。</p><p id="bb16" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">在每一步，代理接收由<a class="ae jt" href="https://hackernoon.com/tagged/environment" rel="noopener ugc nofollow" target="_blank">环境</a>产生的观察，并且代理本身通过采取行动来影响环境。</p><p id="e8a4" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">RL的机器<a class="ae jt" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>问题与来自试错交互的数据流有关。</p><h1 id="53f8" class="lx ky hu bd kz ly lz ma ld mb mc md lh me mf mg lk mh mi mj ln mk ml mm lq mn dt translated">历史和国家</h1><p id="6135" class="pw-post-body-paragraph iu iv hu ix b iy ls ja jb jc lt je jf ju mo ji jj jv mp jm jn jw mq jq jr js hn dt translated">经验流、观察顺序、行动和回报。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mr"><img src="../Images/9a8597c2526d957ef52b31b6be6ad723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vbB2eqhMgk3wn1F1qwRJyg.png"/></div></div></figure><p id="4850" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">H(t)是</p><ul class=""><li id="5666" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">到时间t为止所有观测的历史</li><li id="eca5" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">我们的下一步行动取决于历史</li><li id="8581" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">我们建立从历史H(t)到一组动作的映射(算法)。</li><li id="6e30" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">环境回报观察(发射)和基于H(t)的奖励</li></ul><p id="6584" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">状态:是对信息的总结，以决定下一步的行动。它捕捉历史来确定接下来应该发生的所有事情。</p><ul class=""><li id="43b7" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">国家是历史的函数。</li><li id="54a7" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">S(t) = f[H(t)]</li><li id="bcbc" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">Def 1:环境状态:环境内部使用的信息，用于根据环境的历史确定下一个事件。确定环境的下一个事件所必需的信息，这通常对代理不可见。它们有助于理解环境。通常情况下，与算法无关。</li></ul><p id="58cd" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">注意:对于多代理问题，代理可以将其他代理视为环境的一部分。</p><ul class=""><li id="57e2" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">Def 2:代理状态:存在于我们算法中的数字的状态。它总结了代理的信息和内部状态。代理状态被我们的RL算法所利用，并决定下一个动作。该功能由我们定义。</li></ul><h1 id="3d89" class="lx ky hu bd kz ly lz ma ld mb mc md lh me mf mg lk mh mi mj ln mk ml mm lq mn dt translated">信息状态:马尔可夫状态</h1><p id="d02d" class="pw-post-body-paragraph iu iv hu ix b iy ls ja jb jc lt je jf ju mo ji jj jv mp jm jn jw mq jq jr js hn dt translated">一个信息系统包含所有有用的历史信息。</p><p id="3df5" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">马尔可夫性质:一个状态是马尔可夫的当且仅当:下一个状态的概率，给定你的当前状态与所有先前状态相同。换句话说，只有当前状态决定下一个状态，而历史是不相关的。</p><p id="8c20" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">换句话说，如果马尔可夫性质成立。鉴于现在，未来独立于历史。因为国家代表了过去的一切。</p><p id="11a8" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">另一个定义:状态是对未来的充分统计。</p><p id="2f68" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">根据定义:环境状态是马尔可夫的。</p><p id="4cfe" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">而整个历史也是一个马尔可夫状态。(不是一个有用的)</p><ol class=""><li id="f91a" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ms kp kq kr dt translated">完全可观察的环境:代理可以看到完整的环境，代理状态=环境状态=信息系统。这被称为MDP(马尔可夫决策过程)。</li><li id="fe5b" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ms kp kq kr dt translated">部分可观测环境:智能体间接观测环境。在这种情况下，代理状态！=环境状态。这被称为部分可观测MDP (POMDP)。可能的代理状态表示:</li></ol><ul class=""><li id="708a" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">天真的方法:代理状态=完整的历史。</li></ul><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mt"><img src="../Images/b15426e3a666e679464d3c72b546d16c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3W8exDX27LiLGAZup0RFJw.png"/></div></div></figure><ul class=""><li id="3662" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">贝叶斯方法:我们发展信念，其中代理状态是概率的向量，以选择下一步。</li><li id="0adb" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">RNN:先前状态和最新观察的线性组合。所以这是旧状态到新状态的线性转换，伴随着给定的观察，伴随着一些非线性。</li></ul><h1 id="342d" class="lx ky hu bd kz ly lz ma ld mb mc md lh me mf mg lk mh mi mj ln mk ml mm lq mn dt translated">在RL特工内部</h1><p id="e153" class="pw-post-body-paragraph iu iv hu ix b iy ls ja jb jc lt je jf ju mo ji jj jv mp jm jn jw mq jq jr js hn dt translated">RL代理可能包括(也可能不包括)以下一项:</p><ul class=""><li id="2003" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">策略:这是代理如何选择它的动作。将其状态映射到操作</li><li id="0d5f" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">价值函数:对每个状态或动作进行估价。</li><li id="57ff" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">模型:代理对环境工作的感知。</li></ul><h2 id="6194" class="kx ky hu bd kz la lb lc ld le lf lg lh ju li lj lk jv ll lm ln jw lo lp lq lr dt translated">政策:</h2><p id="05f2" class="pw-post-body-paragraph iu iv hu ix b iy ls ja jb jc lt je jf ju mo ji jj jv mp jm jn jw mq jq jr js hn dt translated">这是一张从国家到行动的地图。确定当代理处于某种状态时它将做什么。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mu"><img src="../Images/03baad42a04a6c60d1ed3a7ad519aabc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t6DXqywGttqz85st_kW3uA.png"/></div></div></figure><ul class=""><li id="450a" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">确定性策略:a = F(S)。我们希望从经验中学习这一政策，并希望通过它获得最大回报。</li><li id="b8fa" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">随机策略:允许随机的探索活动。这是在给定状态下采取行动的概率(随机地图)。</li></ul><h2 id="3c9d" class="kx ky hu bd kz la lb lc ld le lf lg lh ju li lj lk jv ll lm ln jw lo lp lq lr dt translated">价值函数:</h2><p id="da5d" class="pw-post-body-paragraph iu iv hu ix b iy ls ja jb jc lt je jf ju mo ji jj jv mp jm jn jw mq jq jr js hn dt translated">这是对预期未来回报的预测。我们在行动之间做出选择，决定追求最高的回报，这是通过价值函数得到的估计。</p><p id="e81e" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">价值函数取决于我们的行为方式，取决于政策。如果我们采取行动，它会给予奖励，从而有助于优化我们的行为。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mv"><img src="../Images/bef1a6ae0556d973f67266fe1afc67a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zAijBI4t0ipGX3XMZWD8hA.png"/></div></div></figure><p id="2c12" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">伽马:贴现。它影响我们是否关心当前/以后的状态。它决定了评估未来的视野。(地平线-我们需要计算我们行动的结果有多远)。</p><h2 id="7f6a" class="kx ky hu bd kz la lb lc ld le lf lg lh ju li lj lk jv ll lm ln jw lo lp lq lr dt translated">型号:</h2><p id="f6d6" class="pw-post-body-paragraph iu iv hu ix b iy ls ja jb jc lt je jf ju mo ji jj jv mp jm jn jw mq jq jr js hn dt translated">它用于了解环境，预测环境下一步会做什么。没有必要创建一个环境模型。但是当我们这样做的时候它是有用的。</p><figure class="jy jz ka kb fq kc fe ff paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="fe ff mw"><img src="../Images/29a01b0cfd4fb731f374898c87af99b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cIiKn4gI61vB1PoKBrYftw.png"/></div></div></figure><p id="4420" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">它可以分为两种状态:</p><ul class=""><li id="9c70" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">过渡:预测环境的动态。下一个州。</li><li id="2cb9" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">奖励:预测即时奖励。这分为:</li></ul><ol class=""><li id="96f8" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ms kp kq kr dt translated">状态转换模型:在给定当前奖励的情况下，预测状态转换。</li><li id="9be4" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ms kp kq kr dt translated">奖励模型:它预测给定当前状态下的预期奖励。</li></ol><p id="6984" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">RL代理:</p><p id="dd6a" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">我们根据上述三个概念中的哪一个来对我们的代理进行分类。比方说，如果我们有一个基于价值的代理:如果它有一个价值函数，一个策略是隐含的。</p><ul class=""><li id="bbe2" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">基于价值</li><li id="65e0" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">基于策略</li><li id="2f0a" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">演员评论家</li></ul><p id="ca04" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">基于策略:维护每个状态的数据结构，而不存储值函数。</p><p id="09d8" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">演员评论家:结合了政策和价值功能。</p><p id="e737" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">因此，RL问题可以分类为:</p><ul class=""><li id="ccae" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">无模型:我们不试图理解环境，我们直接看到经验，并制定政策。</li><li id="1ab8" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">基于模型:涉及一个RL代理。</li></ul><h1 id="d26e" class="lx ky hu bd kz ly lz ma ld mb mc md lh me mf mg lk mh mi mj ln mk ml mm lq mn dt translated">RL内部的问题</h1><h2 id="ebf3" class="kx ky hu bd kz la lb lc ld le lf lg lh ju li lj lk jv ll lm ln jw lo lp lq lr dt translated">学习和规划:</h2><p id="2ff1" class="pw-post-body-paragraph iu iv hu ix b iy ls ja jb jc lt je jf ju mo ji jj jv mp jm jn jw mq jq jr js hn dt translated">当涉及到顺序决策时，有两个问题。</p><ul class=""><li id="ae81" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">RL问题:</li></ul><ol class=""><li id="319d" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ms kp kq kr dt translated">最初环境是未知的(通过反复试验)。</li><li id="843a" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ms kp kq kr dt translated">与环境互动。</li><li id="5e93" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ms kp kq kr dt translated">改进it政策。</li></ol><ul class=""><li id="1fe3" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">规划:</li></ul><ol class=""><li id="20cf" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ms kp kq kr dt translated">我们描述环境，agent的模型为agent所知。</li><li id="32f4" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ms kp kq kr dt translated">代理计算其模型并改进其策略</li></ol><h2 id="d400" class="kx ky hu bd kz la lb lc ld le lf lg lh ju li lj lk jv ll lm ln jw lo lp lq lr dt translated">探索与开发</h2><p id="1011" class="pw-post-body-paragraph iu iv hu ix b iy ls ja jb jc lt je jf ju mo ji jj jv mp jm jn jw mq jq jr js hn dt translated">RL的另一个重要方面。</p><ul class=""><li id="dc41" class="kj kk hu ix b iy iz jc jd ju kl jv km jw kn js ko kp kq kr dt translated">RL就像试错学习。</li><li id="d700" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">当我们探索的时候，我们可能会错过奖励。</li><li id="4a10" class="kj kk hu ix b iy ks jc kt ju ku jv kv jw kw js ko kp kq kr dt translated">我们想找出最佳策略。</li></ul><p id="138d" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">探索:选择放弃一些已知的奖励，以发现更多关于环境的东西。</p><p id="cbc9" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">利用:利用已知信息来获取最大回报。</p><p id="fa2c" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">有一个探索和开发的权衡。</p><h2 id="71f9" class="kx ky hu bd kz la lb lc ld le lf lg lh ju li lj lk jv ll lm ln jw lo lp lq lr dt translated">预测和控制</h2><p id="f7f0" class="pw-post-body-paragraph iu iv hu ix b iy ls ja jb jc lt je jf ju mo ji jj jv mp jm jn jw mq jq jr js hn dt translated">预测:给定当前政策，对未来的估计</p><p id="d3d9" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">控制:找到最佳策略。</p><p id="791a" class="pw-post-body-paragraph iu iv hu ix b iy iz ja jb jc jd je jf ju jh ji jj jv jl jm jn jw jp jq jr js hn dt translated">在RL，我们需要评估我们所有的政策，找出最好的。</p><blockquote class="ir is it"><p id="8ef0" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated">订阅时事通讯，获取深度学习和计算机视觉阅读的每周精选列表。</p><p id="5360" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hn dt translated"><a class="ae jt" href="http://twitter.com/bhutanisanyam1" rel="noopener ugc nofollow" target="_blank">你可以在推特@bhutanisanyam1 </a>上找到我</p></blockquote><figure class="jy jz ka kb fq kc"><div class="bz el l di"><div class="mx my l"/></div></figure></div></div>    
</body>
</html>