<html>
<head>
<title>Using Q-Learning to teach a robot how to walk — A.I. Odyssey part. 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Q-Learning来教机器人如何走路——人工智能奥德赛部分。3</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/using-q-learning-to-teach-a-robot-how-to-walk-a-i-odyssey-part-3-5285237cc3b1?source=collection_archive---------8-----------------------#2018-03-13">https://medium.com/hackernoon/using-q-learning-to-teach-a-robot-how-to-walk-a-i-odyssey-part-3-5285237cc3b1?source=collection_archive---------8-----------------------#2018-03-13</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="53fb" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这一集:Q值，强化学习，等等。<br/> <em class="jp">确保检查出</em> <strong class="it hv"> <em class="jp"> </em> </strong> <a class="ae jq" rel="noopener" href="/@juliendespois/finding-the-genre-of-a-song-with-deep-learning-da8f59a61194#.r28dkpg2e"> <strong class="it hv"> <em class="jp">部分。1</em></strong></a><strong class="it hv"><em class="jp"/></strong><em class="jp">和</em><strong class="it hv"><em class="jp"/></strong><a class="ae jq" href="https://hackernoon.com/talk-to-you-computer-with-you-eyes-and-deep-learning-a-i-odyssey-part-2-7d3405ab8be1#.6esc69gby" rel="noopener ugc nofollow" target="_blank"><strong class="it hv"><em class="jp">部分。2</em></strong></a><strong class="it hv"><em class="jp"/></strong><em class="jp">太！</em></p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff jr"><img src="../Images/fb48a3d49e37f2ab7cba1442fd7362c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*moE-vkRzpzZ0YmifhjwHww.gif"/></div></div></figure><h1 id="ae96" class="kd ke hu bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la dt translated">介绍</h1><p id="ff3c" class="pw-post-body-paragraph ir is hu it b iu lb iw ix iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo hn dt translated">今天，我们将学习如何创建一个虚拟代理，它能发现如何与世界互动。我们将要使用的技术叫做<strong class="it hv"> Q-Learning，</strong>它是<em class="jp">超级</em> <em class="jp">酷</em>。</p><h2 id="dfc4" class="lg ke hu bd kf lh li lj kj lk ll lm kn jc ln lo kr jg lp lq kv jk lr ls kz lt dt translated">主体、状态和目标</h2><p id="79fb" class="pw-post-body-paragraph ir is hu it b iu lb iw ix iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo hn dt translated">让我们看看我们的代理人！</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff lu"><img src="../Images/fe7107f337cbbc7059cb34399ef9ab9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*TXCHM7ya3Gz2L9JyqZtL6w.png"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">Henry, the robot</figcaption></figure><p id="d25f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这是<strong class="it hv"> <em class="jp">亨利</em> </strong>，他是一个年轻的虚拟机器人，他有一个梦想:<strong class="it hv">环游世界。问题是他不太了解这个世界。事实上，他根本不知道如何用<em class="jp">移动</em>！他只知道他的<strong class="it hv"> GPS位置</strong>，它的脚的<strong class="it hv">位置，如果它们在地面上</strong>的话</strong>和<strong class="it hv">。</strong></p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff lu"><img src="../Images/30b644d1cd8dd40e004daf872f6b9585.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*IghfELBDB9D551QpwxmnLQ.png"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">What Henry perceives of the world</figcaption></figure><p id="35b3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这些元素可以分为两个部分:状态<strong class="it hv"><em class="jp"/></strong>和目标<strong class="it hv"> <em class="jp">。</em> </strong>我们代理人的<em class="jp">状态</em>是与他身体相关的信息的集合，而<em class="jp">目标</em>是代理人想要增加的。在我们的例子中，代理人想用它的脚来改变它的位置。</p><blockquote class="lz ma mb"><p id="85e5" class="ir is jp it b iu iv iw ix iy iz ja jb mc jd je jf md jh ji jj me jl jm jn jo hn dt translated"><strong class="it hv"> <em class="hu">状态</em> </strong>:【右脚伸展，右脚着地，左脚伸展，左脚着地】<br/> <strong class="it hv"> <em class="hu">目标</em> </strong>:【位置】</p></blockquote><h2 id="814e" class="lg ke hu bd kf lh li lj kj lk ll lm kn jc ln lo kr jg lp lq kv jk lr ls kz lt dt translated">行动</h2><p id="13c5" class="pw-post-body-paragraph ir is hu it b iu lb iw ix iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo hn dt translated">幸运的是，亨利有能力表演动作。即使他看不到太多的世界，他也知道自己能做什么，这取决于他的状态。<em class="jp">举个例子，</em>当他双脚着地的时候，他可以<em class="jp">举起</em>一只。当一只脚在空中时，他可以向前<em class="jp">移动</em>或向后<em class="jp">移动</em>。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mf"><img src="../Images/d0032b46806f7970d54bfbcd580ce064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*30A3QOnBZryRvcIfy_R9Sg.png"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">The actions available to our robot, depending on his state. <strong class="bd mg">NB:</strong> In reality, the diagram is a bit more complex, as the robot takes into account the extension of the foot.</figcaption></figure><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mh"><img src="../Images/07c29100c814975d06b92038b4c1d9f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/1*Cuv6aA4LhUFUBAOFuWLLhQ.gif"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">Here’s Henry performing random actions</figcaption></figure><h1 id="ef57" class="kd ke hu bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la dt translated">政策</h1><p id="a2ac" class="pw-post-body-paragraph ir is hu it b iu lb iw ix iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo hn dt translated">为了达到他的目标，代理遵循一组称为<strong class="it hv"> <em class="jp">策略</em> </strong>的指令。这些指令可以由代理学习或由人类手动写下。</p><h2 id="5ea3" class="lg ke hu bd kf lh li lj kj lk ll lm kn jc ln lo kr jg lp lq kv jk lr ls kz lt dt translated">遵循硬编码的策略</h2><p id="711c" class="pw-post-body-paragraph ir is hu it b iu lb iw ix iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo hn dt translated">让我们写一个简单的政策让我们的代理人遵守。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mi"><img src="../Images/0a6bfe4586e8faf0177bafbf41a2005b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*FUehblHAPnW84_qEZ3nqKA.png"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">A simple, hard-coded policy</figcaption></figure><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mj"><img src="../Images/c35daf808525d19136501bced9bbf9e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/1*so_22B_fO6rzsNRkwGvIvQ.gif"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">The agent following the simple policy. Look at him go!</figcaption></figure><p id="8ca5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这看起来很有希望，尽管机器人失去了很多抬起和放下脚的时间。这不是真正的<strong class="it hv">高效</strong>。</p><p id="508e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">事实上，我们<em class="jp">很少</em>手动写出好的政策。这要么是因为我们<strong class="it hv">不知道任何</strong> <em class="jp">(例如，复杂任务)</em>，要么是因为它们通常<strong class="it hv">不健壮</strong> <em class="jp">(如果代理以某种方式开始左脚向上，上面的策略将立即失败，因为代理不能执行期望的动作)。</em></p><h2 id="d2cc" class="lg ke hu bd kf lh li lj kj lk ll lm kn jc ln lo kr jg lp lq kv jk lr ls kz lt dt translated">q学习</h2><p id="7b3c" class="pw-post-body-paragraph ir is hu it b iu lb iw ix iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo hn dt translated">为了帮助我们的代理人完成他的梦想，我们将使用一种被称为<a class="ae jq" href="http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="it hv">的<a class="ae jq" href="https://www.youtube.com/watch?v=RtxI449ZjSc&amp;feature=relmfu" rel="noopener ugc nofollow" target="_blank"> <strong class="it hv">强化学习</strong> </a>技巧——学习</strong> </a> <strong class="it hv"> </strong>到<strong class="it hv"> </strong>帮助机器人<em class="jp">学习</em>健壮的和<strong class="it hv">高效的</strong>策略。该技术在于当代理处于某个状态时，将一个数字或<strong class="it hv"> Q值</strong>分配给执行某个动作<strong class="it hv">的事件。该值代表该动作的<strong class="it hv">进度</strong>。</strong></p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mi"><img src="../Images/baa29e844680e24332111e9e68c8af0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*YZ80N25evnCOlTl8fqXLZQ.png"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">Some State-Action pairs associated with their Q-Value</figcaption></figure><p id="7c1f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">该值由一个<strong class="it hv"> <em class="jp">奖励函数</em> </strong> <em class="jp">，</em>决定，指示该行动对达成目标是否有<strong class="it hv">正面</strong>或<strong class="it hv">负面</strong>影响。<em class="jp">在我们的例子</em>中，如果亨利将<strong class="it hv">从他所在的地方移开</strong>，这是<em class="jp">好</em>，如果他将<strong class="it hv">退回</strong>，这是<em class="jp">坏</em>。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff lu"><img src="../Images/ef60c94fc7ab146ea0e2bfd9952aafbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*buF-xFWsR1DAJ5O-qXJtLw.png"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">Our agent’s basic reward function</figcaption></figure><blockquote class="lz ma mb"><p id="3a70" class="ir is jp it b iu iv iw ix iy iz ja jb mc jd je jf md jh ji jj me jl jm jn jo hn dt translated">这里的关键点是，具有<strong class="it hv">值</strong>的不是<strong class="it hv">动作<strong class="it hv">本身</strong>，而是当代理处于<strong class="it hv">特定状态</strong>时<strong class="it hv">执行</strong> <strong class="it hv">动作</strong>的事实。</strong></p></blockquote><p id="df1c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">知道了<strong class="it hv">所有的Q值</strong>，代理<em class="jp">可以</em>，在每一步，根据他的状态选择<em class="jp">行动</em>，这将带来最好的回报。这将是他的政策。</p><p id="b66e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这里的问题是亨利不知道Q值是多少！这意味着他<strong class="it hv">无法</strong>选择一个能让他<strong class="it hv">更接近</strong>目标的行动！</p><p id="b020" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">幸运的是，我们有办法让他自己发现Q值。</p><h1 id="b922" class="kd ke hu bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la dt translated">用ε-贪婪算法学习Q值</h1><h2 id="30ee" class="lg ke hu bd kf lh li lj kj lk ll lm kn jc ln lo kr jg lp lq kv jk lr ls kz lt dt translated">培训代理</h2><p id="c5b8" class="pw-post-body-paragraph ir is hu it b iu lb iw ix iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo hn dt translated">为了了解其状态-动作对的Q值，亨利必须亲自尝试这些动作。问题是可能有数十亿种可能的组合，而程序不可能全部尝试。我们希望亨利<strong class="it hv">尝试尽可能多的</strong>组合作为<strong class="it hv">可能的</strong>，但是<strong class="it hv">关注<strong class="it hv">最好的</strong>。</strong></p><p id="3214" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了实现这一点，我们将使用<a class="ae jq" href="http://tokic.com/www/tokicm/publikationen/papers/AdaptiveEpsilonGreedyExploration.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="it hv">ε-贪婪</strong> </a>算法来训练机器人。它是这样工作的:在<em class="jp">每个</em> <em class="jp">步骤</em>，机器人有一个<strong class="it hv">概率ε</strong>执行一个<strong class="it hv">随机</strong>可用动作，否则它根据它知道的Q值选择最佳动作。动作的结果用于<strong class="it hv">更新</strong>Q值。</p><h2 id="3aa3" class="lg ke hu bd kf lh li lj kj lk ll lm kn jc ln lo kr jg lp lq kv jk lr ls kz lt dt translated">更新Q值和远距离奖励的问题</h2><p id="bf1f" class="pw-post-body-paragraph ir is hu it b iu lb iw ix iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo hn dt translated">更新Q值的一个简单方法是用机器人刚刚通过动作体验到的值代替机器人在存储器中的值。然而，这带来了一个真正有问题的问题:机器人无法看到未来的一步。这使得机器人对任何<strong class="it hv">未来</strong>的奖励视而不见。</p><blockquote class="lz ma mb"><p id="4144" class="ir is jp it b iu iv iw ix iy iz ja jb mc jd je jf md jh ji jj me jl jm jn jo hn dt translated">抬起一只脚是行走所必需的，但是当它没有带来任何直接的回报时，亨利<strong class="it hv">怎么能发现</strong>这是一个好的行动呢？</p></blockquote><p id="62fc" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这个问题的解决方案由<a class="ae jq" href="https://en.wikipedia.org/wiki/Bellman_equation" rel="noopener ugc nofollow" target="_blank">贝尔曼方程</a>给出，它提出了一种计算<strong class="it hv">未来</strong>奖励的方法。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff jr"><img src="../Images/22512cede1b188ef1d01c8e39ca05de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OiFnL4TYdfboilvQUy7HLQ.png"/></div></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">Bellman equation</figcaption></figure><p id="5acb" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">首先</strong>，不是用新值替换旧值，而是旧值以<strong class="it hv">一定速率</strong>(<em class="jp">α，学习速率</em>)逐渐消失。这使得机器人能够考虑噪音。有些行动可能有时有效，有时无效。随着Q值的这种<em class="jp">渐进的</em>演变，一个错误的奖励<strong class="it hv">不会搞乱整个系统。</strong></p><p id="a9de" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">也就是</strong>，新值的计算不仅使用即时奖励，还使用<strong class="it hv">预期最大值</strong>。这个价值由我们期望从可获得的行动中得到的<strong class="it hv">最佳回报</strong>组成。这对学习过程的效率有着巨大的影响。这样，奖励<a class="ae jq" href="http://www.cs.upc.edu/~mmartin/Ag4-4x.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="it hv">及时传播回来</strong>。</a></p><blockquote class="lz ma mb"><p id="7031" class="ir is jp it b iu iv iw ix iy iz ja jb mc jd je jf md jh ji jj me jl jm jn jo hn dt translated">随着这种变化，抬起一只脚的Q值变成了正的，因为它受益于向前迈出一步的未来最佳期望值。</p></blockquote><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mi"><img src="../Images/d3428ce46f5c9a4233141de332e684fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*crboaZE1Wv5tCYED1BYpPg.png"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">Q-Values updated with Bellman equation. Note how <em class="mk">putting a foot down </em>now is seen as <strong class="bd mg">positive</strong> instead of <strong class="bd mg">neutral</strong>, as is benefits from the reward of taking the <strong class="bd mg">next</strong> step.</figcaption></figure><h2 id="3a58" class="lg ke hu bd kf lh li lj kj lk ll lm kn jc ln lo kr jg lp lq kv jk lr ls kz lt dt translated">探索与开发的困境</h2><p id="82a6" class="pw-post-body-paragraph ir is hu it b iu lb iw ix iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo hn dt translated">通过利用价值epsilon，我们面临着探索与开发的两难境地。一个拥有<strong class="it hv">高ε</strong>的代理人会<strong class="it hv">多半</strong>尝试<strong class="it hv">随机</strong>行动(<em class="jp">探索</em>)，而一个拥有<strong class="it hv">低ε</strong>的代理人会<strong class="it hv">很少</strong>尝试<strong class="it hv">新</strong>行动(<em class="jp">剥削</em>)。我们需要找到一个甜蜜点，使我们的机器人能够尝试许多新的东西，而不会在没有希望的领先优势上浪费太多时间。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff ml"><img src="../Images/ab08102989fb30c3efbd5b41c4e16617.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*raKw5kjZE_WvPTUuvqSvCw.png"/></div></figure><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mm"><img src="../Images/7d9db7b88f406cdd15efe6481285791c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*Jc-U9psrANdMy9gZdS6qmg.gif"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">Two agents, trained with different epsilons, following their best policy.</figcaption></figure><p id="0db3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在这里，更专注于<strong class="it hv">探索的代理</strong>发现了一种非常有效的移动技术。相反，<em class="jp">黄色</em>机器人没有充分利用他的脚的伸展，因为它没有花足够的时间尝试随机动作。<strong class="it hv">注意</strong>必须找到一个<strong class="it hv">平衡</strong>，因为花费太多时间在探索上会阻止代理学习<strong class="it hv">复杂的策略。</strong></p><h1 id="8790" class="kd ke hu bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la dt translated">结论</h1><p id="9fdf" class="pw-post-body-paragraph ir is hu it b iu lb iw ix iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo hn dt translated">看那个！亨利正以惊人的速度环游世界！他设法自己找到了一个比我们给他的简单策略更好的移动方式，加上他可以从小错误中恢复，因为在任何状态下他只需要跟随他信任的Q值来引导他！</p><p id="5cb9" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">您可以在这里玩代码:</p><div class="mn mo fm fo mp mq"><a href="https://github.com/despoisj/QLearningWalkingRobot" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab ej"><div class="ms ab mt cl cj mu"><h2 class="bd hv fv z el mv eo ep mw er et ht dt translated">GitHub-despisj/qlearningwalkingbot:一个简单的Q学习示例，带有一个可爱的步行机器人</h2><div class="mx l"><h3 class="bd b fv z el mv eo ep mw er et ek translated">一个简单的Q-学习的例子，有一个可爱的步行机器人。</h3></div><div class="my l"><p class="bd b gc z el mv eo ep mw er et ek translated">github.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne kb mq"/></div></div></a></div><p id="3308" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">🎉你已经到达终点了！我希望你喜欢这篇文章。如果你做了，请喜欢它，分享它，订阅时事通讯，给我发送比萨饼，在媒体上跟随我，或者做任何你想做的事情！🎉</p><blockquote class="lz ma mb"><p id="01bd" class="ir is jp it b iu iv iw ix iy iz ja jb mc jd je jf md jh ji jj me jl jm jn jo hn dt translated"><strong class="it hv">如果你喜欢人工智能，</strong> <a class="ae jq" href="http://eepurl.com/cATXvT" rel="noopener ugc nofollow" target="_blank"> <strong class="it hv">订阅时事通讯</strong> </a> <strong class="it hv">接收文章更新和更多内容！</strong></p></blockquote></div></div>    
</body>
</html>