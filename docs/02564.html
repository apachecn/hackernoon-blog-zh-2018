<html>
<head>
<title>Memorizing is not learning! — 6 tricks to prevent overfitting in machine learning.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">死记硬背不是学习！—防止机器学习中过拟合的6招。</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42?source=collection_archive---------2-----------------------#2018-03-22">https://medium.com/hackernoon/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42?source=collection_archive---------2-----------------------#2018-03-22</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div class="fe ff hs"><img src="../Images/05c5cde5f2e59af02c3c5602259d2f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*XWh6hd8BgI3RKhd8bkrbuw.png"/></div></figure><div class=""/><h1 id="8181" class="iy iz ib bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv dt translated">介绍</h1><p id="b45f" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated"><strong class="jy ic">过拟合</strong>可能是<em class="ku">机器学习最令人沮丧的问题。</em>在本文中，我们将看到<strong class="jy ic">什么</strong> <strong class="jy ic"> it </strong> <strong class="jy ic">是</strong>，<strong class="jy ic">如何发现</strong> <strong class="jy ic"> it </strong>，以及最重要的<strong class="jy ic">如何</strong> <strong class="jy ic">防止</strong> <strong class="jy ic"> it发生</strong>。</p><h1 id="3999" class="iy iz ib bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv dt translated">什么是过度拟合？</h1><p id="4874" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated"><strong class="jy ic">过度拟合</strong>这个词指的是对训练数据建模太好的模型。该模型不是学习数据的一般<strong class="jy ic">分布</strong>，而是学习每个数据点的<em class="ku">预期</em> <em class="ku">输出</em>。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div class="fe ff kv"><img src="../Images/e4ccdde0ba591cad9407e95567558eda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*SBUK2QEfCP-zvJmKm14wGQ.png"/></div></figure><p id="87aa" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">这和<strong class="jy ic">记忆</strong> <strong class="jy ic">数学测验的答案</strong>而不是<strong class="jy ic">知道</strong><strong class="jy ic"/><strong class="jy ic">公式</strong>是一样的。正因为如此，模型不能<em class="ku">一般化</em>。只要你在<em class="ku">熟悉的</em> <em class="ku">领地</em>里，一切都是好的，但是一旦你走出去，你就迷失了。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div class="fe ff hs"><img src="../Images/d807662b6d3298e66f5d892951529757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*qkL_6Cu0PqAm0My-he3qFg.png"/></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">Looks like this little guy <strong class="bd lj">doesn’t know how</strong> to do a multiplication. He only <strong class="bd lj">remembers</strong> the answers to the questions he has already seen.</figcaption></figure><p id="8390" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">棘手的部分是，乍一看，<strong class="jy ic">可能看起来</strong>你的模型表现良好，因为它在<em class="ku">训练</em>数据上有一个非常<strong class="jy ic">小的</strong> <em class="ku"> </em> <strong class="jy ic">误差</strong>。然而，一旦你要求它<strong class="jy ic">预测</strong> <strong class="jy ic">新的数据点，</strong>它就会<strong class="jy ic">失败</strong>。</p><h1 id="aee3" class="iy iz ib bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv dt translated">如何检测过度拟合</h1><p id="91bd" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated">如上所述，过拟合的特征是模型<strong class="jy ic">的<strong class="jy ic">不能</strong>到</strong> <strong class="jy ic">的一般化</strong>。为了测试这种能力，一个简单的方法是将数据集分成两部分:训练集<strong class="jy ic"/><strong class="jy ic"/>和测试集<strong class="jy ic"/><strong class="jy ic">。</strong> <em class="ku">选择模型时，你可能想把数据集一分为三，</em> <a class="ae lk" href="https://hackernoon.com/stop-feeding-garbage-to-your-model-the-6-biggest-mistakes-with-datasets-and-how-to-avoid-them-3cb7532ad3b7" rel="noopener ugc nofollow" target="_blank"> <em class="ku">我在这里解释一下为什么</em> </a> <em class="ku">。</em></p><ol class=""><li id="4277" class="ll lm ib jy b jz la kd lb kh ln kl lo kp lp kt lq lr ls lt dt translated"><em class="ku">训练</em>集合代表了大约<strong class="jy ic">的80% </strong>的<em class="ku">可用</em>数据，并且用于训练模型(你不会说吧？！).</li><li id="e435" class="ll lm ib jy b jz lu kd lv kh lw kl lx kp ly kt lq lr ls lt dt translated"><em class="ku">测试</em>集合由数据集剩余的<strong class="jy ic"> 20% </strong>组成，用于<em class="ku">测试</em>模型在之前<strong class="jy ic">从未</strong> <strong class="jy ic">见过</strong> <strong class="jy ic">的数据上的<strong class="jy ic">精度</strong>。</strong></li></ol><p id="8d98" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">通过这种分割，我们可以检查模型在<strong class="jy ic">每套</strong>上的表现，以深入了解<strong class="jy ic">如何进行</strong>训练过程，并在发生时发现<em class="ku">过度配合</em>。<em class="ku">此表</em>显示了不同的情况。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff lz"><img src="../Images/fa7c25f964659585d6a67dbef067ac01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XvSvKfde8u89TMwjkz3kg.png"/></div></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">Overfitting can be seen as the <strong class="bd lj">difference</strong> between the <strong class="bd lj">training</strong> and <strong class="bd lj">testing</strong> error.</figcaption></figure><p id="0157" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated"><strong class="jy ic"> <em class="ku">注意:</em> </strong>为了让这种技术工作，你需要确保两个部分都代表你的数据。一个<em class="ku">好的做法</em>是在<em class="ku">拆分</em>之前<strong class="jy ic">打乱</strong>数据集的顺序。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div class="fe ff kv"><img src="../Images/6f5a5a731199bf96f2677056b2c59860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*I5NeyJTfXnaTWluR6ZrW1w.png"/></div></figure><p id="e17a" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">过度拟合可能会非常令人沮丧，因为它会在<em class="ku">残酷地击败</em>你的<strong class="jy ic">希望</strong>之前<strong class="jy ic">提升</strong>你的<strong class="jy ic">希望</strong>。幸运的是，有一些技巧可以防止这种情况发生。</p><h1 id="80a5" class="iy iz ib bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv dt translated">如何防止过度拟合模型和数据</h1><p id="3d0d" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated"><em class="ku">首先是</em>，我们可以试着看看我们系统的<em class="ku">组件</em>来寻找解决方案。这意味着改变我们正在使用的<em class="ku">数据</em>，或者哪个<em class="ku">型号</em>。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff me"><img src="../Images/cb08997965c9ebdae3ffa09a468672c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNprBp4Jviw7RRhth84Jqw.png"/></div></div></figure><h2 id="f6b3" class="mf iz ib bd ja mg mh mi je mj mk ml ji kh mm mn jm kl mo mp jq kp mq mr ju ms dt translated">收集更多数据</h2><p id="f293" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated">你的模型只能<em class="ku">储存</em>这么多信息。这就意味着<strong class="jy ic">多了</strong> <strong class="jy ic">训练数据</strong>你就喂它，少了<strong class="jy ic"/><strong class="jy ic">很可能</strong>它就给<strong class="jy ic"/>过量了。原因是，随着你<strong class="jy ic">添加</strong>更多的<strong class="jy ic">数据</strong>，模型变得<strong class="jy ic">无法</strong>到<strong class="jy ic">过拟合</strong>所有样本，并且是<strong class="jy ic">迫使</strong>到<strong class="jy ic">泛化</strong>取得进展。</p><p id="3664" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">收集更多的示例应该是每个数据科学任务的第一步，因为更多的数据将导致<em class="ku">增加模型的</em> <em class="ku">准确性</em>，同时减少<em class="ku">过度拟合</em>的机会。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff mt"><img src="../Images/23a00f1ff991a5586a1e09cd84c6aa91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L392ucqge-zTsOJYieRN7A.png"/></div></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">The <strong class="bd lj">more data </strong>you get, the <strong class="bd lj">less</strong> likely the model is to <strong class="bd lj">overfit.</strong></figcaption></figure><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff me"><img src="../Images/103feb906be91afb4a24205564259baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QtcllICf19la5GJrCmAhmg.png"/></div></div></figure><h2 id="29af" class="mf iz ib bd ja mg mh mi je mj mk ml ji kh mm mn jm kl mo mp jq kp mq mr ju ms dt translated">数据增强和噪声</h2><p id="9909" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated">收集更多的数据是一个<em class="ku">繁琐</em>和<strong class="jy ic">昂贵</strong>的过程。如果做不到，就尽量让你的数据<em class="ku">显得</em>好像是<strong class="jy ic">更</strong> <strong class="jy ic">多样</strong>。为此，使用<a class="ae lk" href="https://hackernoon.com/stop-feeding-garbage-to-your-model-the-6-biggest-mistakes-with-datasets-and-how-to-avoid-them-3cb7532ad3b7" rel="noopener ugc nofollow" target="_blank">数据扩充技术</a>,这样每次模型处理样本时，都会与前一次略有不同。这将使模型<strong class="jy ic">更难</strong>为每个样本<em class="ku">学习</em> <em class="ku">参数</em>。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff mu"><img src="../Images/9bb109ef91aeba547b432acc9333ee93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CwznrhEFc9R9XhqN8LnsTA.png"/></div></div><figcaption class="lf lg fg fe ff lh li bd b be z ek"><strong class="bd lj">Each</strong> <strong class="bd lj">iteration</strong> sees as <strong class="bd lj">different</strong> <strong class="bd lj">variation</strong> of the original sample.</figcaption></figure><p id="6f62" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">另一个好的做法是添加<strong class="jy ic">噪声:</strong></p><ul class=""><li id="63dc" class="ll lm ib jy b jz la kd lb kh ln kl lo kp lp kt mv lr ls lt dt translated"><strong class="jy ic">到输入</strong>:这与数据扩充的目的相同，但也有助于使<strong class="jy ic">模型</strong> <strong class="jy ic">对</strong>到<em class="ku">自然扰动</em>具有鲁棒性，它可能会在野外遇到<strong class="jy ic"/>。</li><li id="1119" class="ll lm ib jy b jz lu kd lv kh lw kl lx kp ly kt mv lr ls lt dt translated"><strong class="jy ic">到输出</strong>:同样，这样会让训练更加多样化。</li></ul><p id="db06" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated"><strong class="jy ic"> <em class="ku">注意:</em> </strong>在这两种情况下，你都需要确保<strong class="jy ic">的噪音</strong>的幅度不要太<em class="ku">大</em>。否则，你可能会分别结束<em class="ku">淹没</em>输入的信息在噪声<em class="ku">中，或者</em>使输出<em class="ku">不正确。</em>两者都会阻碍训练进程。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff me"><img src="../Images/62301b623004155c17fe4f00277d5c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uv2699bEDsyDp_yfku5Itw.png"/></div></div></figure><h2 id="cd9d" class="mf iz ib bd ja mg mh mi je mj mk ml ji kh mm mn jm kl mo mp jq kp mq mr ju ms dt translated">简化模型</h2><p id="b33b" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated">如果，即使你现在有了所有的数据，你的模型<em class="ku">仍然</em>设法过拟合你的训练数据集，可能是模型<strong class="jy ic">太</strong> <strong class="jy ic">强大</strong>。然后你可以尝试<strong class="jy ic">降低模型的复杂度</strong>。</p><p id="a9a7" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">如前所述，一个模型只能过度拟合<em class="ku">到</em>到<em class="ku">多的</em>数据。通过逐步降低其复杂性——<em class="ku"/><strong class="jy ic"><em class="ku">参数</em> </strong> <em class="ku">在一个</em> <strong class="jy ic"> <em class="ku">神经网络</em> </strong> <em class="ku">等。</em> —你可以使模型<em class="ku">足够简单</em>使其<em class="ku">不会过度拟合，</em>但<em class="ku">足够复杂</em>以便<em class="ku">从你的数据中学习</em>。要做到这一点，根据模型的复杂性，查看两个数据集<strong class="jy ic">上的<strong class="jy ic">错误</strong>是很方便的。</strong></p><p id="5c93" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">这样做还有一个好处，就是让车型<strong class="jy ic">更轻</strong>、<strong class="jy ic">训练更快</strong>、<strong class="jy ic">跑</strong>、<strong class="jy ic">更快</strong>。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff mw"><img src="../Images/6b41cf25ab1006635f9303cbfcc118a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vuZxFMi5fODz2OEcpG-S1g.png"/></div></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">On the left, the model is too simple. On the right it overfits.</figcaption></figure></div><div class="ab cl mx my hc mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="hn ho hp hq hr"><h1 id="4c81" class="iy iz ib bd ja jb ne jd je jf nf jh ji jj ng jl jm jn nh jp jq jr ni jt ju jv dt translated">如何防止过度适应训练过程</h1><p id="f6a7" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated">第二种可能性是改变T2训练T3的方式。这包括改变<strong class="jy ic">损失函数、</strong>或者模型<em class="ku">在训练期间的作用方式</em>。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff me"><img src="../Images/4c3f1b906dd5ecb61546a166e85fd0c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieJ-F7G2zK5eNYcU1oDkEg.png"/></div></div></figure><h2 id="471f" class="mf iz ib bd ja mg mh mi je mj mk ml ji kh mm mn jm kl mo mp jq kp mq mr ju ms dt translated">提前终止</h2><p id="0854" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated">在大多数情况下，模型<strong class="jy ic">通过学习数据的正确分布来开始</strong>，并且在某一点上，开始过度拟合数据。通过识别此<strong class="jy ic"/><strong class="jy ic">发生</strong>的<em class="ku">时刻</em>，您可以在过拟合发生之前<strong class="jy ic">停止学习过程</strong> <em class="ku">。和以前一样，这是通过查看随着时间推移的<em class="ku">训练误差</em>来完成的。</em></p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff nj"><img src="../Images/7611cb3b23c0c2f133540a57c5564adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xWfbNW3arf39wxk4ZkI2Mw.png"/></div></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">When the <strong class="bd lj">testing error </strong>starts to <strong class="bd lj">increase</strong>, it’s time to stop!</figcaption></figure></div><div class="ab cl mx my hc mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="hn ho hp hq hr"><h1 id="9e8e" class="iy iz ib bd ja jb ne jd je jf nf jh ji jj ng jl jm jn nh jp jq jr ni jt ju jv dt translated">如何防止过度拟合—正则化</h1><p id="f47d" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated"><strong class="jy ic">正则化</strong>是模型的<strong class="jy ic">约束</strong>学习<strong class="jy ic">到<em class="ku">减少</em> <strong class="jy ic">过拟合</strong>的过程。它可以有许多不同的形式，我们将会看到其中的几种。</strong></p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff me"><img src="../Images/d59c2270579fef9382e3fc9a6ce40d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ca6abUQCrHeWS3E83lIeHQ.png"/></div></div></figure><h2 id="796c" class="mf iz ib bd ja mg mh mi je mj mk ml ji kh mm mn jm kl mo mp jq kp mq mr ju ms dt translated">L1和L2正规化</h2><p id="09e9" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated">最<em class="ku">强大的</em>和众所周知的正则化技术之一是<strong class="jy ic">向<strong class="jy ic">损失函数</strong>添加一个惩罚</strong>。最常见的叫做<a class="ae lk" href="http://mathworld.wolfram.com/L1-Norm.html" rel="noopener ugc nofollow" target="_blank"><em class="ku"/></a><a class="ae lk" href="http://mathworld.wolfram.com/L2-Norm.html" rel="noopener ugc nofollow" target="_blank"><em class="ku"/></a>:</p><ol class=""><li id="7a10" class="ll lm ib jy b jz la kd lb kh ln kl lo kp lp kt lq lr ls lt dt translated"><strong class="jy ic"> L1 </strong> <strong class="jy ic">罚值</strong>旨在最小化权重的<strong class="jy ic">绝对值</strong> <strong class="jy ic">值</strong></li><li id="f4c4" class="ll lm ib jy b jz lu kd lv kh lw kl lx kp ly kt lq lr ls lt dt translated"><strong class="jy ic"> L2罚值</strong>旨在最小化权重的<strong class="jy ic">平方</strong>和<strong class="jy ic">大小</strong>。</li></ol><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div class="fe ff nk"><img src="../Images/1ea12c5f906bd9f2776a6b38dc422058.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*JP9VzwxsRXjocb3WgVOTxA.png"/></div></figure><p id="27b4" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">由于处罚，该模型被迫<em class="ku">使</em> <em class="ku">在其重量上妥协</em>，因为它不能再任意使<strong class="jy ic"/><strong class="jy ic">大</strong>。这使得模型<strong class="jy ic">更加</strong> <strong class="jy ic">通用</strong>，有助于打击过拟合。</p><p id="8621" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated"><em class="ku"> L1 </em>惩罚具有额外的优点，它强制执行<a class="ae lk" href="http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ic">特征选择</strong></a><strong class="jy ic"/>其中<strong class="jy ic"> </strong>意味着它倾向于将<em class="ku">较少的</em> <em class="ku">有用的</em>参数设置为0。这有助于识别<em class="ku">数据集</em>中<strong class="jy ic">最相关的特征</strong>。不利的一面是，通常<strong class="jy ic">不如<strong class="jy ic">计算效率高</strong>不如<em class="ku"> L2 </em>罚。</strong></p><p id="fcff" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">这是权重矩阵的样子。注意<strong class="jy ic"> L1 </strong>矩阵是如何<strong class="jy ic">稀疏</strong>有很多零，<strong class="jy ic"> L2 </strong>矩阵有<em class="ku">略</em> <strong class="jy ic">较小</strong> <strong class="jy ic">权重</strong>。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff nl"><img src="../Images/2c3ffbe43caff481fb533ea6880d092b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fFYf-XJ3LrmDgO68YWjw4Q.png"/></div></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">(The darker the color, the bigger the weight. White cases basically mean that the weight is 0)</figcaption></figure><p id="ffb3" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated"><strong class="jy ic">另一种</strong>可能是<a class="ae lk" href="https://arxiv.org/abs/1505.05424" rel="noopener ugc nofollow" target="_blank">在训练过程中给<em class="ku">参数</em> </a>添加噪声，这有助于<strong class="jy ic">泛化</strong>。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="fe ff me"><img src="../Images/eb170ff60dccec4bdc709dc5eec862d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G8K0qouZMklY1OAT2903bQ.png"/></div></div></figure><h2 id="9381" class="mf iz ib bd ja mg mh mi je mj mk ml ji kh mm mn jm kl mo mp jq kp mq mr ju ms dt translated">深度学习:辍学和Dropconnect</h2><p id="75d9" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated">这种<strong class="jy ic">极其有效的</strong>技术是专门针对<strong class="jy ic">深度学习的，</strong>因为它依赖于这样一个事实，即<em class="ku">神经网络</em>处理从一个<strong class="jy ic">层</strong>到下一层的信息。想法是在训练期间随机停用<strong class="jy ic">神经元</strong> ( <a class="ae lk" href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ku">脱落</em> </a>)或<strong class="jy ic">连接</strong> ( <em class="ku">脱落连接</em>)。</p><figure class="kw kx ky kz fq hw fe ff paragraph-image"><div class="fe ff nm"><img src="../Images/c23c50f750306331a2396d5379d5f7b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*DfrGMZdwpRQITkdmOtKwtg.jpeg"/></div></figure><p id="100a" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">这迫使网络成为<strong class="jy ic">冗余</strong>，因为它不再能够<strong class="jy ic">依赖<em class="ku">特定</em> <strong class="jy ic">神经元</strong>或<strong class="jy ic">连接</strong>来提取<em class="ku">特定</em> <strong class="jy ic">特征</strong>。一旦训练完成，所有的神经元和连接都恢复了。已经表明，这种技术<em class="ku">在某种程度上</em> <em class="ku">相当于</em>具有<a class="ae lk" href="https://arxiv.org/abs/1706.06859" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ic">集合</strong>方法</a>，这<strong class="jy ic">有利于</strong> <strong class="jy ic">一般化，</strong>从而减少过拟合。</strong></p><h1 id="c730" class="iy iz ib bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv dt translated">结论</h1><p id="0190" class="pw-post-body-paragraph jw jx ib jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hn dt translated">如你所知，过度拟合是数据科学家必须面对的主要问题之一。如果你不知道如何让<em class="ku">停止</em>它，这可能是一个<em class="ku">真正的痛苦</em>去处理。使用本文介绍的技术，您现在应该能够<em class="ku">防止</em>您的模型在<strong class="jy ic">学习过程中作弊</strong>，并获得您应得的<strong class="jy ic">结果</strong>！</p><p id="416d" class="pw-post-body-paragraph jw jx ib jy b jz la kb kc kd lb kf kg kh lc kj kk kl ld kn ko kp le kr ks kt hn dt translated">🎉你已经到达终点了！我希望你喜欢这篇文章。如果你做了，请随意喜欢它，分享它，向你的猫解释它，在medium上跟随我，或者做任何你想做的事情！🎉</p><blockquote class="nn no np"><p id="01bd" class="jw jx ku jy b jz la kb kc kd lb kf kg nq lc kj kk nr ld kn ko ns le kr ks kt hn dt translated"><strong class="jy ic">如果你喜欢数据科学和人工智能，</strong> <a class="ae lk" href="http://eepurl.com/cATXvT" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ic">订阅时事通讯</strong> </a> <strong class="jy ic">接收文章更新和更多内容！</strong></p></blockquote></div></div>    
</body>
</html>