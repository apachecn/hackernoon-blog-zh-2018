<html>
<head>
<title>Why Coding Multi-Agent Systems is Hard</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么编写多智能体系统很难</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/why-coding-multi-agent-systems-is-hard-2064e93e29bb?source=collection_archive---------4-----------------------#2018-03-25">https://medium.com/hackernoon/why-coding-multi-agent-systems-is-hard-2064e93e29bb?source=collection_archive---------4-----------------------#2018-03-25</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff hs"><img src="../Images/45673824036c375cb15d713f6d3c4015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3UNWV9Ys8wg7BWp7iSGuGw.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><a class="ae ih" href="http://www.geekinsider.com/newest-addition-pantheon-sentient-robot-films-chappie/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><div class=""/><blockquote class="jh ji jj"><p id="7608" class="jk jl jm jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated">“三十五年人工智能研究的主要教训是，难的问题容易，容易的问题难。”平克(1994)，《语言本能》</p></blockquote><p id="dcc0" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt km translated">我认为编写软件代理程序来收集图上的宝藏是小菜一碟。我完全错了。事实证明，对代理进行编码，使他们不会愚蠢地行事，在本质上是很困难的。</p><h1 id="df81" class="kv kw ik bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls dt translated">定义良好的多代理设置</h1><blockquote class="jh ji jj"><p id="a25c" class="jk jl jm jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated">“一个<strong class="jn il">代理</strong>可以被视为通过<strong class="jn il">传感器</strong>感知其环境，并通过<strong class="jn il">效应器</strong>对该环境做出反应的任何东西。”<a class="ae ih" href="https://people.eecs.berkeley.edu/~russell/aima1e/chapter02.pdf" rel="noopener ugc nofollow" target="_blank">人工智能:现代方法</a>，斯图尔特·拉塞尔<strong class="jn il"> </strong>和<strong class="jn il"> </strong>彼得·诺维格</p></blockquote><figure class="lu lv lw lx fq hw fe ff paragraph-image"><div class="fe ff lt"><img src="../Images/989dee551740d0d71d6b6605a2b4e7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*-cRsauWjIjLgs_OkuloGKw.png"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek">Simulation of a multi-agent system collecting treasures using <a class="ae ih" href="http://graphstream-project.org/" rel="noopener ugc nofollow" target="_blank">GraphStream Library</a></figcaption></figure><p id="698d" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jn il">这里是一个简单的多主体问题。让n个代理在一个全连通图中移动来收集财宝。代理人的行动、感知和交流都受到限制。它们只能观察和移动到与它们直接相连的节点，并与足够近的代理通信。</strong></p><p id="94e6" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jn il">特工</strong>有三种:<em class="jm">探险者</em>、<em class="jm">收藏家、</em>和<em class="jm">无限背包特工</em>。探险者注定要探索地图，因为他们不允许挑选宝藏。能够收集的人是收集者，但是他们不能携带太多，必须将收集到的宝物交给无限背包代理。</p><p id="d0e5" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">智能体的感知能力有限，但能记住过去的观察结果。每个智能体都有自己的世界表示，自己的图，是真实图的子图。他们的子图是他们访问过的所有节点的内存，以及他们曾经看到或拍摄的边。他们必须将这个图传达给其他人，这样他们就可以共享所有子图的重构。</p><p id="035f" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><a class="ae ih" href="http://jade.tilab.com/" rel="noopener ugc nofollow" target="_blank"><strong class="jn il">JADE</strong></a><strong class="jn il">(Java Agent development Framework)将用于实现所谓的“行为”</strong>(教程可以在<a class="ae ih" href="http://jade.tilab.com/doc/tutorials/JADEProgramming-Tutorial-for-beginners.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>，或者<a class="ae ih" href="https://gitlab.com/herpsonc/startJade" rel="noopener ugc nofollow" target="_blank">这里</a>)。在这个<a class="ae ih" href="https://www.cs.cmu.edu/~softagents/multi.html" rel="noopener ugc nofollow" target="_blank">多代理系统</a>框架中，一个<em class="jm">行为</em>是一个代理将要执行的一组指令。在每个回合中，每个代理都依次执行它的每个行为。</p><p id="fde2" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jn il">你的目标</strong>:实现代理的行为，让他们在一定的时间内收集尽可能多的财宝。</p><p id="c9b5" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">看起来很简单，对吧？</p><p id="34f1" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">(注意:这个项目是多智能体系统介绍的一部分(ANDROIDE的一门课程，我目前在UPMC攻读人工智能硕士学位)。它的灵感来自于生存恐怖游戏<a class="ae ih" href="https://en.wikipedia.org/wiki/Hunt_the_Wumpus" rel="noopener ugc nofollow" target="_blank"> Hunt The Wumpus </a>，在该项目的完整版本中，代理需要处理一个四处游荡的恐怖Wumpus)。</p><h1 id="3cf2" class="kv kw ik bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls dt translated">不平凡的行为</h1><p id="3616" class="pw-post-body-paragraph jk jl ik jn b jo ly jq jr js lz ju jv kj ma jy jz kk mb kc kd kl mc kg kh ki hn dt translated">想象两个代理在一条长长的走廊上向相反的方向移动。图中每个节点只能有一个智能体，因此它们必须协调行动，以免阻塞道路。必须实现特定的协议来考虑这种情况。</p><figure class="lu lv lw lx fq hw"><div class="bz el l di"><div class="md me l"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek">Conflict of agents in a simulation: MyExplorerAgent2 is blocking the two others</figcaption></figure><h2 id="81d2" class="mf kw ik bd kx mg mh mi lb mj mk ml lf kj mm mn lj kk mo mp ln kl mq mr lr ms dt translated">协调</h2><p id="1d0b" class="pw-post-body-paragraph jk jl ik jn b jo ly jq jr js lz ju jv kj ma jy jz kk mb kc kd kl mc kg kh ki hn dt translated"><strong class="jn il">特工感知有限，能力各异。因此，合作是必要条件。当冲突发生时，必须有一个协议来解除这种情况。他们必须分享他们的子图，看谁更接近高度连接的节点，并就谁将移动达成一致。</strong></p><p id="6e54" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">探险家代理还必须就谁将探索未知图的哪一部分达成一致，以优化他们的行动并防止冲突。</p><h2 id="617f" class="mf kw ik bd kx mg mh mi lb mj mk ml lf kj mm mn lj kk mo mp ln kl mq mr lr ms dt translated">信息交流</h2><p id="6561" class="pw-post-body-paragraph jk jl ik jn b jo ly jq jr js lz ju jv kj ma jy jz kk mb kc kd kl mc kg kh ki hn dt translated">在多智能体设置中交换信息以使每个智能体访问全局知识的过程被称为<a class="ae ih" href="https://www.sciencedirect.com/science/article/pii/0012365X73901210" rel="noopener ugc nofollow" target="_blank"> <em class="jm">八卦问题</em> </a> <em class="jm">。</em></p><figure class="lu lv lw lx fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mt"><img src="../Images/e7a6494c29d7b8adc0ed9ee50c0fce9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*02XOLDdJf-KBctma1RyhCw.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><a class="ae ih" href="https://jisajournal.springeropen.com/articles/10.1186/1869-0238-4-14" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="3200" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">例如，让我们假设集合{1，2，…，n}中的每个代理都确切地知道一条信息，称为秘密。然后，一个非常简单的协议是让代理1给2，3，…，n打电话，了解他们的秘密。然后，当1知道了所有的秘密，他打电话给2，…，n告诉他们那些秘密，然后所有人都知道了所有的秘密。总共进行了n-1+n-1 = 2n-2次调用。实际上，<a class="ae ih" href="https://arxiv.org/abs/1511.00867" rel="noopener ugc nofollow" target="_blank">最优解</a>需要2n-4次调用，接近我们的简单算法。</p><p id="7455" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">然而，在我们的问题中，<strong class="jn il">直到所有的节点都被探索过之后，总信息才被完全知晓</strong>，这使得算法稍微复杂一些，因为总知识是动态的(代理探索图越多，他们的总知识就越多)。</p><p id="0071" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jn il">这就是优化妥协出现的地方。</strong>n个<em class="jm">代理之间必须交换的消息的最佳数量是多少，这样他们才能知道所有的秘密？更多的信息意味着更好的全球知识和更好的协调。然而，由于有数千个代理和数百万个节点，<strong class="jn il">每毫秒发送数千条消息的成本是不可忽略的，并且成为计算负担。</strong></em></p><h2 id="1f09" class="mf kw ik bd kx mg mh mi lb mj mk ml lf kj mm mn lj kk mo mp ln kl mq mr lr ms dt translated">异步通信</h2><p id="75bf" class="pw-post-body-paragraph jk jl ik jn b jo ly jq jr js lz ju jv kj ma jy jz kk mb kc kd kl mc kg kh ki hn dt translated">代理之间的通信是<em class="jm">异步的</em>。由于代理的执行是<a class="ae ih" href="https://en.wikipedia.org/wiki/Distributed_computing" rel="noopener ugc nofollow" target="_blank">分布式的</a>，因此没有<strong class="jn il">全局时钟</strong>来同步代理的动作。此外，当交换信息时，每个代理都有一个包含来自其他代理的消息的邮箱，因此<strong class="jn il">通信可能会延迟。在此延迟期间，一个代理可能会移动到很远的地方，并且<strong class="jn il">永远不会回复原始消息</strong>。</strong></p><h2 id="5635" class="mf kw ik bd kx mg mh mi lb mj mk ml lf kj mm mn lj kk mo mp ln kl mq mr lr ms dt translated"><strong class="ak">联盟形成</strong></h2><figure class="lu lv lw lx fq hw fe ff paragraph-image"><div class="fe ff mu"><img src="../Images/36f539d0618a6ae3d1fe1b33bd5dc89c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*JyATMpZWPHtJ8P6ys9ePKw.png"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek">Example of coalition formation : <a class="ae ih" href="https://www.slideshare.net/SurSamtani/coalition-formation-and-price-of-anarchy-in-cournot-oligopolies" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="faf4" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">某些目标是无法单独实现的(例如，举起重物)。因此，代理人可能会同意组成一组代理人，称为<em class="jm">联盟</em>，以实现共同的目标。</p><p id="570f" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">有了具备三种不同<em class="jm">必要</em>技能(探索、收集和积累)的三个智能体，<strong class="jn il">必须形成至少三个智能体的<em class="jm">联盟</em></strong>。因此，必须实现创建和更新联盟的协议。一种可能是使用<a class="ae ih" href="https://www.degruyter.com/view/books/9781400881970/9781400881970-018/9781400881970-018.xml" rel="noopener ugc nofollow" target="_blank"> Shapley值</a>(由代理联盟创造的剩余)来确定哪个联盟是最有价值的。</p></div><div class="ab cl mv mw hc mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hn ho hp hq hr"><p id="6c36" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">即使是一个简单的问题设置，<strong class="jn il">几个障碍很快出现，算法的复杂性似乎无法克服</strong>。当试图构建能够以类似人类的方式运行的人工智能算法时，这是一个反复出现的现象。</p><h1 id="4071" class="kv kw ik bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls dt translated">构建展示简单行为的人工智能很难</h1><blockquote class="jh ji jj"><p id="eaf4" class="jk jl jm jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hn dt translated">“让计算机在智力测试或玩跳棋时表现出成人水平的表现相对容易，但在感知和移动性方面，让它们拥有一岁儿童的技能却很难或不可能。”莫拉维克(1988)心智儿童</p></blockquote><p id="04a7" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">如果我们用人类代替代理人，我相信他们会很快明白如何在这场游戏中获胜，交流他们在图表中看到的东西，并形成联盟来收集最多的宝藏。然而，<strong class="jn il">对智能代理实施严格的行为规则被证明是惊人的困难。</strong></p><p id="036f" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">这就是<a class="ae ih" href="https://en.wikipedia.org/wiki/Moravec%27s_paradox#" rel="noopener ugc nofollow" target="_blank">莫拉维克悖论</a>:</p><blockquote class="nc"><p id="569d" class="nd ne ik bd nf ng nh ni nj nk nl ki ek translated">对人类来说容易的事情对机器来说却非常困难</p></blockquote><p id="0465" class="pw-post-body-paragraph jk jl ik jn b jo nm jq jr js nn ju jv kj no jy jz kk np kc kd kl nq kg kh ki hn dt translated">在下棋方面，AI达到了超人类的表现。但是对于基本的人类行为，如行走或协调行动以探索地图，人工智能算法令人惊讶地更难。</p><p id="dafb" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">国际象棋大师加里·卡斯帕罗夫在<a class="ae ih" href="https://www.amazon.com/Deep-Thinking-Machine-Intelligence-Creativity/dp/161039786X" rel="noopener ugc nofollow" target="_blank"> <em class="jm">深度思考</em> </a>中作出如下表述:任何足够先进的算法在同时进行的一场比赛中击败20名顶级棋手并不困难。<strong class="jn il">但是没有人工智能(在机器人中)可以在拥挤的酒吧里走来走去，自己移动棋子</strong>。</p><figure class="lu lv lw lx fq hw fe ff paragraph-image"><div class="fe ff nr"><img src="../Images/e37babec1a7bb46a7118a7d487e23aa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*brejKdggHUdQE-EFRfSZIg.jpeg"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek">Source: <a class="ae ih" href="https://www.youtube.com/watch?v=adFd0f7K46w" rel="noopener ugc nofollow" target="_blank">DARPA robots falling down</a></figcaption></figure><h2 id="ba64" class="mf kw ik bd kx mg mh mi lb mj mk ml lf kj mm mn lj kk mo mp ln kl mq mr lr ms dt translated">机器学习在非常特殊的环境下工作</h2><p id="45c4" class="pw-post-body-paragraph jk jl ik jn b jo ly jq jr js lz ju jv kj ma jy jz kk mb kc kd kl mc kg kh ki hn dt translated"><em class="jm">但是我们为什么不用最新的机器学习(ML)算法来解决我们的问题呢？… </em>你说。嗯，<strong class="jn il">纯ML算法只能用于某些任务</strong>。</p><p id="c9a3" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">是的，强化学习(RL)算法风靡一时，可以解决令人震惊的难题，例如在雅达利游戏或围棋游戏<a class="ae ih" href="https://deepmind.com/blog/alphago-zero-learning-scratch/" rel="noopener ugc nofollow" target="_blank">中达到</a><a class="ae ih" href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank">超人的水平。但是<strong class="jn il">那些游戏都是全可视的游戏，数据输入很小</strong>，我们的寻宝问题就不是这样，地图在开始的时候并不是全可视的。</a></p><figure class="lu lv lw lx fq hw fe ff paragraph-image"><div class="fe ff ns"><img src="../Images/638a5b635b63637415e7a895bfd35c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*pQFdcaxdroezdIVCL_XLLg.png"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek">Source: <a class="ae ih" href="http://Deep Reinforcement Learning Doesn't Work Yet" rel="noopener ugc nofollow" target="_blank">Deep Reinforcement Learning Doesn’t Work Yet</a> (Feb. 2018)</figcaption></figure><p id="9c2f" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">但是OpenAI不是正在研究一个多智能体系统来使用机器学习算法在5比5的设置中击败Dota 2中的人类吗？...你说。</p><p id="c36d" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">是的，当<a class="ae ih" href="https://blog.openai.com/dota-2/" rel="noopener ugc nofollow" target="_blank">在Dota 2中以1比1的比分击败</a>世界冠军时，OpenAI已经展示了令人印象深刻的结果。但是<strong class="jn il">主要是因为他们强大的计算能力</strong>,<a class="ae ih" href="https://www.quora.com/How-did-OpenAI-create-the-Dota-2-game-bot-that-beats-professionals" rel="noopener ugc nofollow" target="_blank">不是人工智能的突破</a>。</p><p id="1585" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">他们的目标是在5对5的环境中获胜，使用<a class="ae ih" href="https://blog.openai.com/more-on-dota-2/" rel="noopener ugc nofollow" target="_blank">一个580万游戏的数据集</a>。因此，他们似乎正在用完整的机器学习方法(从人类游戏中学习)来解决多智能体问题，并且似乎缺少了多智能体系统的自上而下的方法。</p><p id="635f" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated"><strong class="jn il">代理人不推断，不归纳</strong>。纯ML可以用于单个智能体或完全可观测的系统，但对于不完全已知世界中的多智能体系统，必须采用更通用的方法。</p><h2 id="b418" class="mf kw ik bd kx mg mh mi lb mj mk ml lf kj mm mn lj kk mo mp ln kl mq mr lr ms dt translated">我们不知道如何实现可伸缩的行为</h2><p id="54e9" class="pw-post-body-paragraph jk jl ik jn b jo ly jq jr js lz ju jv kj ma jy jz kk mb kc kd kl mc kg kh ki hn dt translated">只有两个探员在走廊上向相反的方向走，我们遇到了一个问题。实施一项协议来处理这一具体问题是可能的。</p><blockquote class="nc"><p id="038c" class="nd ne ik bd nf ng nh ni nj nk nl ki ek translated">但是400个节点的地图上有100个代理怎么办？</p></blockquote><figure class="nu nv nw nx ny hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff nt"><img src="../Images/bb3397f9f603cf8d8b63f21ab76c19e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5CVF4ZbusmMq_7p482uKnQ.png"/></div></div></figure><p id="c8e2" class="pw-post-body-paragraph jk jl ik jn b jo jp jq jr js jt ju jv kj jx jy jz kk kb kc kd kl kf kg kh ki hn dt translated">在为少量代理人硬编码特性和拥有多代理人系统的可扩展T21和可推广实现之间存在差距。</p><h2 id="1276" class="mf kw ik bd kx mg mh mi lb mj mk ml lf kj mm mn lj kk mo mp ln kl mq mr lr ms dt translated">需要做些什么</h2><p id="19f1" class="pw-post-body-paragraph jk jl ik jn b jo ly jq jr js lz ju jv kj ma jy jz kk mb kc kd kl mc kg kh ki hn dt translated">必须通过研究开发特定的多智能体协议来解决这类问题。<a class="ae ih" href="https://deepmind.com/blog/alphago-zero-learning-scratch/" rel="noopener ugc nofollow" target="_blank">没有先验知识的学习</a>不会教会智能体如何沟通，因为搜索空间太大。纯数据驱动的方法不会有任何结果。</p><h1 id="61a2" class="kv kw ik bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls dt translated">结论</h1><p id="a0fd" class="pw-post-body-paragraph jk jl ik jn b jo ly jq jr js lz ju jv kj ma jy jz kk mb kc kd kl mc kg kh ki hn dt translated">实现一个能够解决寻宝问题的算法被证明比看起来要困难得多。构想能够解决简单问题的多智能体系统绝非易事。<strong class="jn il">机器学习算法在最近十年取得了很大的成果，但单靠它们并不能解决所有的人工智能问题。</strong></p></div></div>    
</body>
</html>