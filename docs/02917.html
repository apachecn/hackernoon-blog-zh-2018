<html>
<head>
<title>How to deal with Vanishing/Exploding gradients in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何处理Keras中的消失/爆炸渐变</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/how-to-deal-with-vanishing-exploding-gradients-in-keras-b4ab6e5f3a0a?source=collection_archive---------21-----------------------#2018-04-02">https://medium.com/hackernoon/how-to-deal-with-vanishing-exploding-gradients-in-keras-b4ab6e5f3a0a?source=collection_archive---------21-----------------------#2018-04-02</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div class="fe ff hs"><img src="../Images/683bcf424b47af04aaaae9165ce103c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*LO0nU7_fpCmVePJfqtcgFg.jpeg"/></div><figcaption class="hz ia fg fe ff ib ic bd b be z ek">Blocks of rocks vanishing at the distance</figcaption></figure><div class=""/><p id="1e98" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如果你已经训练了你的深度学习模型一段时间，并且它的准确率仍然相当低。你可能想检查它是否正在遭受消失或爆炸梯度。</p><h1 id="6735" class="ka kb if bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dt translated">消失/爆炸渐变简介</h1><h2 id="a2ed" class="ky kb if bd kc kz la lb kg lc ld le kk jn lf lg ko jr lh li ks jv lj lk kw ll dt translated">消失渐变</h2><p id="efe5" class="pw-post-body-paragraph jc jd if je b jf lm jh ji jj ln jl jm jn lo jp jq jr lp jt ju jv lq jx jy jz hn dt translated">反向传播很难改变非常深的神经网络中早期层的权重。在梯度下降期间，当它从最后一层返回到第一层时，梯度值在每一步上乘以权重矩阵，因此梯度可以指数地快速下降到零。结果，网络不能有效地学习参数。</p><p id="7770" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">使用非常深的网络可以表示非常复杂的功能。它可以学习许多不同抽象层次的特性，从边缘(较低层)到非常复杂的特性(较深层)。例如，早期的ImageNet模型，如VGG16和VGG19，正在努力通过添加更多的层来实现更高的图像分类精度。但是网络变得越深，就越难更新早期层的参数。</p><figure class="ls lt lu lv fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="fe ff lr"><img src="../Images/8db03e504acbc89b31cc5e0561e20742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NHgjysMp3gqRWgqU.jpg"/></div></div><figcaption class="hz ia fg fe ff ib ic bd b be z ek">Neural network backprop</figcaption></figure><p id="4cc0" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">消失梯度也出现在具有递归神经网络的序列模型中。导致它们在捕获长期依赖关系时效率低下。</p><p id="4051" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">以这句话为例。该模型被训练以生成句子。</p><blockquote class="ma mb mc"><p id="2cd4" class="jc jd md je b jf jg jh ji jj jk jl jm me jo jp jq mf js jt ju mg jw jx jy jz hn dt translated">“那些<strong class="je ig">猫</strong>抓到了一条鱼，…..，<strong class="je ig">他们</strong>非常高兴</p></blockquote><p id="bf7a" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">RNN需要记住单词“<strong class="je ig">猫</strong>”的复数形式，以便在后面的句子中生成单词“<strong class="je ig">它们</strong>”。</p><p id="a0f1" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这是一个展开的循环网络，展示了这个想法。</p><figure class="ls lt lu lv fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="fe ff mh"><img src="../Images/f43bb22f54ec67f1c7ff51c569c269bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_T6hOoK7IXvnBJE0.jpg"/></div></div><figcaption class="hz ia fg fe ff ib ic bd b be z ek">RNN backprop over time</figcaption></figure><h2 id="f044" class="ky kb if bd kc kz la lb kg lc ld le kk jn lf lg ko jr lh li ks jv lj lk kw ll dt translated">爆炸渐变</h2><p id="d26c" class="pw-post-body-paragraph jc jd if je b jf lm jh ji jj ln jl jm jn lo jp jq jr lp jt ju jv lq jx jy jz hn dt translated">相比于消失渐变，爆炸渐变更容易实现。顾名思义,“爆炸”。在训练过程中，它会导致模型的参数变得非常大，以至于即使输入中非常小的变化也会导致后面层的输出发生很大的更新。我们可以通过简单地观察层权重的值来发现问题。有时会溢出，值变成NaN。</p><p id="cfa1" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在Keras中，您可以将层的权重作为Numpy数组列表来查看。</p><figure class="ls lt lu lv fq hw"><div class="bz el l di"><div class="mi mj l"/></div></figure><h1 id="8218" class="ka kb if bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dt translated">解决方法</h1><p id="37e6" class="pw-post-body-paragraph jc jd if je b jf lm jh ji jj ln jl jm jn lo jp jq jr lp jt ju jv lq jx jy jz hn dt translated">理解消失/爆炸梯度可能如何发生。这里有一些你可以在Keras框架中应用的简单解决方案。</p><h2 id="8f64" class="ky kb if bd kc kz la lb kg lc ld le kk jn lf lg ko jr lh li ks jv lj lk kw ll dt translated">在序列模型中使用LSTM/GRU</h2><p id="8e41" class="pw-post-body-paragraph jc jd if je b jf lm jh ji jj ln jl jm jn lo jp jq jr lp jt ju jv lq jx jy jz hn dt translated">普通的递归神经网络没有复杂的机制来“捕获”长期的依赖性。相反，像LSTM/GRU这样的现代RNN引入了“<strong class="je ig">盖茨</strong>的概念，来人为地保留那些长期记忆。</p><p id="61b8" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">简单来说，在GRU(门控循环单元)，有两个“门”。</p><p id="e202" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">一个称为<strong class="je ig">更新门</strong>的门决定是否用候选值更新当前存储单元。候选值由先前的存储单元输出和当前输入来计算。与普通RNN相比，该候选值将被直接用于替换存储单元值。</p><p id="86ba" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">第二门是<strong class="je ig">相关门</strong>告知先前存储单元输出与计算当前候选值的相关程度。</p><p id="f1e4" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在Keras中，将LSTM/GRU层应用到您的网络非常简单。</p><p id="1640" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里是一个包含LSTM层的最小模型，可以应用于情感分析。</p><figure class="ls lt lu lv fq hw"><div class="bz el l di"><div class="mi mj l"/></div></figure><p id="5046" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如果你很难选择LSTM或格鲁。LSTM是更强大的捕捉远程关系，但计算成本比格鲁。在大多数情况下，GRU应该足以进行顺序处理。例如，如果你只是想快速训练一个模型作为概念验证，GRU是正确的选择。当您想要提高现有模型的准确性时，您可以用LSTM替换现有的RNN，并训练更长的时间。</p><h2 id="7a63" class="ky kb if bd kc kz la lb kg lc ld le kk jn lf lg ko jr lh li ks jv lj lk kw ll dt translated">使用剩余网络</h2><p id="baeb" class="pw-post-body-paragraph jc jd if je b jf lm jh ji jj ln jl jm jn lo jp jq jr lp jt ju jv lq jx jy jz hn dt translated">剩余网络的思想是允许通过“捷径”或“跳过连接”直接反向传播到更早的层。</p><figure class="ls lt lu lv fq hw fe ff paragraph-image"><div class="fe ff mk"><img src="../Images/1feb0268be04f83b1dd28400d45cebae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/0*k2somZNNCWo6Ih3R.jpg"/></div><figcaption class="hz ia fg fe ff ib ic bd b be z ek">shortcut</figcaption></figure><p id="5b87" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">ResNet块的详细实现超出了本文的范围，但是我将向您展示在Keras中实现“标识块”是多么容易。“相同”意味着块输入激活与输出激活具有相同的维度。</p><figure class="ls lt lu lv fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="fe ff ml"><img src="../Images/5da8f0b24cf26428d8d0380f6bbc3cfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*N20-2BZGLGU6kuyb.jpg"/></div></div><figcaption class="hz ia fg fe ff ib ic bd b be z ek">Identity ResNet block</figcaption></figure><p id="2a17" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这是这个身份模块的Keras代码。</p><figure class="ls lt lu lv fq hw"><div class="bz el l di"><div class="mi mj l"/></div></figure><p id="d73b" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">还有另一个ResNet块，称为卷积块，当输入和输出维度不匹配时可以使用它。</p><p id="5c21" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">准备好必要的ResNet块后，我们可以将它们堆叠在一起，形成一个像ResNet50一样的深度ResNet模型，您可以轻松地用Keras加载它。</p><figure class="ls lt lu lv fq hw"><div class="bz el l di"><div class="mi mj l"/></div></figure><figure class="ls lt lu lv fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="fe ff mm"><img src="../Images/aa05701f220e83708f64260ccb302e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*c06lvpzB7npTt0w6.png"/></div></div><figcaption class="hz ia fg fe ff ib ic bd b be z ek">ResNet50</figcaption></figure><h2 id="e68c" class="ky kb if bd kc kz la lb kg lc ld le kk jn lf lg ko jr lh li ks jv lj lk kw ll dt translated">使用ReLu激活代替Sigmoid/Tanh</h2><p id="5237" class="pw-post-body-paragraph jc jd if je b jf lm jh ji jj ln jl jm jn lo jp jq jr lp jt ju jv lq jx jy jz hn dt translated">Sigmoid函数将激活值挤压在0~1之间。而双曲正切函数将激活值挤压在-1~1之间。</p><figure class="ls lt lu lv fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="fe ff mn"><img src="../Images/f6e903a7c8a69740ed3736e84953dff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*N57zaTNlfmRrxkDT.jpg"/></div></div><figcaption class="hz ia fg fe ff ib ic bd b be z ek">Sigmoid</figcaption></figure><p id="712f" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">可以看到，随着预激活的绝对值变大(x轴)，输出激活值不会有太大变化。它将是0或1。如果该层陷入这种状态，模型将拒绝更新其权重。</p><p id="ed9e" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">另一方面，这里是ReLu激活功能。</p><figure class="ls lt lu lv fq hw fe ff paragraph-image"><div class="fe ff mo"><img src="../Images/2366999407269430dd6bf21b08842bbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/0*ti5s1QutJnCXURjK.png"/></div><figcaption class="hz ia fg fe ff ib ic bd b be z ek">ReLu</figcaption></figure><p id="4ac4" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">对于随机初始化的网络，只有大约50%的隐藏单元被激活(具有非零输出)。这就是所谓的稀疏激活。</p><p id="9231" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">你可能会担心ReLu的零部分，它可能会完全关闭一个神经系统。然而，实验结果往往与这一假设相矛盾，表明硬零实际上可以帮助监督训练。我们假设，只要梯度可以沿着某些路径传播，硬非线性就不会造成伤害。</p><p id="0e5a" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">ReLu的另一个好处是易于实现，只需要比较、加法和乘法。因此它在计算上更有效。</p><p id="5e6d" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在Keras中应用ReLu也很容易。</p><figure class="ls lt lu lv fq hw"><div class="bz el l di"><div class="mi mj l"/></div></figure><h2 id="1009" class="ky kb if bd kc kz la lb kg lc ld le kk jn lf lg ko jr lh li ks jv lj lk kw ll dt translated">重量初始化</h2><ul class=""><li id="6734" class="mp mq if je b jf lm jj ln jn mr jr ms jv mt jz mu mv mw mx dt translated">权重应该被随机初始化以打破对称性。</li><li id="bd33" class="mp mq if je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">然而，可以将偏差初始化为零。只要权重被随机初始化，对称性仍然被破坏。</li><li id="296d" class="mp mq if je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">不要初始化太大的值。</li></ul><p id="444e" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">Keras默认的权重初始化器是<a class="ae nd" href="https://keras.io/initializers/#glorot_uniform" rel="noopener ugc nofollow" target="_blank"> glorot_uniform </a> aka。泽维尔统一初始化器。默认<strong class="je ig">偏差</strong>初始值为“零”。所以我们应该可以默认了。</p><h2 id="b8ed" class="ky kb if bd kc kz la lb kg lc ld le kk jn lf lg ko jr lh li ks jv lj lk kw ll dt translated">爆炸渐变的渐变剪辑</h2><p id="1a59" class="pw-post-body-paragraph jc jd if je b jf lm jh ji jj ln jl jm jn lo jp jq jr lp jt ju jv lq jx jy jz hn dt translated">顾名思义，在反向投影过程中，渐变裁剪通过最大值或最大范数来裁剪参数的渐变。</p><p id="a846" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">Keras支持这两种方式。</p><figure class="ls lt lu lv fq hw"><div class="bz el l di"><div class="mi mj l"/></div></figure><h2 id="9b09" class="ky kb if bd kc kz la lb kg lc ld le kk jn lf lg ko jr lh li ks jv lj lk kw ll dt translated">对爆炸梯度应用类似L2范数正则化</h2><p id="20d1" class="pw-post-body-paragraph jc jd if je b jf lm jh ji jj ln jl jm jn lo jp jq jr lp jt ju jv lq jx jy jz hn dt translated">正则化在优化期间对层参数(权重、偏差)应用惩罚。</p><p id="b6cd" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">L2规范在网络的成本函数中应用“权重衰减”。它的效果由参数λ控制，随着λ变大，许多神经元的权重变得非常小，实际上使它们变得不那么有效，结果使模型更加线性。</p><p id="add1" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">用<strong class="je ig"> Tanh </strong>激活函数举例，当激活值较小时，激活几乎是线性的</p><figure class="ls lt lu lv fq hw fe ff paragraph-image"><div class="fe ff ne"><img src="../Images/0d6036d421d1939e1c13cbc4d86bbf27.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/0*GD0BkPC9XcEzzGBG.gif"/></div><figcaption class="hz ia fg fe ff ib ic bd b be z ek">Tanh</figcaption></figure><p id="aa25" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在Keras中，正则项的使用可以像这样简单，</p><figure class="ls lt lu lv fq hw"><div class="bz el l di"><div class="mi mj l"/></div></figure><h1 id="3710" class="ka kb if bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dt translated">总结和进一步阅读</h1><p id="c6c9" class="pw-post-body-paragraph jc jd if je b jf lm jh ji jj ln jl jm jn lo jp jq jr lp jt ju jv lq jx jy jz hn dt translated">在本文中，我们首先了解什么是消失/爆炸渐变，然后是用Keras API代码片段处理这两个问题的解决方案。</p><h1 id="5cb6" class="ka kb if bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dt translated">进一步阅读</h1><p id="33de" class="pw-post-body-paragraph jc jd if je b jf lm jh ji jj ln jl jm jn lo jp jq jr lp jt ju jv lq jx jy jz hn dt translated">正则子的使用<a class="ae nd" href="https://keras.io/regularizers/" rel="noopener ugc nofollow" target="_blank">https://keras.io/regularizers/</a></p><p id="4d2a" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">https://keras.io/layers/recurrent/的LSTM/GRU<a class="ae nd" href="https://keras.io/layers/recurrent/" rel="noopener ugc nofollow" target="_blank"/></p><p id="c92a" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">其他Keras权重初始化器来看看。<a class="ae nd" href="https://keras.io/initializers/" rel="noopener ugc nofollow" target="_blank">https://keras.io/initializers/</a></p></div><div class="ab cl nf ng hc nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="hn ho hp hq hr"><p id="de3c" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><em class="md">原载于我的网站</em><a class="ae nd" href="https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/" rel="noopener ugc nofollow" target="_blank"><em class="md">www.dlology.com</em></a><em class="md">。</em></p><p id="2807" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><em class="md">在</em> <a class="ae nd" href="https://github.com/Tony607" rel="noopener ugc nofollow" target="_blank"> GitHub </a>、<a class="ae nd" href="https://www.linkedin.com/in/chengweizhang/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae nd" href="https://mp.weixin.qq.com/s?__biz=MzIyNTI3NTI3Mg==&amp;mid=100000016&amp;idx=1&amp;sn=3b7e519ffcabd65efe339a9ee8bb327d&amp;chksm=68037df75f74f4e1aa762fc8d69d9ce725f40189ca4534786492267b448239926c943a8b2970&amp;mpshare=1&amp;scene=1&amp;srcid=1212R4Cm7BhxGSfTAa9V5Ofw#rd" rel="noopener ugc nofollow" target="_blank">微信</a>、<a class="ae nd" href="https://twitter.com/TonyZhang607" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae nd" href="https://www.facebook.com/chengwei.zhang.96" rel="noopener ugc nofollow" target="_blank"> FaceBook </a>上找我。</p></div></div>    
</body>
</html>