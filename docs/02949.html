<html>
<head>
<title>Information Theory of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络信息论</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/information-theory-of-neural-networks-c96a0f0a8d9?source=collection_archive---------15-----------------------#2018-04-03">https://medium.com/hackernoon/information-theory-of-neural-networks-c96a0f0a8d9?source=collection_archive---------15-----------------------#2018-04-03</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="3ff6" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">打开黑盒…稍微</h2></div><blockquote class="jj"><p id="bf4c" class="jk jl hu bd jm jn jo jp jq jr js jt ek translated">"信息:概率的负倒数值."—克劳德·香农</p></blockquote><p id="2970" class="pw-post-body-paragraph ju jv hu jw b jx jy iv jz ka kb iy kc kd ke kf kg kh ki kj kk kl km kn ko jt hn dt translated">这篇博客的目的不是理解神经<a class="ae kp" href="https://hackernoon.com/tagged/network" rel="noopener ugc nofollow" target="_blank">网络</a>背后潜在的数学概念，而是从信息处理的角度可视化神经网络。</p></div><div class="ab cl kq kr hc ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hn ho hp hq hr"><h1 id="f3db" class="kx ky hu bd kz la lb lc ld le lf lg lh ja li jb lj jd lk je ll jg lm jh ln lo dt translated">编码器-解码器</h1><p id="9baa" class="pw-post-body-paragraph ju jv hu jw b jx lp iv jz ka lq iy kc kd lr kf kg kh ls kj kk kl lt kn ko jt hn dt translated">在我们开始之前:</p><blockquote class="lu lv lw"><p id="52ba" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">编码器-解码器不是两个CNN/rnn组合在一起！事实上也不一定是神经网络！</p></blockquote><figure class="mh mi mj mk fq ml fe ff paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="fe ff mg"><img src="../Images/aedc6ad2faaae89cfafeeac68dfeaa0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ICOovMPf3eR-HscOWjXR6Q.jpeg"/></div></div></figure><p id="3bfe" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">本来，信息论的一个概念。<a class="ae kp" href="https://www.cs.toronto.edu/~hinton/science.pdf" rel="noopener ugc nofollow" target="_blank">编码器只是简单地压缩信息，解码器扩展编码后的信息。</a></p><p id="f135" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">在机器<a class="ae kp" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>的情况下，编码和解码都是全丢失过程，即一些信息总是丢失。</p><p id="7694" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">编码器的编码输出被称为<em class="lx">上下文向量</em>，这是解码器的输入。</p><p id="23c2" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">有两种方法可以设置编码器-解码器设置:</p><ol class=""><li id="c9f5" class="ms mt hu jw b jx ly ka lz kd mu kh mv kl mw jt mx my mz na dt translated">编码器反函数中的解码器。这样，解码器试图再现原始信息。这是用来消除数据噪声的。这个设置有一个特殊的名字，叫做自动编码器。</li><li id="a892" class="ms mt hu jw b jx nb ka nc kd nd kh ne kl nf jt mx my mz na dt translated">编码器是压缩算法，解码器是生成算法。这有助于将上下文从一种格式转换为另一种格式。</li></ol><blockquote class="lu lv lw"><p id="6cb4" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">示例应用:</p><p id="0a73" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">自动编码器:把英文文本压缩成矢量的编码器。解码器从向量生成原始英文文本。</p><p id="f79d" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">编码器-解码器:将英文文本压缩成向量的编码器。解码器从向量生成原文的法语翻译。</p><p id="1ea5" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">编码器-解码器:将英文文本压缩成向量的编码器。从文本内容生成图像的解码器。</p></blockquote></div><div class="ab cl kq kr hc ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hn ho hp hq hr"><h1 id="5d53" class="kx ky hu bd kz la lb lc ld le lf lg lh ja li jb lj jd lk je ll jg lm jh ln lo dt translated">信息论</h1><p id="65f8" class="pw-post-body-paragraph ju jv hu jw b jx lp iv jz ka lq iy kc kd lr kf kg kh ls kj kk kl lt kn ko jt hn dt translated">现在，如果我说每个神经网络，本身，是一个编码器-解码器设置；对大多数人来说，这听起来很荒谬。</p><p id="ac5a" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated"><strong class="jw hv">让我们重新想象一下神经网络。</strong></p><p id="8b0a" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">假设输入层是X，它们的真实标签/类(存在于训练集中)是y。现在我们已经知道神经网络找到了X和y之间的潜在函数。</p><p id="67c2" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated"><em class="lx">所以X可以看成是Y的高熵分布.高熵是因为X包含了Y的信息但它也包含了很多其他信息。</em></p><blockquote class="lu lv lw"><p id="cb19" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">示例:</p><p id="6c2f" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">“这小子不错。”包含足够的信息来告诉我们它的“积极”情绪。</p><p id="87d6" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">酪</p><p id="ebd0" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">它还包含以下内容:</p><p id="62f0" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">1.是一个特定的男孩</p><p id="39d0" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">2.只是一个男孩</p><p id="f255" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">3.句子的时态是现在时</p><p id="29e3" class="ju jv lx jw b jx ly iv jz ka lz iy kc ma mb kf kg mc md kj kk me mf kn ko jt hn dt translated">这句话的非熵版本应该是“正的”。是的，这也是输出。我们过一会儿再回到这个话题。</p></blockquote><p id="0f55" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">现在想象每一个隐藏的层作为一个单一的变量H(因此层将被命名为H0，H1 …..H(n-1))</p><p id="ce90" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">现在每一层都变成了变量，神经网络变成了<a class="ae kp" href="https://brilliant.org/wiki/markov-chains/" rel="noopener ugc nofollow" target="_blank">马尔可夫链</a>。因为每个变量只依赖于前一层。</p><p id="b525" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated"><em class="lx">所以本质上每一层都形成了一个信息的党派。</em></p><p id="56b2" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">下面是一个神经网络的可视化马尔可夫链。</p><figure class="mh mi mj mk fq ml fe ff paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="fe ff mg"><img src="../Images/bc161a05f65020a07158bd044bb8a5d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xomkoSgT4-Icsu8NnqU0mg.jpeg"/></div></div></figure><p id="64b3" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">最后一层Y_应该产生最小熵输出(相对于原始标签/类“Y”)。</p><p id="59d5" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated"><em class="lx">这个获取Y_的过程就是在X层的信息流经H层的时候对其进行挤压，只保留与Y最相关的信息，这就是信息瓶颈。</em></p></div><div class="ab cl kq kr hc ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hn ho hp hq hr"><h1 id="5e95" class="kx ky hu bd kz la lb lc ld le lf lg lh ja li jb lj jd lk je ll jg lm jh ln lo dt translated">交互信息</h1><figure class="mh mi mj mk fq ml fe ff paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="fe ff mg"><img src="../Images/afb8c5581d2d125d52efd88a022f1073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GXaA5KmLrl4P-FDSJdtrdA.jpeg"/></div></div></figure><p id="bb79" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">I(X，Y) = H(X) — H(X|Y)</p><p id="f246" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">H -&gt;熵</p><p id="b38d" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">h(X)-&gt; X的熵</p><p id="a5d2" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">H(X|Y) -&gt;给定Y的X的条件熵</p><p id="f921" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">换句话说，H(X|Y)表示如果Y已知，从X中去除了多少不确定性。</p><h2 id="d88e" class="ng ky hu bd kz nh ni nj ld nk nl nm lh kd nn no lj kh np nq ll kl nr ns ln nt dt translated">互信息的性质</h2><ol class=""><li id="2935" class="ms mt hu jw b jx lp ka lq kd nu kh nv kl nw jt mx my mz na dt translated">当你沿着马尔可夫链移动时，互信息只会减少</li><li id="aea5" class="ms mt hu jw b jx nb ka nc kd nd kh ne kl nf jt mx my mz na dt translated">互信息对于重新参数化是不变的，即在图层中混排值不会改变输出</li></ol></div><div class="ab cl kq kr hc ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hn ho hp hq hr"><h1 id="5d26" class="kx ky hu bd kz la lb lc ld le lf lg lh ja li jb lj jd lk je ll jg lm jh ln lo dt translated">重温瓶颈</h1><p id="01a8" class="pw-post-body-paragraph ju jv hu jw b jx lp iv jz ka lq iy kc kd lr kf kg kh ls kj kk kl lt kn ko jt hn dt translated">在神经网络的马尔可夫表示中，每一层都成为信息的一个分区。</p><p id="54f7" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">在信息论中，这些划分被称为相关信息的连续细化。你不必担心细节。</p><p id="5d7c" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">另一种方式是将输入编码和解码成输出。</p><figure class="mh mi mj mk fq ml fe ff paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="fe ff mg"><img src="../Images/aa229b818e2c2c37ee2e6138bcf9f8a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fyOYhqxXJNrzN7SxSuKiJA.jpeg"/></div></div></figure><p id="2e4a" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">因此，对于足够的隐藏层:</p><ol class=""><li id="b349" class="ms mt hu jw b jx ly ka lz kd mu kh mv kl mw jt mx my mz na dt translated">深度神经网络的样本复杂度由最后一个隐层的编码互信息决定</li><li id="d0cd" class="ms mt hu jw b jx nb ka nc kd nd kh ne kl nf jt mx my mz na dt translated">精度由解码的最后一个隐藏层的互信息决定</li></ol><figure class="mh mi mj mk fq ml fe ff paragraph-image"><div class="fe ff nx"><img src="../Images/4e7f9b05f24cb712c92c451be022f540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*dbVNyK7nacJHhvutuzyEdg.jpeg"/></div></figure><p id="5bc8" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated"><em class="lx">样本复杂度是获得一定准确度所需的样本数量和种类。</em></p></div><div class="ab cl kq kr hc ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hn ho hp hq hr"><h1 id="bead" class="kx ky hu bd kz la lb lc ld le lf lg lh ja li jb lj jd lk je ll jg lm jh ln lo dt translated">培训阶段的相互信息</h1><p id="7af7" class="pw-post-body-paragraph ju jv hu jw b jx lp iv jz ka lq iy kc kd lr kf kg kh ls kj kk kl lt kn ko jt hn dt translated">我们计算相互之间的信息</p><ol class=""><li id="a04a" class="ms mt hu jw b jx ly ka lz kd mu kh mv kl mw jt mx my mz na dt translated">层和输入</li><li id="3437" class="ms mt hu jw b jx nb ka nc kd nd kh ne kl nf jt mx my mz na dt translated">层和输出</li></ol><figure class="mh mi mj mk fq ml fe ff paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="fe ff ny"><img src="../Images/ce5b1f89b21c0a791531fc2a56944532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1rm8So6vBDdN5oWFJeDrHA.jpeg"/></div></div><figcaption class="nz oa fg fe ff ob oc bd b be z ek">Initial Conditions</figcaption></figure><p id="4afa" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">最初，权重是随机初始化的。因此，关于正确的输出几乎一无所知。对于连续层，关于输入的互信息减少，并且隐藏层中关于输出的信息也很少。</p><figure class="mh mi mj mk fq ml fe ff paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="fe ff ny"><img src="../Images/a75b84700fa9ced308e9810510f00125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_YbjhJ9h_6sZR1a6Yj5yQ.jpeg"/></div></div><figcaption class="nz oa fg fe ff ob oc bd b be z ek">Compression Phase</figcaption></figure><p id="df05" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">当我们训练神经网络时，图开始向上移动，表示关于输出的信息的获得。</p><p id="e7c3" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">酪</p><p id="386e" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">图也开始向右侧移动，表示后面层中关于输入的信息增加。</p><p id="c74e" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">这是他最长的阶段。这里，图的密度最大，图集中在右上方。这意味着与输出相关的输入信息的压缩。</p><p id="9019" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">这被称为压缩阶段。</p><figure class="mh mi mj mk fq ml fe ff paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="fe ff ny"><img src="../Images/6a52aafad3205b871296960c6ecbb129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JXNRt_1D2zbXK5l6ejVNnQ.jpeg"/></div></div><figcaption class="nz oa fg fe ff ob oc bd b be z ek">Expansion Phase</figcaption></figure><p id="94f1" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">在压缩阶段之后，曲线开始向顶部移动，但也向左侧移动。</p><p id="c3d8" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">这意味着，对于连续的层，存在关于输入的信息损失，并且在最后一层中保留的是关于输出的<em class="lx">最低熵信息</em>。</p></div><div class="ab cl kq kr hc ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hn ho hp hq hr"><h1 id="38cb" class="kx ky hu bd kz la lb lc ld le lf lg lh ja li jb lj jd lk je ll jg lm jh ln lo dt translated">虚拟化</h1><p id="8e54" class="pw-post-body-paragraph ju jv hu jw b jx lp iv jz ka lq iy kc kd lr kf kg kh ls kj kk kl lt kn ko jt hn dt translated">神经网络的马尔可夫链版本突出了一点，学习是从一层到另一层发生的。图层拥有预测输出所需的所有信息(外加一些噪声)。</p><p id="e74f" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">所以我们用每一层来预测输出。这有助于我们窥视所谓黑盒的分层知识。</p><p id="c395" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">这给了我们一个视角，需要多少层才能对输出做出足够准确的预测。如果在较早的层达到饱和，则该层之后的层可以被修剪/丢弃。</p><p id="b3b0" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">这些层通常有数百或数千维。我们的进化不允许我们想象任何超越三维的事物。所以我们使用降维技术。</p><p id="b2ea" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">有各种方法来执行降维。克里斯托夫·奥拉有一个精彩的博客解释这些方法。我不会深入t-SNE的细节，你可以查看这个<a class="ae kp" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank">博客</a>了解详情。</p><p id="9e49" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">为了保持简洁，SNE霸王龙试图降低维数，将高维空间的邻居保留在低维空间。因此，这导致了相当准确的2D和三维绘图。</p><p id="2591" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">以下是具有两层的语言模型的层图。</p><p id="9572" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">关于情节:</p><ol class=""><li id="3303" class="ms mt hu jw b jx ly ka lz kd mu kh mv kl mw jt mx my mz na dt translated">精选16个单词</li><li id="56eb" class="ms mt hu jw b jx nb ka nc kd nd kh ne kl nf jt mx my mz na dt translated">使用最终的语言模型找到上述16个单词的N个同义词(2D的N=200，3D的N=50)</li><li id="c14a" class="ms mt hu jw b jx nb ka nc kd nd kh ne kl nf jt mx my mz na dt translated">在每一层找到每个单词的表示向量</li><li id="3fe3" class="ms mt hu jw b jx nb ka nc kd nd kh ne kl nf jt mx my mz na dt translated">使用t-SNE找出以上所选单词及其同义词的2D和三维简化表示</li><li id="d50e" class="ms mt hu jw b jx nb ka nc kd nd kh ne kl nf jt mx my mz na dt translated">绘制简化的表示</li></ol><div class="mh mi mj mk fq ab cb"><figure class="od ml oe of og oh oi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/6f89d37b91c61ba96f68865d5b8c34f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*TkvVA6cxM6rcl0GHGps4ZQ.png"/></div></figure><figure class="od ml oe of og oh oi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/e88c9c842502d436a51f4879f03ec157.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7HqPTclRdGWCm2t_49WxZQ.png"/></div><figcaption class="nz oa fg fe ff ob oc bd b be z ek oj di ok ol">2D plots of Layer 1 and Layer 2</figcaption></figure></div><div class="ab cb"><figure class="od ml oe of og oh oi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/b96d09f1e76571463c727003a8f0a253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*PbY873h94s7r5qKm7GiSiA.png"/></div></figure><figure class="od ml oe of og oh oi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/820b5ba2db07801f4afed9cb504119de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*9M3jIfQPi2kLhc3IwVAuFg.png"/></div><figcaption class="nz oa fg fe ff ob oc bd b be z ek oj di ok ol">3D plots of Layer 1 and Layer 2</figcaption></figure></div></div><div class="ab cl kq kr hc ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hn ho hp hq hr"><h1 id="ca69" class="kx ky hu bd kz la lb lc ld le lf lg lh ja li jb lj jd lk je ll jg lm jh ln lo dt translated">摘要</h1><ol class=""><li id="2d94" class="ms mt hu jw b jx lp ka lq kd nu kh nv kl nw jt mx my mz na dt translated">几乎每个深度神经网络都像一个编码器-解码器</li><li id="c322" class="ms mt hu jw b jx nb ka nc kd nd kh ne kl nf jt mx my mz na dt translated">大部分训练时间花在压缩阶段</li><li id="93d7" class="ms mt hu jw b jx nb ka nc kd nd kh ne kl nf jt mx my mz na dt translated">层是自下而上学习的</li><li id="970a" class="ms mt hu jw b jx nb ka nc kd nd kh ne kl nf jt mx my mz na dt translated">在压缩阶段之后，神经网络对输入的遗忘越多，它就越准确(消除输入中不相关的部分)。</li></ol><p id="3f75" class="pw-post-body-paragraph ju jv hu jw b jx ly iv jz ka lz iy kc kd mb kf kg kh md kj kk kl mf kn ko jt hn dt translated">我已经把数学排除在这个博客之外了。如果你对信息论、博弈论、学习理论等数学足够熟悉，那么请观看Mastero <strong class="jw hv"> Naftali Tishby </strong>的<a class="ae kp" href="https://www.youtube.com/watch?v=bLqJHjXihK8&amp;t=856s" rel="noopener ugc nofollow" target="_blank"> <strong class="jw hv">视频</strong> </a>。</p></div><div class="ab cl kq kr hc ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hn ho hp hq hr"><blockquote class="jj"><p id="eae5" class="jk jl hu bd jm jn om on oo op oq jt ek translated">谢谢\(-_- )/</p></blockquote><figure class="or os ot ou ov ml"><div class="bz el l di"><div class="ow ox l"/></div></figure></div></div>    
</body>
</html>