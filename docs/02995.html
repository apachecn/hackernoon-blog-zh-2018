<html>
<head>
<title>Another reason why your Docker containers may be slow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">码头集装箱速度慢的另一个原因</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/another-reason-why-your-docker-containers-may-be-slow-d37207dec27f?source=collection_archive---------0-----------------------#2018-04-05">https://medium.com/hackernoon/another-reason-why-your-docker-containers-may-be-slow-d37207dec27f?source=collection_archive---------0-----------------------#2018-04-05</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="01a2" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在我上一篇<a class="ae jp" href="https://hackernoon.com/kubernetes-for-dev-infrastructure-40b9175cb8c0" rel="noopener ugc nofollow" target="_blank">博客文章</a>中，我谈到了Kubernetes以及<a class="ae jp" href="https://thoughtspot.com" rel="noopener ugc nofollow" target="_blank"> ThoughtSpot </a>如何使用它来满足其开发基础设施的需求。今天，我想接着讲一个最近发生的很短但很有趣的调试故事。它重申了集装箱化<strong class="it hv">的事实！= </strong>虚拟化展示了即使所有的cgroup限制都设置为合理的值，容器化的进程如何竞争资源，并且主机上有足够的计算能力。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff jq"><img src="../Images/f0b6ec0f8a859948765e795dfb52cbae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NW7bdB1sCnwzju8qL_Ys4g.png"/></div></div></figure><p id="5e98" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，我们使用我们内部的Kubernetes集群<a class="ae jp" href="https://hackernoon.com/kubernetes-for-dev-infrastructure-40b9175cb8c0" rel="noopener ugc nofollow" target="_blank">来运行一系列CI/CD和开发相关的工作流</a>，一切进展顺利，除了一件事:当发布我们产品的Dockerized副本时，我们看到的性能比我们预期的要差得多。我们的每个容器都有大量的CPU和内存限制，通过Pod配置设置了5个CPU / 30 Gb RAM。在一个虚拟机上，这对于我们微小的(10 Kb)测试数据集上的所有查询来说已经足够了。在Docker &amp; Kubernetes上，在事情变得太慢之前，我们只能在72 CPU / 512 Gb RAM的机器上发布一个产品的3-4个副本。过去在几毫秒内完成的查询现在需要一两秒钟，这导致了我们的CI管道中的各种故障。所以，我们开始调试。</p><p id="0815" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">当然，通常的怀疑是我们在Docker中打包产品时可能犯的配置错误。然而，与虚拟机或裸机安装相比，我们找不到任何可能导致速度变慢的东西。一切看起来都很正常。下一步，我们从一个<a class="ae jp" href="https://github.com/akopytov/sysbench" rel="noopener ugc nofollow" target="_blank"> Sysbench </a>包中运行了各种测试。我们已经测试了CPU、磁盘、RAM性能，看起来与裸机没有任何不同。我们产品中的一些服务保存了所有活动的详细跟踪，稍后可以用于性能分析。通常，如果我们在一种资源(CPU、RAM、磁盘、网络)上处于饥饿状态，一些调用的时间会有明显的偏差，这就是我们如何确定速度慢的原因。然而，在这种情况下，没有任何问题。所有的计时比例都与健康配置中的相同，除了每一次呼叫都比裸机上的呼叫慢得多。没有任何东西给我们指出实际问题的方向，我们准备放弃，但后来我们发现了这个:【https://sysdig.com/blog/container-isolation-gone-wrong/<a class="ae jp" href="https://sysdig.com/blog/container-isolation-gone-wrong/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="0c6a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在本文中，作者分析了一个类似的神秘案例，其中两个假定为轻量级的进程在同一台机器上的Docker中运行时互相残杀，尽管资源限制被设置为非常保守的值。我们的两个主要收获是:</p><ol class=""><li id="858f" class="kc kd hu it b iu iv iy iz jc ke jg kf jk kg jo kh ki kj kk dt translated">他的问题的根本原因最终是在Linux内核中。由于内核dentry缓存的设计，一个进程的行为使得<code class="eh kl km kn ko b">__d_lookup_loop</code>内核调用明显变慢，这直接影响了另一个进程的性能。</li><li id="343c" class="kc kd hu it b iu kp iy kq jc kr jg ks jk kt jo kh ki kj kk dt translated">作者使用了<code class="eh kl km kn ko b">perf</code>来追踪一个内核bug——一个漂亮的调试工具，我们以前从未使用过(真可惜！).</li></ol><blockquote class="ku kv kw"><p id="9d59" class="ir is kx it b iu iv iw ix iy iz ja jb ky jd je jf kz jh ji jj la jl jm jn jo hn dt translated">perf(有时称为perf_events或perf tools，最初是用于Linux的性能计数器，PCL)是Linux中的一个性能分析工具，可从Linux内核版本2.6.31获得。名为perf的用户空间控制实用程序可以从命令行访问，它提供了许多子命令；它能够对整个系统(内核和用户代码)进行统计分析。</p><p id="14a7" class="ir is kx it b iu iv iw ix iy iz ja jb ky jd je jf kz jh ji jj la jl jm jn jo hn dt translated">它支持硬件性能计数器、跟踪点、软件性能计数器(例如hrtimer)和动态探测器(例如kprobes或uprobes)。2012年，两位IBM工程师认为perf(连同OProfile)是Linux上最常用的两种性能计数器分析工具之一</p></blockquote><p id="1610" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">所以，我们想:为什么在我们的案例中不能是类似的东西呢？我们在容器中运行数百个不同的进程，它们都共享同一个内核。肯定有瓶颈！双手握着<code class="eh kl km kn ko b">perf</code>的武器，我们继续调试，这让我们有了一些有趣的发现。</p><p id="cea4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">下面是ThoughtSpot在健康(快速)机器上(左侧)和容器内(右侧)运行的几十秒钟的性能记录。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff lb"><img src="../Images/7fbdd53dfed5348873b0fac36e0eb740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7BFL4myz7eMQ0owKN1gxWw.png"/></div></div></figure><p id="0446" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们可以立即注意到，右侧的前5个调用与内核相关，大部分时间花在内核空间，而在左侧，大部分时间是由我们自己在用户空间运行的进程花费的。更有趣的是，一直占用时间的呼叫是一个<code class="eh kl km kn ko b">posix_fadvise</code>。</p><blockquote class="ku kv kw"><p id="4944" class="ir is kx it b iu iv iw ix iy iz ja jb ky jd je jf kz jh ji jj la jl jm jn jo hn dt translated">程序可以使用posix_fadvise()来宣布将来以特定模式访问<br/>文件数据的意图，从而允许<br/>内核执行适当的优化。</p></blockquote><p id="2748" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">它可以在各种情况下使用，所以它不会直接暗示问题可能来自哪里。然而，在搜索了我们的代码库之后，我只找到了一个地方，它有可能被系统中的每个进程击中:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff lc"><img src="../Images/0f793dcfdadbed558d6735ef031a2b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A-zZ5OT0KrdhphyyRJNREg.png"/></div></div></figure><p id="1c07" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">它在名为<code class="eh kl km kn ko b">glog</code>的第三方日志库中。我们在整个项目中都使用它，这一行在<code class="eh kl km kn ko b">LogFileObject::Write</code>中——可能是整个库中最关键的路径。每个“日志到文件”事件都会调用它，我们产品的多个实例可能会非常频繁地记录日志。快速浏览一下源代码，可以发现通过设置一个<code class="eh kl km kn ko b">--drop_log_memory=false</code>标志可以禁用<code class="eh kl km kn ko b">fadvise</code>部分:</p><pre class="jr js jt ju fq ld ko le lf aw lg dt"><span id="0eca" class="lh li hu ko b fv lj lk l ll lm">if (FLAGS_drop_log_memory) {<br/> if (file_length_ &gt;= logging::kPageSize) {<br/>   // don’t evict the most recent page<br/>   uint32 len = file_length_ &amp; ~(logging::kPageSize — 1);<br/>   posix_fadvise(fileno(file_), 0, len, POSIX_FADV_DONTNEED);<br/> }<br/>}</span></pre><p id="d3d4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们立即尝试了一下，然后…答对了！</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff ln"><img src="../Images/f9b0b8b4ff5ede4e93610aeb6e1ac581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x4qk866K2958SEKebkiV6w.png"/></div></div></figure><p id="c712" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">以前需要几秒钟的时间现在只需要8秒钟。)毫秒。稍微搜索了一下就把我们带到了<a class="ae jp" href="https://issues.apache.org/jira/browse/MESOS-920" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/MESOS-920</a>和<a class="ae jp" href="https://github.com/google/glog/pull/145" rel="noopener ugc nofollow" target="_blank">https://github.com/google/glog/pull/145</a>，这进一步证实了这确实是缓慢的根本原因。最有可能的是，它甚至在虚拟机或裸机上影响了我们，但是因为我们在每个机器/内核上只有每个进程的一个副本，所以它们调用<code class="eh kl km kn ko b">fadvise</code>的速度要慢几倍，因此不会增加大量开销。将日志进程的数量增加3到4倍，同时让它们共享同一个内核——这就是导致<code class="eh kl km kn ko b">fadvise</code>成为真正瓶颈的原因。</p><h2 id="4546" class="lh li hu bd lo lp lq lr ls lt lu lv lw jc lx ly lz jg ma mb mc jk md me mf mg dt translated">结论</h2><p id="2b90" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated">虽然这绝对不是一个新发现，但大多数人仍然没有记住，在容器的情况下，“孤立”的进程不仅竞争<strong class="it hv"> CPU </strong>、<strong class="it hv"> RAM </strong>、<strong class="it hv">磁盘</strong>和<strong class="it hv">网络</strong>，还竞争各种<strong class="it hv">内核</strong> <strong class="it hv">资源</strong>。而且，由于内核极其复杂，低效率可能出现在最意想不到的地方(就像Sysdig的文章中的<code class="eh kl km kn ko b">__d_lookup_loop</code>)。这并不意味着容器比传统虚拟化更差或更好——就其目的而言，它是一个优秀的工具。我们应该始终意识到内核是共享资源，并准备好调试内核空间中的奇怪冲突。此外，这些碰撞是入侵者突破“<em class="kx">轻量级</em>”隔离并在容器之间创建各种隐蔽通道的绝佳机会。最后，<code class="eh kl km kn ko b">perf</code>是一个很棒的工具，它可以向您展示系统中正在发生的一切，并帮助您调试各种性能问题。如果你打算在Docker上运行高负载的应用程序，你绝对应该花时间学习<code class="eh kl km kn ko b">perf</code>。</p><h2 id="530c" class="lh li hu bd lo lp lq lr ls lt lu lv lw jc lx ly lz jg ma mb mc jk md me mf mg dt translated">链接</h2><p id="725a" class="pw-post-body-paragraph ir is hu it b iu mh iw ix iy mi ja jb jc mj je jf jg mk ji jj jk ml jm jn jo hn dt translated"><a class="ae jp" href="https://hackernoon.com/kubernetes-for-dev-infrastructure-40b9175cb8c0" rel="noopener ugc nofollow" target="_blank">开发基础设施的Kubernetes</a></p><p id="8c16" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><a class="ae jp" href="https://hackernoon.com/my-engineering-journey-to-date-8250d69fd079" rel="noopener ugc nofollow" target="_blank">我的工程之旅</a></p><p id="24eb" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><a class="ae jp" href="https://onebar.io" rel="noopener ugc nofollow" target="_blank">你懈怠的知识库</a></p><figure class="jr js jt ju fq jv"><div class="bz el l di"><div class="mm mn l"/></div></figure></div></div>    
</body>
</html>