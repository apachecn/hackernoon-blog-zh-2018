<html>
<head>
<title>Gradient Boosting and XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度增强和XGBoost</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/gradient-boosting-and-xgboost-90862daa6c77?source=collection_archive---------1-----------------------#2018-05-06">https://medium.com/hackernoon/gradient-boosting-and-xgboost-90862daa6c77?source=collection_archive---------1-----------------------#2018-05-06</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/52952b9191159e14fe9a3c680d17ad81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iFN_PWPWs6TQ9JzDp2v9Wg.jpeg"/></div></div></figure><p id="72b5" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">从我们结束的地方开始，让我们继续讨论不同的boosting算法。如果您还没有阅读解释boosting和AdaBoost的前一篇文章，请看看。</p><p id="f87b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">链接:<a class="ae ka" rel="noopener" href="/@grohith327/boosting-algorithms-adaboost-gradient-boosting-and-xgboost-f74991cad38c">https://medium . com/@ grohith 327/boosting-algorithms-AdaBoost-gradient-boosting-and-xgboost-f 74991 CAD 38 c</a></p><h2 id="4bca" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">梯度提升:</h2><p id="3008" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">继续，让我们来看看另一种增强<a class="ae ka" href="https://hackernoon.com/tagged/algorithm" rel="noopener ugc nofollow" target="_blank">算法</a>，梯度增强。梯度提升也是一种提升算法(咄！)，因此它也试图从一群弱学习者中创建一个强学习者。该算法类似于自适应增强(AdaBoost ),但在某些方面有所不同。在这种方法中，我们试图将助推问题视为一个优化问题，即我们采用一个损失函数并试图优化它。这个想法是由<a class="ae ka" href="https://en.wikipedia.org/wiki/Leo_Breiman" rel="noopener ugc nofollow" target="_blank">利奥·布雷曼</a>首先提出的。</p><h2 id="8139" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">如何将梯度推进解释为优化问题？</h2><p id="76e4" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">我们采用一个弱学习者(在前面的例子中是决策树桩),在每一步，我们添加另一个弱学习者来提高性能并建立一个强学习者。这减少了损失函数的损失。我们迭代地添加每个模型并计算损失。该损失表示误差残差(实际值和预测值之间的差异),并且使用该损失值来更新预测以最小化残差。</p><p id="9edb" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">让我们一步一步地分解它。</p><figure class="lc ld le lf fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff lb"><img src="../Images/2738a015ade12a7b3a11282934ff00f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DZE1JsoE-JyxiHZzzF1owg.png"/></div></div></figure><p id="0058" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在第一次迭代中，我们采用一个简单的模型，并试图拟合完整的数据。从上图可以看出，地面真实模型的预测值是不同的。误差残差绘制在图像的右侧。损失函数试图通过添加更多弱学习器来减少这些误差残差。新的弱学习者被添加以集中于现有学习者表现差的领域。</p><figure class="lc ld le lf fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff lg"><img src="../Images/79ca3e403c8cfb8442079afe9d45fb7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*swscIYlcOcdHjjzG0IE7dw.png"/></div></div></figure><p id="9268" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">三次迭代后，您可以观察到模型能够更好地拟合数据。这个过程反复进行，直到残差为零。</p><figure class="lc ld le lf fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff lh"><img src="../Images/2d95a368dc392ffa09bad65b6d3f8a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j5D9dVJzGrAsd24YbTxvYA.png"/></div></div></figure><p id="9701" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">经过20次迭代后，模型几乎完全符合数据，残差下降到零。</p><pre class="lc ld le lf fq li lj lk ll aw lm dt"><span id="2443" class="kb kc hu lj b fv ln lo l lp lq"># Gradient Boosting <br/>from sklearn.ensemble import GradientBoostingClassifier</span><span id="8ca1" class="kb kc hu lj b fv lr lo l lp lq">clf = GradientBoostingClassifier()<br/># n_estimators = 100 (default)<br/># loss function = deviance(default) used in Logistic Regression<br/>clf.fit(x_train,y_train)<br/>clf.predict(x_test)</span></pre><p id="f2cc" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv"> XGBoost(极限梯度提升):</strong></p><p id="05f0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">XGBoost 席卷了数据科学竞赛。XGBoost似乎是用于赢得数据科学竞赛的分类器/预测器集成的一部分。为什么会这样呢？XGBoost为什么这么厉害？</p><p id="2c14" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">XGBoost类似于梯度增强算法，但它有一些锦囊妙计，使它脱颖而出。</p><p id="118f" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">XGBoost的特性有:</p><ul class=""><li id="0265" class="ls lt hu je b jf jg jj jk jn lu jr lv jv lw jz lx ly lz ma dt translated">对树木的巧妙惩罚</li><li id="7a6c" class="ls lt hu je b jf mb jj mc jn md jr me jv mf jz lx ly lz ma dt translated">叶节点的成比例收缩</li><li id="aa5f" class="ls lt hu je b jf mb jj mc jn md jr me jv mf jz lx ly lz ma dt translated"><a class="ae ka" href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" rel="noopener ugc nofollow" target="_blank">牛顿助推</a></li><li id="538a" class="ls lt hu je b jf mb jj mc jn md jr me jv mf jz lx ly lz ma dt translated">额外随机化参数</li></ul><p id="eb93" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在XGBoost中，树可以具有不同数量的终端节点，并且用较少证据计算的树的左权重收缩得更厉害。牛顿推进使用牛顿-拉夫森近似法，该方法提供了比梯度下降更直接的到达最小值的路线。额外的随机化参数可用于降低树之间的相关性，如前一篇文章所述，分类器之间的相关性越小，我们的分类器集成就越好。一般来说，XGBoost比梯度增强快，但是梯度增强具有广泛的应用</p><pre class="lc ld le lf fq li lj lk ll aw lm dt"><span id="d48f" class="kb kc hu lj b fv ln lo l lp lq"># XGBoost <br/>from xgboost import XGBClassifier<br/>clf = XGBClassifier()<br/># n_estimators = 100 (default)<br/># max_depth = 3 (default)<br/>clf.fit(x_train,y_train)<br/>clf.predict(x_test)</span></pre><h2 id="8654" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">结论</h2><p id="3b71" class="pw-post-body-paragraph jc jd hu je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">这些树提升算法已经获得了巨大的欢迎，并且出现在几乎所有kagglers的指令库中。我希望这两篇文章能够让您对这三种算法有一些基本的了解</p><h2 id="448c" class="kb kc hu bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">参考:</h2><div class="mg mh fm fo mi mj"><a rel="noopener follow" target="_blank" href="/mlreview/gradient-boosting-from-scratch-1e317ae4587d"><div class="mk ab ej"><div class="ml ab mm cl cj mn"><h2 class="bd hv fv z el mo eo ep mp er et ht dt translated">从零开始的渐变提升</h2><div class="mq l"><h3 class="bd b fv z el mo eo ep mp er et ek translated">简化复杂的算法</h3></div><div class="mr l"><p class="bd b gc z el mo eo ep mp er et ek translated">medium.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx ja mj"/></div></div></a></div><p id="2540" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><a class="ae ka" href="https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf" rel="noopener ugc nofollow" target="_blank">https://brage . bib sys . no/XM lui/bitstream/handle/11250/2433761/16128 _ full text . pdf</a></p><figure class="lc ld le lf fq iv"><div class="bz el l di"><div class="my mz l"/></div></figure></div></div>    
</body>
</html>