<html>
<head>
<title>Boosting Algorithms: AdaBoost, Gradient Boosting and XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">增强算法:AdaBoost、梯度增强和XGBoost</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/boosting-algorithms-adaboost-gradient-boosting-and-xgboost-f74991cad38c?source=collection_archive---------2-----------------------#2018-05-06">https://medium.com/hackernoon/boosting-algorithms-adaboost-gradient-boosting-and-xgboost-f74991cad38c?source=collection_archive---------2-----------------------#2018-05-06</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/c07b35847d0c6eaba515cc925294f7b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hNnC9r25vEAuRffNy2mjdw.jpeg"/></div></div></figure><p id="5690" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">神经网络和遗传算法是我们模仿自然的天真方法。它们很好地解决了一类问题，但它们确实有各种障碍，如过拟合、局部极小值、消失梯度等等。与其他算法相比，还有一组算法没有得到太多的认可(在我看来),它们是助推算法。</p><h2 id="bc5d" class="ka kb hu bd kc kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku dt translated">什么是助推？</h2><p id="2d05" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated">强化是一种将一组弱学习者转化为强学习者的方法。假设我们有一个二元分类任务。弱学习者在对对象进行分类时具有略小于0.5的错误率，即弱学习者比通过掷硬币来决定略好。强学习者的错误率更接近于0。为了把一个弱学习者转变成强学习者，我们找来一个弱学习者家庭，把他们结合起来，然后投票。这就把这个弱学习者家庭变成了强学习者。</p><p id="6626" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里的想法是，弱学习者的家庭之间应该有一个最小的相关性。</p><figure class="lb lc ld le fq iv fe ff paragraph-image"><div class="fe ff la"><img src="../Images/d2b757e4e98908186708674a2e339c68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*TeaT7IJHA8TDYpGCOFMzGg.png"/></div></figure><p id="ad57" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里设A，B，C是不同的量词。它们的区域A表示分类器A错误分类(出错)的地方，区域B表示分类器B错误分类的地方，区域C表示分类器C错误分类的地方。由于每个分类器的误差之间没有相关性，将它们组合起来并使用民主投票技术来对每个对象进行分类，这个分类器家族永远不会出错。我想这将提供一个基本的理解升压。继续推进<a class="ae lf" href="https://hackernoon.com/tagged/algorithm" rel="noopener ugc nofollow" target="_blank">算法</a>的类型。</p><h2 id="f424" class="ka kb hu bd kc kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku dt translated">升压算法的类型:</h2><p id="c0d0" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated">我想解释不同的升压算法，但不涉及任何数学，因为我觉得这会使事情复杂化，并违背本文的目的，即简单性(希望如此)。不同类型的升压算法包括:</p><ul class=""><li id="0484" class="lg lh hu je b jf jg jj jk jn li jr lj jv lk jz ll lm ln lo dt translated">adaboost算法</li><li id="67d8" class="lg lh hu je b jf lp jj lq jn lr jr ls jv lt jz ll lm ln lo dt translated">梯度推进</li><li id="a035" class="lg lh hu je b jf lp jj lq jn lr jr ls jv lt jz ll lm ln lo dt translated">XGBoost</li></ul><p id="a71e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这三种算法获得了巨大的人气，尤其是XGBoost，它赢得了许多数据科学比赛。</p><h2 id="e28b" class="ka kb hu bd kc kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku dt translated">AdaBoost(自适应升压):</h2><p id="efb0" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated">适应性增强技术是由Yoav Freund和Robert Schapire提出的，他们因其工作获得了哥德尔奖。AdaBoost致力于改善基础学习者失败的地方。基础学习器是一个<a class="ae lf" href="https://hackernoon.com/tagged/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>算法，它是一个弱学习器，在此基础上应用boosting方法将其转变为强学习器。任何接受训练数据权重的机器学习算法都可以用作基础学习器。在下面的例子中，<a class="ae lf" href="https://en.wikipedia.org/wiki/Decision_stump" rel="noopener ugc nofollow" target="_blank">决策树桩</a>被用作基本学习者。</p><p id="44a8" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们获取训练数据，并从该数据中随机采样点，然后应用决策树桩算法对这些点进行分类。在对采样点进行分类之后，我们将决策树的树桩与完整的训练数据进行拟合。这个过程反复发生，直到完整的训练数据无任何误差地拟合，或者直到指定的最大估计数。</p><p id="5ad9" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">从训练数据中取样并应用决策树桩后，模型符合如下所示。</p><figure class="lb lc ld le fq iv fe ff paragraph-image"><div class="fe ff lu"><img src="../Images/7c61cf224d7ae0c0a0c1cf65e974f49a.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*nK0dpeJV5J32TwZhWiWVzA.png"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">Decision Stump 1</figcaption></figure><p id="6d58" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们可以观察到三个阳性样本被错误分类为阴性。因此，我们夸大了这些错误分类样本的权重，以便它们在再次采样时有更好的机会被选中。</p><figure class="lb lc ld le fq iv fe ff paragraph-image"><div class="fe ff lz"><img src="../Images/4b3649b5b185d4fba737dfdff6c4debe.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*4LXC5CzEWyKW7_iSal-JRA.png"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">Decision Stump 2</figcaption></figure><p id="1b13" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">当下次对数据进行采样时，将决策树桩2与决策树桩1相结合，以拟合训练数据。因此，我们这里有一个微型系综，试图完美地拟合数据。这两个决策树桩的微型集合将三个阴性样本误分类为阳性。因此，我们夸大了这些错误分类样本的权重，以便它们在再次采样时有更好的机会被选中。</p><figure class="lb lc ld le fq iv fe ff paragraph-image"><div class="fe ff ma"><img src="../Images/ee23c1d05b578adb2976a1a542e31414.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*8-aYgofMN0hN14t4cFnJPw.png"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">Decision Stump 3</figcaption></figure><p id="6bf6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">选择先前错误分类的样本，并应用判定树桩3来拟合训练数据。我们可以发现两个阳性样本归类为阴性，一个阴性样本归类为阳性。然后，使用三个决策树桩(1、2和3)的集合来拟合完整的训练数据。当使用这三个决策树桩的集合时，该模型完美地拟合了训练数据。</p><figure class="lb lc ld le fq iv fe ff paragraph-image"><div class="fe ff mb"><img src="../Images/f95c8b5ddbcfb90c24a759b18b2b5e19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*_OR57AG1IjL2yqYXMTtOGw.png"/></div><figcaption class="lv lw fg fe ff lx ly bd b be z ek">Ensemble of 3 Decision Stumps</figcaption></figure><p id="8be9" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">AdaBoost的缺点是它很容易被噪声数据击败，算法的效率受离群值的影响很大，因为算法试图完美地拟合每个点。你可能会想，既然算法试图去适应每一个点，它不会过度适应吗？不，不是的。答案已经通过实验结果找到了，有各种猜测，但没有具体的推理。</p><p id="42da" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">代码:</p><pre class="lb lc ld le fq mc md me mf aw mg dt"><span id="75eb" class="ka kb hu md b fv mh mi l mj mk"># AdaBoost Algorithm<br/>from sklearn.ensemble import AdaBoostClassifier</span><span id="b034" class="ka kb hu md b fv ml mi l mj mk">clf = AdaBoostClassifier()<br/># n_estimators = 50 (default value) <br/># base_estimator = DecisionTreeClassifier (default value)<br/>clf.fit(x_train,y_train)<br/>clf.predict(x_test)</span></pre><p id="405f" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">继续解释梯度增强和XGBoost将进一步增加这篇已经很长的文章的长度。因此，我决定把它们作为另一篇文章来写。请点击下面的链接。</p><p id="08ff" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">链接:<a class="ae lf" rel="noopener" href="/@grohith327/gradient-boosting-and-xgboost-90862daa6c77">https://medium . com/@ grohith 327/gradient-boosting-and-xgboost-90862 daa6c 77</a></p><h2 id="bc2b" class="ka kb hu bd kc kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku dt translated">参考资料:</h2><p id="e212" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated"><a class="ae lf" href="https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/</a></p><figure class="lb lc ld le fq iv"><div class="bz el l di"><div class="mm mn l"/></div></figure></div></div>    
</body>
</html>