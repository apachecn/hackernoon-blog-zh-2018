<html>
<head>
<title>I Made a News Scraper with 100 Lines of Python.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我用100行Python做了一个新闻刮刀。</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/i-made-a-news-scrapper-with-100-lines-of-python-2e1de1f28f22?source=collection_archive---------1-----------------------#2018-05-17">https://medium.com/hackernoon/i-made-a-news-scrapper-with-100-lines-of-python-2e1de1f28f22?source=collection_archive---------1-----------------------#2018-05-17</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/df57cc17834e8ab10ecee20ee0d77a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IugT4tq7DPXpdSrlxS1TWA.jpeg"/></div></div></figure><p id="58aa" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">每天我坐地铁去办公室，在那里我的手机根本没有信号。但是medium app不让我离线看故事，所以我决定自己做一个新闻刮刀。</p><p id="4ca6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我不想做一个非常花哨的应用程序，所以我只完成了能满足我需求的最小原型。这个概念非常简单:</p><ol class=""><li id="5c63" class="ka kb hu je b jf jg jj jk jn kc jr kd jv ke jz kf kg kh ki dt translated">找一些新闻来源</li><li id="433c" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">用Python抓取新闻页面</li><li id="6435" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">解析html并用BeautifulSoup提取内容</li><li id="349c" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">把它转换成可读格式，然后给我自己发一封电子邮件</li></ol><p id="a4cd" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在让我解释一下我是如何完成每一部分的。</p></div><div class="ab cl ko kp hc kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hn ho hp hq hr"><h1 id="ecb1" class="kv kw hu bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls dt translated">新闻来源:Reddit</h1><p id="6eda" class="pw-post-body-paragraph jc jd hu je b jf lt jh ji jj lu jl jm jn lv jp jq jr lw jt ju jv lx jx jy jz hn dt translated">人们向Reddit提交链接并投票，因此Reddit是阅读新闻的好消息来源。现在的问题是:如何获取每日热门新闻列表？</p><p id="2b60" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在考虑网络抓取之前，我们应该尝试找出目标网站是否提供任何API，因为使用API是完全合法的，最重要的是，API提供了机器可读的数据，所以我们不需要解析HTML。</p><p id="68f9" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">幸运的是Reddit提供了API。从<a class="ae ly" href="https://www.reddit.com/dev/api/" rel="noopener ugc nofollow" target="_blank">它的API列表</a>我们可以很容易的找到我们需要的:<code class="eh lz ma mb mc b">/top</code>。该端点将返回Reddit或给定subreddit上的头条新闻。这样我们就可以用它从我们感兴趣的子主题中检索头条新闻。</p><p id="3629" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">下一个问题是:我们如何访问这个API？</p><p id="a194" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在阅读了Reddit文档之后，我找到了访问这个端点的最佳方式。</p><p id="9215" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">第一步是在Reddit上创建一个应用程序。登录我的帐户，进入“首选项→应用程序”。底部有一个名为“创建另一个应用程序…”的按钮。单击它并创建一个“脚本”类型的应用程序。注意，我们不需要提供“about url”或“redirect url ”,因为我们的应用程序不打算被公开访问或被任何其他人访问。</p><figure class="me mf mg mh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff md"><img src="../Images/5338c60504a0750d4898026a4e2228c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tcYfDsvYHmjzWggK7Ek27w.png"/></div></div></figure><p id="7af0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">创建应用程序后，我们可以在应用程序信息框中找到应用程序id和密码。</p><figure class="me mf mg mh fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mi"><img src="../Images/7487302d3d3ad79e2704705578fa022f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UnI9qXG6ee4ZEh7fy_Uvwg.png"/></div></div></figure><p id="7ee3" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">下一个问题是如何使用这个凭证。因为我们只需要获取给定subreddit的头条新闻，我们不需要访问任何用户相关信息，所以从技术上讲，我们不需要提供任何用户信息，如用户名或密码。Reddit提供了“<a class="ae ly" href="https://github.com/reddit-archive/reddit/wiki/OAuth2#application-only-oauth" rel="noopener ugc nofollow" target="_blank">仅应用OAuth </a>”，通过它我们的应用可以匿名访问公共信息。尝试以下命令来测试API:</p><pre class="me mf mg mh fq mj mc mk ml aw mm dt"><span id="de4a" class="mn kw hu mc b fv mo mp l mq mr">$ <strong class="mc hv">curl -X POST -H 'User-Agent: myawesomeapp/1.0' -d grant_type=client_credentials --user 'OUR_CLIENT_ID:OUR_CLIENT_SECRET'  </strong><a class="ae ly" href="https://www.reddit.com/api/v1/access_token" rel="noopener ugc nofollow" target="_blank"><strong class="mc hv">https://www.reddit.com/api/v1/access_token</strong></a></span></pre><p id="eefb" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们将得到这样的访问令牌:</p><pre class="me mf mg mh fq mj mc mk ml aw mm dt"><span id="823e" class="mn kw hu mc b fv mo mp l mq mr">{"access_token": "ABCDEFabcdef0123456789", "token_type": "bearer", "expires_in": 3600, "scope": "*"}</span></pre><p id="ffae" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">太棒了。有了访问令牌，我们可以做任何事情。</p><p id="fb09" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">最后一点，我们不想从头开始写API访问代码。我们可以使用python客户端库:</p><div class="ms mt fm fo mu mv"><a href="https://github.com/praw-dev/praw" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab ej"><div class="mx ab my cl cj mz"><h2 class="bd hv fv z el na eo ep nb er et ht dt translated">praw-发展/praw</h2><div class="nc l"><h3 class="bd b fv z el na eo ep nb er et ek translated">praw - PRAW是“Python Reddit API Wrapper”的首字母缩写，是一个Python包，允许简单访问Reddit的…</h3></div><div class="nd l"><p class="bd b gc z el na eo ep nb er et ek translated">github.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ja mv"/></div></div></a></div><p id="85f0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">让我们做一个快速测试。我们从<code class="eh lz ma mb mc b">/r/Python</code>获取前5名提交的内容:</p><pre class="me mf mg mh fq mj mc mk ml aw mm dt"><span id="c1ad" class="mn kw hu mc b fv mo mp l mq mr">&gt;&gt;&gt; <strong class="mc hv">import praw</strong><br/>&gt;&gt;&gt; <strong class="mc hv">import pprint</strong><br/>&gt;&gt;&gt; <strong class="mc hv">reddit = praw.Reddit(client_id='OUR_CLIENT_ID',</strong><br/>...                      <strong class="mc hv">client_secret='OUR_SECRET',</strong><br/>...                      <strong class="mc hv">grant_type='client_credentials',</strong><br/>...                      <strong class="mc hv">user_agent='mytestscript/1.0')</strong><br/>&gt;&gt;&gt; <strong class="mc hv">subs = reddit.subreddit('Python').top(limit=5)</strong><br/>&gt;&gt;&gt; <strong class="mc hv">pprint.pprint([(s.score, s.title) for s in subs])</strong><br/>[(6555, 'Automate the boring stuff with python - tinder'),<br/> (4548,<br/>  'MS is considering official Python integration with Excel, and is asking for '<br/>  'input'),<br/> (4102, 'Python Cheet Sheet for begineers'),<br/> (3285,<br/>  'We started late, but we managed to leave Python footprint on r/place!'),<br/> (2899, "Python Section at Foyle's, London")]</span></pre><p id="ffa4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">太好了！</p><h1 id="20d8" class="kv kw hu bd kx ky nk la lb lc nl le lf lg nm li lj lk nn lm ln lo no lq lr ls dt translated">刮新闻版面</h1><p id="8cf3" class="pw-post-body-paragraph jc jd hu je b jf lt jh ji jj lu jl jm jn lv jp jq jr lw jt ju jv lx jx jy jz hn dt translated">下一步相当简单。从上一步，我们可以得到<code class="eh lz ma mb mc b">Submission</code>对象，它的<code class="eh lz ma mb mc b">url</code>属性正是我们想要的URL。我们还可以通过检查<code class="eh lz ma mb mc b">domain</code>属性来过滤URL，以确保我们只从Reddit以外的网站抓取链接。</p><pre class="me mf mg mh fq mj mc mk ml aw mm dt"><span id="daa3" class="mn kw hu mc b fv mo mp l mq mr">subs = [sub for sub in subs if not sub.domain.startswith('self.')]</span></pre><p id="72e7" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">接下来我们需要做的就是获取URL。这可以通过<code class="eh lz ma mb mc b">requests</code>来实现:</p><pre class="me mf mg mh fq mj mc mk ml aw mm dt"><span id="500b" class="mn kw hu mc b fv mo mp l mq mr">for sub in subs:<br/>  res = requests.get(sub.url)<br/>  if (res.status_code == 200 and 'content-type' in res.headers and<br/>      res.headers.get('content-type').startswith('text/html')):<br/>    html = res.text</span></pre><p id="4444" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里我们跳过了内容类型不是<code class="eh lz ma mb mc b">text/html</code>的提交。这是因为用户可能会提交直接的图像链接，这不是我们的目标。</p><h1 id="35e4" class="kv kw hu bd kx ky nk la lb lc nl le lf lg nm li lj lk nn lm ln lo no lq lr ls dt translated">提取内容</h1><p id="acf8" class="pw-post-body-paragraph jc jd hu je b jf lt jh ji jj lu jl jm jn lv jp jq jr lw jt ju jv lx jx jy jz hn dt translated">下一步是从新闻HTML中提取文本内容。我们的目标是提取新闻标题和新闻内容，忽略诸如页眉、页脚、边栏或任何我们不需要阅读的内容。</p><p id="1409" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这是一项相当艰巨的任务，老实说，没有通用的、完美的解决方案。我们可以使用<code class="eh lz ma mb mc b">BeautifulSoup</code>来提取文本内容，但是它会提取包括页眉和页脚在内的所有内容。</p><p id="d9ed" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我很幸运地发现，现代网站比它们的祖先创建的格式更好。我们不再看到表格布局和<code class="eh lz ma mb mc b">&lt;font&gt;</code> s和<code class="eh lz ma mb mc b">&lt;br&gt;</code> s，取而代之的是，文章页面清楚地标注了标题的<code class="eh lz ma mb mc b">&lt;h1&gt;</code>和段落的<code class="eh lz ma mb mc b">&lt;p&gt;</code>。我发现大多数网站会把标题和主要内容放在同一个容器元素中，例如:</p><pre class="me mf mg mh fq mj mc mk ml aw mm dt"><span id="9acf" class="mn kw hu mc b fv mo mp l mq mr">&lt;header&gt;Site Navigation&lt;/header&gt;<br/>&lt;div id="#main"&gt;<br/>  &lt;section&gt;<br/>    &lt;h1 class="title"&gt;Page Title&lt;/h1&gt;<br/>  &lt;/section&gt;<br/>  &lt;section&gt;<br/>    &lt;p&gt;Paragraph 1&lt;/p&gt;<br/>    &lt;p&gt;Paragraph 2&lt;/p&gt;<br/>  &lt;/section&gt;<br/>&lt;/div&gt;<br/>&lt;aside&gt;Sidebar&lt;/aside&gt;<br/>&lt;footer&gt;Copyright...&lt;/footer&gt;</span></pre><p id="eac9" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里的顶层<code class="eh lz ma mb mc b">&lt;div id="#main"&gt;</code>是标题和内容的公共容器。所以我们可以做一个算法来找到内容:</p><ol class=""><li id="5ec0" class="ka kb hu je b jf jg jj jk jn kc jr kd jv ke jz kf kg kh ki dt translated">找到<code class="eh lz ma mb mc b">&lt;h1&gt;</code>作为标题。通常页面中只有一个<code class="eh lz ma mb mc b">&lt;h1&gt;</code>，出于SEO的目的。</li><li id="9b93" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">找到<code class="eh lz ma mb mc b">&lt;h1&gt;</code>的父节点，测试父节点是否有足够的<code class="eh lz ma mb mc b">&lt;p&gt;</code>元素。</li><li id="9985" class="ka kb hu je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki dt translated">重复步骤2，直到找到具有足够多<code class="eh lz ma mb mc b">&lt;p&gt;</code>元素的父元素或到达<code class="eh lz ma mb mc b">&lt;body&gt;</code>标签。如果找到足够多的<code class="eh lz ma mb mc b">&lt;p&gt;</code>，那么父元素就是主要内容标签。如果在找到足够多的<code class="eh lz ma mb mc b">&lt;p&gt;</code>之前到达了<code class="eh lz ma mb mc b">&lt;body&gt;</code>标签，则该页面不包含可读内容。</li></ol><p id="c81a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这是一个简单的算法，没有考虑任何语义信息，但它实际上非常适合我们的目的。无论如何，如果这个算法失败了，我们可以简单地忽略新闻，少读一条新闻对我来说也无妨……你肯定可以通过解析<code class="eh lz ma mb mc b">&lt;header&gt;</code>、<code class="eh lz ma mb mc b">&lt;footer&gt;</code>或<code class="eh lz ma mb mc b">#main</code>、<code class="eh lz ma mb mc b">.sidebar</code> id / classes来做出更精确的算法。</p><p id="818c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">有了这个算法，我们可以很容易地编写解析器代码:</p><pre class="me mf mg mh fq mj mc mk ml aw mm dt"><span id="7099" class="mn kw hu mc b fv mo mp l mq mr">soup = BeautifulSoup(text, 'html.parser')</span><span id="3fa5" class="mn kw hu mc b fv np mp l mq mr"># find the article title<br/>h1 = soup.body.find('h1')</span><span id="0180" class="mn kw hu mc b fv np mp l mq mr"># find the common parent for &lt;h1&gt; and all &lt;p&gt;s.<br/>root = h1<br/>while root.name != 'body' and len(root.find_all('p')) &lt; 5:<br/>  root = root.parent</span><span id="05f1" class="mn kw hu mc b fv np mp l mq mr">if len(root.find_all('p')) &lt; 5:<br/>  return None</span><span id="9282" class="mn kw hu mc b fv np mp l mq mr"># find all the content elements.<br/>ps = root.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre'])</span></pre><p id="7f22" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里我们使用<code class="eh lz ma mb mc b">len(root.find_all('p')) &lt; 5</code>作为主要内容的条件，因为一条真实的新闻不太可能少于5段。您可以根据需要增加该值。</p><h1 id="5082" class="kv kw hu bd kx ky nk la lb lc nl le lf lg nm li lj lk nn lm ln lo no lq lr ls dt translated">转换成可读格式</h1><p id="e32d" class="pw-post-body-paragraph jc jd hu je b jf lt jh ji jj lu jl jm jn lv jp jq jr lw jt ju jv lx jx jy jz hn dt translated">最后一步是将内容转换成可读格式。在这个例子中，我选择了Markdown，但是您可以制作自己的花式转换器。</p><p id="73d8" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这个例子中，我只取了<code class="eh lz ma mb mc b">&lt;h#&gt;</code>和<code class="eh lz ma mb mc b">&lt;p&gt;</code>，<code class="eh lz ma mb mc b">&lt;pre&gt;</code>，所以一个快捷的函数可以很容易地将它们转换成Markdown。</p><pre class="me mf mg mh fq mj mc mk ml aw mm dt"><span id="1fe1" class="mn kw hu mc b fv mo mp l mq mr">ps = root.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre'])<br/>ps.insert(0, h1)    # add the title<br/>content = [tag2md(p) for p in ps]</span><span id="5db8" class="mn kw hu mc b fv np mp l mq mr"><br/>def tag2md(tag):<br/>  if tag.name == 'p':<br/>    return tag.text<br/>  elif tag.name == 'h1':<br/>    return f'{tag.text}\n{"=" * len(tag.text)}'<br/>  elif tag.name == 'h2':<br/>    return f'{tag.text}\n{"-" * len(tag.text)}'<br/>  elif tag.name in ['h3', 'h4', 'h5', 'h6']:<br/>    return f'{"#" * int(tag.name[1:])} {tag.text}'<br/>  elif tag.name == 'pre':<br/>    return f'```\n{tag.text}\n```'</span></pre><h1 id="446b" class="kv kw hu bd kx ky nk la lb lc nl le lf lg nm li lj lk nn lm ln lo no lq lr ls dt translated">把它们放在一起</h1><p id="1764" class="pw-post-body-paragraph jc jd hu je b jf lt jh ji jj lu jl jm jn lv jp jq jr lw jt ju jv lx jx jy jz hn dt translated">最后，这是完整的代码:</p><figure class="me mf mg mh fq iv"><div class="bz el l di"><div class="nq nr l"/></div></figure><p id="0814" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">正好100行！尝试运行它…</p><pre class="me mf mg mh fq mj mc mk ml aw mm dt"><span id="2a9d" class="mn kw hu mc b fv mo mp l mq mr">Scraping /r/Python...<br/>  - Retrieving https://imgs.xkcd.com/comics/python_environment.png<br/>      x fail or not html<br/>  - Retrieving https://thenextweb.com/dd/2017/04/24/universities-finally-realize-java-bad-introductory-programming-language/#.tnw_PLAz3rbJ<br/>      =&gt; done, title = "Universities finally realize that Java is a bad introductory programming language"<br/>  - Retrieving https://github.com/numpy/numpy/blob/master/doc/neps/dropping-python2.7-proposal.rst<br/>      x fail or not html<br/>  - Retrieving http://www.thedurkweb.com/sms-spoofing-with-python-for-good-and-evil/<br/>      =&gt; done, title = "SMS Spoofing with Python for Good and Evil"<br/>  ...</span></pre><p id="0561" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">和刮新闻:</p><figure class="me mf mg mh fq iv fe ff paragraph-image"><div class="fe ff ns"><img src="../Images/de06c62c3b2179bfbcca68399edbe426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*ILH-YnHjv4IXOT6soTc5WQ.png"/></div></figure><p id="2911" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后，我需要做的最后一件事是将这个脚本放在我的服务器上，设置一个cron作业每天运行一次，并将生成的文件发送到我的电子邮件。</p><p id="6aed" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我没有在细节上花太多精力，所以还有很多可以改进的地方。您可以继续向这个脚本添加更多的特性，比如使结果文件更漂亮和提取图像。</p></div><div class="ab cl ko kp hc kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hn ho hp hq hr"><p id="38fc" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">感谢阅读！希望这个脚本是有用的，至少它可以作为访问Reddit API的一个例子。喜欢这个帖子请推荐。</p></div></div>    
</body>
</html>