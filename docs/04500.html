<html>
<head>
<title>How to Initialize weights in a neural net so it performs well? — Super fast explanation for Xavier’s Random Weight Initialization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何初始化神经网络中的权重，使其表现良好？Xavier随机权重初始化的超快速解释</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/how-to-initialize-weights-in-a-neural-net-so-it-performs-well-3e9302d4490f?source=collection_archive---------4-----------------------#2018-05-29">https://medium.com/hackernoon/how-to-initialize-weights-in-a-neural-net-so-it-performs-well-3e9302d4490f?source=collection_archive---------4-----------------------#2018-05-29</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/8783fd82fbb5c906e6b6fa2ecc1b0bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9uceGl170_Py_fBWuSbgmQ.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek"><a class="ae jg" href="http://www.mdpi.com/1099-4300/19/3/101" rel="noopener ugc nofollow" target="_blank">http://www.mdpi.com/1099-4300/19/3/101</a></figcaption></figure><p id="689a" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们知道，在神经网络中，权重通常是随机初始化的，这种初始化需要相当/大量的重复才能收敛到最小损失并达到理想的权重矩阵。问题是，这种初始化容易出现消失或爆炸梯度问题。</p><p id="12ee" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">减少这个问题的一种方法是仔细选择随机权重初始化。Xavier的随机权重初始化(也称为Xavier算法)将网络的大小(输入和输出神经元的数量)纳入方程，并解决了这些问题。</p><p id="0271" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">Xavier Glorot和Yoshua Bengio是初始化更好的随机权重这一概念的贡献者。这不仅减少了遇到梯度问题的机会，而且有助于更快地收敛到最小误差。</p><h2 id="b669" class="kf kg hu bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz dt translated">使其初始化更好权重的一般方法:</h2><p id="9735" class="pw-post-body-paragraph jh ji hu jj b jk la jm jn jo lb jq jr js lc ju jv jw ld jy jz ka le kc kd ke hn dt translated"><strong class="jj hv"> a)如果你在深网中使用ReLu激活函数(我说的是隐藏层的输出激活函数)那么:</strong></p><ol class=""><li id="3af7" class="lf lg hu jj b jk jl jo jp js lh jw li ka lj ke lk ll lm ln dt translated">从平均值为0且标准偏差为1高斯分布中生成权重的随机样本。</li><li id="5b98" class="lf lg hu jj b jk lo jo lp js lq jw lr ka ls ke lk ll lm ln dt translated">将样本乘以(2/ni)平方根。其中ni是该层的输入单元数。</li></ol><p id="99de" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jj hv"> b)同样，如果你使用双曲正切函数:</strong></p><ol class=""><li id="781c" class="lf lg hu jj b jk jl jo jp js lh jw li ka lj ke lk ll lm ln dt translated">从平均值为0且标准偏差为1高斯分布中生成权重的随机样本。</li><li id="8ec4" class="lf lg hu jj b jk lo jo lp js lq jw lr ka ls ke lk ll lm ln dt translated">将样本乘以(1/ni)平方根。其中ni是该层的输入单元数。</li></ol><h1 id="b208" class="lt kg hu bd kh lu lv lw kl lx ly lz kp ma mb mc ks md me mf kv mg mh mi ky mj dt translated">那么这个Xavier的初始化是什么呢？</h1><p id="41ca" class="pw-post-body-paragraph jh ji hu jj b jk la jm jn jo lb jq jr js lc ju jv jw ld jy jz ka le kc kd ke hn dt translated"><strong class="jj hv">Xavier初始化中唯一的主要区别是输出没有项。我们添加该层的输出单元的数量。</strong></p><p id="e4bc" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jj hv">对于Tanh: </strong></p><ol class=""><li id="1f07" class="lf lg hu jj b jk jl jo jp js lh jw li ka lj ke lk ll lm ln dt translated">从平均值为0且标准偏差为1高斯分布中生成权重的随机样本。</li><li id="cd67" class="lf lg hu jj b jk lo jo lp js lq jw lr ka ls ke lk ll lm ln dt translated">将样本乘以(1/(ni+no))平方根。其中ni是输入单元的数量，no分别是该层的输出单元的数量。</li></ol><blockquote class="mk ml mm"><p id="9e2b" class="jh ji mn jj b jk jl jm jn jo jp jq jr mo jt ju jv mp jx jy jz mq kb kc kd ke hn dt translated"># python代码在这里</p><p id="dae2" class="jh ji mn jj b jk jl jm jn jo jp jq jr mo jt ju jv mp jx jy jz mq kb kc kd ke hn dt translated">将numpy作为np导入</p><p id="c48d" class="jh ji mn jj b jk jl jm jn jo jp jq jr mo jt ju jv mp jx jy jz mq kb kc kd ke hn dt translated"><strong class="jj hv"> W = np.random.rand((x_dim，y_dim))*np.sqrt(1/(ni+no)) </strong></p></blockquote><h1 id="312e" class="lt kg hu bd kh lu lv lw kl lx ly lz kp ma mb mc ks md me mf kv mg mh mi ky mj dt translated">为什么这种初始化有助于防止梯度问题？</h1><p id="0993" class="pw-post-body-paragraph jh ji hu jj b jk la jm jn jo lb jq jr js lc ju jv jw ld jy jz ka le kc kd ke hn dt translated">这种初始化有助于将权重矩阵设置为既不大于1也不小于1。因此，它不会爆炸或消失梯度分别。</p><p id="84c6" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我从Coursera令人敬畏的深度学习专业:deeplearning.ai中学到了这一点</p><blockquote class="mk ml mm"><p id="17b7" class="jh ji mn jj b jk jl jm jn jo jp jq jr mo jt ju jv mp jx jy jz mq kb kc kd ke hn dt translated">改进深度神经网络:超参数调整、正则化和优化；</p><p id="b362" class="jh ji mn jj b jk jl jm jn jo jp jq jr mo jt ju jv mp jx jy jz mq kb kc kd ke hn dt translated"><a class="ae jg" href="https://www.coursera.org/learn/deep-neural-network/" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/learn/deep-neural-network/</a></p></blockquote><p id="df15" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jj hv">以下是原文:</strong></p><blockquote class="mk ml mm"><p id="a819" class="jh ji mn jj b jk jl jm jn jo jp jq jr mo jt ju jv mp jx jy jz mq kb kc kd ke hn dt translated"><strong class="jj hv">了解训练深度前馈神经网络的难度</strong></p><p id="2b20" class="jh ji mn jj b jk jl jm jn jo jp jq jr mo jt ju jv mp jx jy jz mq kb kc kd ke hn dt translated"><strong class="jj hv"> <em class="hu">泽维尔·格罗特，约舒亚·本吉奥</em></strong>；PMLR 9:249–256</p></blockquote><figure class="mr ms mt mu fq iv"><div class="bz el l di"><div class="mv mw l"/></div></figure><p id="8c9d" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">如果你喜欢这篇文章，那就鼓掌吧！:)也许一个跟随？</p><p id="6c5a" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在社交网站上与我联系:</p><div class="mx my fm fo mz na"><a href="https://www.linkedin.com/in/rakshith-vasudev/" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab ej"><div class="nc ab nd cl cj ne"><h2 class="bd hv fv z el nf eo ep ng er et ht dt translated">Rakshith Vasudev | LinkedIn</h2><div class="nh l"><h3 class="bd b fv z el nf eo ep ng er et ek translated">查看拉克什特·瓦苏德夫在全球最大的职业社区LinkedIn上的个人资料。拉克什特教育上市…</h3></div><div class="ni l"><p class="bd b gc z el nf eo ep ng er et ek translated">www.linkedin.com</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no ja na"/></div></div></a></div><div class="mx my fm fo mz na"><a href="https://www.facebook.com/imrakshithvasudev/" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab ej"><div class="nc ab nd cl cj ne"><h2 class="bd hv fv z el nf eo ep ng er et ht dt translated">拉克什特·瓦苏德夫</h2><div class="nh l"><h3 class="bd b fv z el nf eo ep ng er et ek translated">拉克什·瓦苏德夫。和我一起学习人工智能，让这个世界变得更美好。张量流…</h3></div><div class="ni l"><p class="bd b gc z el nf eo ep ng er et ek translated">www.facebook.com</p></div></div><div class="nj l"><div class="np l nl nm nn nj no ja na"/></div></div></a></div><div class="mx my fm fo mz na"><a href="https://www.youtube.com/c/rakshithvasudev" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab ej"><div class="nc ab nd cl cj ne"><h2 class="bd hv fv z el nf eo ep ng er et ht dt translated">拉克什特·瓦苏德夫</h2><div class="nh l"><h3 class="bd b fv z el nf eo ep ng er et ek translated">Datascience入门，最佳编程实践。主题包括机器学习和其他。</h3></div><div class="ni l"><p class="bd b gc z el nf eo ep ng er et ek translated">www.youtube.com</p></div></div><div class="nj l"><div class="nq l nl nm nn nj no ja na"/></div></div></a></div></div></div>    
</body>
</html>