<html>
<head>
<title>Machine Un-Learning: Why Forgetting Might Be the Key to AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器不学习:为什么遗忘可能是人工智能的关键</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/machine-un-learning-why-forgetting-might-be-the-key-to-ai-406445177a80?source=collection_archive---------2-----------------------#2018-05-31">https://medium.com/hackernoon/machine-un-learning-why-forgetting-might-be-the-key-to-ai-406445177a80?source=collection_archive---------2-----------------------#2018-05-31</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div class="fe ff ir"><img src="../Images/3e4253652f4e0549de71d610f61dad7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*VZeOKo_-ds4DUQqvKEmChQ.png"/></div></figure><p id="ed04" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">让我们面对现实吧——忘记事情真糟糕。令人沮丧的是，你不记得把钥匙放在哪里了，或者因为想不起刚刚在杂货店遇到的那个同事的名字而结结巴巴地说不出话来。然而，健忘是人类状况的核心，事实上，我们很幸运能够做到这一点。</p><p id="b347" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">对人类来说，遗忘不仅仅是记忆的失败；这是一个积极的过程，有助于大脑接受新信息并更有效地做出决定。</p><p id="2306" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">现在，数据科学家正在应用神经科学原理来改善机器学习，他们相信人类大脑可能掌握着开启图灵完全人工智能的钥匙。</p></div><div class="ab cl jw jx hc jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hn ho hp hq hr"><p id="f2b2" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">根据最近发表在<a class="ae kd" href="https://www.cell.com/neuron/fulltext/S0896-6273(17)30365-3" rel="noopener ugc nofollow" target="_blank"> Neuron </a>上的一篇论文，我们的大脑应该起到信息过滤器的作用。放入一大堆杂乱的数据，过滤有用的部分，然后清除任何不相关的细节，以便讲述一个故事或做出一个决定。不使用的部分会被删除，以便为新数据腾出空间，就像在电脑上运行磁盘清理一样。用神经生物学的术语来说，当神经元之间的突触连接随着时间的推移减弱或消除时，遗忘就会发生，随着新神经元的发育，它们会重新连接海马体的电路，覆盖现有的记忆(<a class="ae kd" href="https://newatlas.com/memory-forgetting-important-remembering/50154/" rel="noopener ugc nofollow" target="_blank">新图谱</a>)。</p><p id="3813" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">对人类来说，遗忘有两个好处:</p><ol class=""><li id="12b1" class="ke kf hu ja b jb jc jf jg jj kg jn kh jr ki jv kj kk kl km dt translated">它通过减少过时信息对我们决策的影响来增强灵活性</li><li id="14fa" class="ke kf hu ja b jb kn jf ko jj kp jn kq jr kr jv kj kk kl km dt translated">它防止过度适应特定的过去事件，促进一般化(<a class="ae kd" href="https://www.cell.com/neuron/fulltext/S0896-6273(17)30365-3" rel="noopener ugc nofollow" target="_blank">神经元</a>)</li></ol><p id="f539" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">为了<a class="ae kd" href="https://hackernoon.com/damn-girl-youve-got-a-high-aq-ba71a5c9c7f6" rel="noopener ugc nofollow" target="_blank">有效地适应</a>，人类需要能够<strong class="ja hv">战略性地忘记</strong>。</p><h2 id="2a72" class="ks kt hu bd ku kv kw kx ky kz la lb lc jj ld le lf jn lg lh li jr lj lk ll lm dt translated"><strong class="ak">但是电脑呢？</strong></h2><p id="aa99" class="pw-post-body-paragraph iy iz hu ja b jb ln jd je jf lo jh ji jj lp jl jm jn lq jp jq jr lr jt ju jv hn dt translated">这是人工智能面临的一大挑战——计算机的遗忘方式与人类不同。深度神经网络是一系列机器学习任务中最成功的技术，但它们不会像我们一样忘记。</p><p id="053b" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">让我们举一个简单的例子——如果你教一个说英语的孩子学习西班牙语，这个孩子会利用学习英语的相关线索将英语应用到西班牙语中——可能是名词、动词时态、造句——同时忘记不相关的部分——想想口音、喃喃自语、语调。孩子可以在策略性遗忘的同时逐渐学习和构建。</p><p id="2268" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">相反，如果一个神经网络被训练来学习英语，参数被调整来求解英语。如果那时，你想教它西班牙语，西班牙语的新适应将<em class="ls">覆盖</em>神经网络先前为英语获得的知识，有效地删除一切并重新开始。这被称为'<strong class="ja hv">灾难性遗忘</strong>'，而且“这是神经网络的根本局限之一”(<a class="ae kd" href="https://deepmind.com/blog/enabling-continual-learning-in-neural-networks/" rel="noopener ugc nofollow" target="_blank">深度记忆</a>)。</p><p id="fa93" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">虽然这仍然是一个新的领域，但科学家们最近在探索如何克服这一限制的一些潜在理论方面取得了长足的进步。</p><h1 id="ce88" class="lt kt hu bd ku lu lv lw ky lx ly lz lc ma mb mc lf md me mf li mg mh mi ll mj dt translated">教人工智能战略性遗忘:三种方法</h1><h2 id="aa10" class="ks kt hu bd ku kv kw kx ky kz la lb lc jj ld le lf jn lg lh li jr lj lk ll lm dt translated">#1.长短期记忆网络(LSTM)</h2><p id="ab34" class="pw-post-body-paragraph iy iz hu ja b jb ln jd je jf lo jh ji jj lp jl jm jn lq jp jq jr lr jt ju jv hn dt translated">LSTMs是一种类型的<strong class="ja hv">递归神经网络</strong>，它使用特定的学习机制来决定在任何时候记住哪些信息，更新哪些信息，以及关注哪些信息”(<a class="ae kd" href="http://blog.echen.me/2017/05/30/exploring-lstms/" rel="noopener ugc nofollow" target="_blank"> Edwin Chen </a>)。</p><p id="27fb" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">用一个电影类比来解释LSTMs的工作原理是最简单的:想象一下，一台计算机正试图通过分析以前的场景来预测电影中接下来会发生什么。在一个场景中，一个女人拿着一把刀——电脑会猜她是厨师还是杀人犯？在另一张照片中，一个女人和一个男人正在金色拱门下吃寿司——他们是在日本还是在麦当劳？也许真的是圣路易斯？</p><p id="fa37" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">很难预测。</p><p id="52f7" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">LSTMs通过帮助神经网络1)忘记/记住，2)保存和3)集中来帮助这个过程:</p><ol class=""><li id="a2c9" class="ke kf hu ja b jb jc jf jg jj kg jn kh jr ki jv kj kk kl km dt translated">忘记/记住:“例如，如果一个场景结束，模型应该忘记当前场景位置、一天中的时间，并重置任何特定于场景的信息；然而，如果一个角色在场景中死亡，它应该继续记住他不再活着。因此，我们希望模型学习一种独立的<em class="ls">遗忘/记忆</em>机制:当新的输入进来时，它需要知道哪些信念应该保留或抛弃。”(埃德温·陈)</li><li id="2a8b" class="ke kf hu ja b jb kn jf ko jj kp jn kq jr kr jv kj kk kl km dt translated">保存:当模型看到一个新的图像时，它需要了解关于该图像的任何信息是否值得使用和保存。如果在某个场景中，这个女人走过一个广告牌——记住这个广告牌很重要吗？或者它只是一个噪音？</li><li id="800d" class="ke kf hu ja b jb kn jf ko jj kp jn kq jr kr jv kj kk kl km dt translated">焦点:我们需要记住电影中的女人是一位母亲，因为我们稍后会看到她的孩子，但在她不在的场景中这可能并不重要，所以我们不需要在那个场景中关注它。同样，并非存储在神经网络长期记忆中的所有内容都是直接相关的，因此LSTM有助于确定在任何给定时间关注哪些部分，同时保持所有内容安全存储以备后用。</li></ol><h2 id="90dc" class="ks kt hu bd ku kv kw kx ky kz la lb lc jj ld le lf jn lg lh li jr lj lk ll lm dt translated">#2.弹性重量合并(EWC)</h2><p id="1b3d" class="pw-post-body-paragraph iy iz hu ja b jb ln jd je jf lo jh ji jj lp jl jm jn lq jp jq jr lr jt ju jv hn dt translated">EWC是2017年3月由谷歌<a class="ae kd" href="https://deepmind.com/" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>的研究人员创建的一种算法，它模拟了一种叫做<strong class="ja hv">突触巩固</strong>的神经科学过程。在突触巩固过程中，我们的大脑评估一项任务，计算用于执行该任务的许多神经元的重要性，权衡一些对正确执行该任务更关键的神经元。这些关键神经元被编码为重要的，不太可能在后续任务中被覆盖。同样，在神经网络中，多个连接(如神经元)用于执行一项任务。EWC将一些连接编码为关键连接，从而保护它们不被覆盖/遗忘。</p><p id="b460" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">在下图中，你可以看到当研究人员将EWC应用于雅达利的一款游戏时发生了什么——蓝线是标准的深度学习过程，红色和棕色线是EWC辅助的:</p><figure class="ml mm mn mo fq iv fe ff paragraph-image"><div class="fe ff mk"><img src="../Images/24e676dd2de13d67b60fd0aa938bf36a.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*IUUC9xXkoL29h1fh.png"/></div><figcaption class="mp mq fg fe ff mr ms bd b be z ek">blue line = standard deep learning, red &amp; brown lines = improvements with the help of EWC</figcaption></figure><h2 id="f746" class="ks kt hu bd ku kv kw kx ky kz la lb lc jj ld le lf jn lg lh li jr lj lk ll lm dt translated">#3.瓶颈理论</h2><p id="2158" class="pw-post-body-paragraph iy iz hu ja b jb ln jd je jf lo jh ji jj lp jl jm jn lq jp jq jr lr jt ju jv hn dt translated">2017年秋天，人工智能社区正在热议<a class="ae kd" href="http://www.cs.huji.ac.il/~tishby/" rel="noopener ugc nofollow" target="_blank">纳夫塔利·提什比</a>的一场演讲，他是来自耶路撒冷希伯来大学的计算机科学家和神经科学家，也是他所谓的瓶颈理论的证据。“这个想法是，一个网络从噪音输入数据中去除无关的细节，就像通过一个瓶颈挤压信息一样，只保留与一般概念最相关的特征”(<a class="ae kd" href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/" rel="noopener ugc nofollow" target="_blank"> Quanta </a>)。</p><p id="d00e" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">正如Tishby解释的那样，神经网络在学习时会经历两个阶段— <strong class="ja hv">拟合和压缩</strong>。在拟合期间，网络标记其训练数据，在压缩期间，一个更长的过程，它“丢弃关于数据的信息，只跟踪最强的特征”(<a class="ae kd" href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/" rel="noopener ugc nofollow" target="_blank">Qanta</a>)——那些将最有助于帮助它概括的特征。以这种方式，压缩是一种战略性遗忘的方式，操纵这个瓶颈可能是人工智能研究人员用来在未来构建更强神经网络的新目标和架构的工具。</p><blockquote class="mt"><p id="0515" class="mu mv hu bd mw mx my mz na nb nc jv ek translated">正如Tishby所说，“学习最重要的部分其实是遗忘。”</p></blockquote><h2 id="5e43" class="ks kt hu bd ku kv nd kx ky kz ne lb lc jj nf le lf jn ng lh li jr nh lk ll lm dt translated">我们的大脑和独特的人类过程，如遗忘，可能掌握着创造强大人工智能的地图，但科学家们仍在集体研究如何阅读这些指示。</h2></div></div>    
</body>
</html>