<html>
<head>
<title>DeepMind’s Amazing Mix &amp; Match RL Technique</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepMind惊人的混搭RL技术</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/deepminds-amazing-mix-match-rl-techique-a6f8ce6ac0b4?source=collection_archive---------6-----------------------#2018-06-10">https://medium.com/hackernoon/deepminds-amazing-mix-match-rl-techique-a6f8ce6ac0b4?source=collection_archive---------6-----------------------#2018-06-10</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="3598" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">#1研究论文解释</h2></div><h1 id="eb5c" class="jj jk hu bd jl jm jn jo jp jq jr js jt ja ju jb jv jd jw je jx jg jy jh jz ka dt translated">混合搭配——用于强化学习的代理课程[ <a class="ae kb" href="https://arxiv.org/pdf/1806.01780" rel="noopener ugc nofollow" target="_blank"> arxiv </a> ]</h1><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="fe ff kc"><img src="../Images/da1b72b5262a73b9354b52cdb3db7895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SkHgi4zw9rCB6PvkApJcrg.png"/></div></div></figure><p id="3ea4" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">如今使用的<a class="ae kb" href="https://hackernoon.com/tagged/reinforcement" rel="noopener ugc nofollow" target="_blank">强化</a>学习技术非常快速，并通过基于梯度的策略优化为不太复杂的环境提供即时结果。</p><blockquote class="lk"><p id="70b3" class="ll lm hu bd ln lo lp lq lr ls lt lj ek translated">基于梯度的政策是竞争性的，而不是合作性的。</p></blockquote><p id="604d" class="pw-post-body-paragraph ko kp hu kq b kr lu iv kt ku lv iy kw kx lw kz la lb lx ld le lf ly lh li lj hn dt translated">那么，如果我们需要在更复杂的环境中获得持久的结果，让代理执行复杂的任务，那该怎么办呢？</p><p id="0d0d" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">在许多世界环境中，我们没有能力改变环境，在现实世界任务中执行强化<a class="ae kb" href="https://hackernoon.com/tagged/learning" rel="noopener ugc nofollow" target="_blank">学习</a>真的很耗时。因此，为了克服这个问题，本文试图借助<strong class="kq hv">课程学习</strong>和<strong class="kq hv">基于人口的培训</strong>来解决这个问题。</p></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><h1 id="1a10" class="jj jk hu bd jl jm mg jo jp jq mh js jt ja mi jb jv jd mj je jx jg mk jh jz ka dt translated">开始之前</h1><h2 id="b883" class="ml jk hu bd jl mm mn mo jp mp mq mr jt kx ms mt jv lb mu mv jx lf mw mx jz my dt translated">课程学习<a class="ae kb" href="https://www.ijcai.org/proceedings/2017/0757.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></h2><blockquote class="lk"><p id="d120" class="ll lm hu bd ln lo lp lq lr ls lt lj ek translated">使用迁移学习为代理设计一系列任务进行训练，以提高最终表现或学习速度。</p></blockquote><h2 id="bd8d" class="ml jk hu bd jl mm mz mo jp mp na mr jt kx nb mt jv lb nc mv jx lf nd mx jz my dt translated"><strong class="ak">基于群体的神经网络训练[ </strong> <a class="ae kb" href="https://deepmind.com/blog/population-based-training-neural-networks/" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> DeepMind博客</strong></a><strong class="ak">】</strong></h2><blockquote class="lk"><p id="d921" class="ll lm hu bd ln lo lp lq lr ls lt lj ek translated">PBT能够在训练时修改网络的超参数。</p></blockquote></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><h1 id="54a0" class="jj jk hu bd jl jm mg jo jp jq mh js jt ja mi jb jv jd mj je jx jg mk jh jz ka dt translated">主要思想</h1><p id="13e6" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">《混搭》背后的主要理念是</p><blockquote class="lk"><p id="7997" class="ll lm hu bd ln lo lp lq lr ls lt lj ek translated">生成RL代理的多个变体，其中我们没有能力修改根据训练复杂性安排的任务，而是仅使用一个混合匹配代理，该混合匹配代理是通过利用不同代理而构建的，不同代理在其策略生成过程中在结构上不同。</p></blockquote><figure class="nk nl nm nn no kh fe ff paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="fe ff nj"><img src="../Images/bfa9ff92c5379d2d5f81f754ecd4d2a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MCRo5w0W73MJYoadnc72SA.jpeg"/></div></div></figure></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><h1 id="4d07" class="jj jk hu bd jl jm mg jo jp jq mh js jt ja mi jb jv jd mj je jx jg mk jh jz ka dt translated">混搭框架</h1><p id="983d" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">在学习复杂性中排列的多个代理(如上所示)被视为一个混合匹配代理，使用混合策略。</p><p id="1de9" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated"><strong class="kq hv">知识转移</strong>(即提炼)是以这样一种方式完成的，即在早期将复杂的代理与较简单的代理相匹配。</p><p id="63a2" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated"><strong class="kq hv">混合系数</strong>被控制，使得最终复杂的目标代理被用于产生经验。</p></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><h1 id="858e" class="jj jk hu bd jl jm mg jo jp jq mh js jt ja mi jb jv jd mj je jx jg mk jh jz ka dt translated">方法详细信息</h1><p id="68ef" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">让我们假设给定一系列可训练代理1 ( <em class="np">和相应的策略π 1，…，π K，每一个都用一些θ i ⊂ θ参数化，它们可以共享一些参数</em></p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff nq"><img src="../Images/329131ec54135e4f0d465550857cc00e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*teLh-aQdkmCzne71LSbhvg.jpeg"/></div></figure><p id="b8ff" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">目标是训练π K，而所有剩余的代理在那里诱导更快/更容易的学习。</p><p id="f43f" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">让我们介绍一下<strong class="kq hv">分类随机变量‘c’</strong>∞Cat(1，…，K|α) <strong class="kq hv">概率质量函数p(c = i) = α i </strong>，它将用于在给定时间选择一个<br/>策略:</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff nr"><img src="../Images/43ce65245ab0b93cfb31088b638706aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*sdqM9l8uGQCw6PN6EUzoNQ.jpeg"/></div></figure><p id="d128" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">Mix &amp; Match的要点是允许课程学习，因此我们需要c的概率质量函数(pmf)随着时间而改变。最初，pmf应该具有α 1 = 1，并且在接近训练结束时α K = 1，从而允许政策课程从简单的π 1到目标π K</p><p id="376a" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">现在，它应该以这样一种方式训练，最大限度地提高长期的表现，并分享知识，不像那些梯度。</p><p id="ae56" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">为了解决这个问题，我们将使用一个类似于<strong class="kq hv">蒸馏的成本(D) </strong>来将策略结合在一起。</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff ns"><img src="../Images/f2578042b8de02cc71301f2ef4df5486.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*8SJWCTQfMZY7CUcbug2Zlw.jpeg"/></div></figure><p id="429a" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">我们考虑的最终优化问题只是原始的<strong class="kq hv"> L^RL </strong> <strong class="kq hv">损失</strong>(如下)，应用于控制策略<strong class="kq hv"> π^mm </strong>和知识转移损失的加权和:</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff nt"><img src="../Images/4075857bcea9658153613c2a45cb4958.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*ETLIY_PuKq20EUz1qdkxSA.jpeg"/></div></figure></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><p id="780e" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">现在，让我们分步骤理解混搭架构→</p><h2 id="1676" class="ml jk hu bd jl mm mn mo jp mp mq mr jt kx ms mt jv lb mu mv jx lf mw mx jz my dt translated">政策混合</h2><p id="63fc" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">为了减少差异，策略混合将通过显式混合来完成。</p><h2 id="aaed" class="ml jk hu bd jl mm mn mo jp mp mq mr jt kx ms mt jv lb mu mv jx lf mw mx jz my dt translated">知识转移</h2><p id="ced3" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">为了简单起见，我们考虑K = 2的情况。考虑确保<strong class="kq hv">最终策略(π2) </strong>匹配<strong class="kq hv">更简单策略(π1) </strong>的问题，同时访问来自<strong class="kq hv">控制策略</strong> (π <strong class="kq hv"> mm) </strong>的样本。</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff nu"><img src="../Images/c6dc0997b8f5f7f75fad4902b09f5382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*u3gpsmBdyepgO_X3ZU6uFQ.jpeg"/></div><figcaption class="nv nw fg fe ff nx ny bd b be z ek"><strong class="bd nz">Same as previous one</strong></figcaption></figure><p id="3ce4" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">为了简单起见，我们直接在轨迹上定义我们的<strong class="kq hv"> M &amp; M损失</strong>，并且从控制策略中采样<strong class="kq hv">轨迹(s ∈ S) </strong>。引入了<strong class="kq hv">1α</strong>项，以便当我们切换到<strong class="kq hv"> π 2 </strong>时，蒸馏成本消失。</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff oa"><img src="../Images/7d6bcad1b1bd1e1e3e2e7f0eaf678c8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*_xVZrKscr84iMQHTempJww.jpeg"/></div></figure><h2 id="5f9b" class="ml jk hu bd jl mm mn mo jp mp mq mr jt kx ms mt jv lb mu mv jx lf mw mx jz my dt translated">通过训练调整α (alpha)</h2><p id="571b" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated"><strong class="kq hv"> α </strong>是<strong class="kq hv">群体质量函数</strong>方程(f <em class="np">第一方程</em>)中使用的变量。</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff nr"><img src="../Images/43ce65245ab0b93cfb31088b638706aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*sdqM9l8uGQCw6PN6EUzoNQ.jpeg"/></div><figcaption class="nv nw fg fe ff nx ny bd b be z ek">This is the first equation</figcaption></figure><p id="4ee5" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">所提出的方法的一个重要组成部分是如何随时间设置α的值。为简单起见，让我们再次考虑K = 2的情况，其中只需要一个α(因为c现在来自伯努利分布),我们<br/>将其视为时间t的函数。</p><p id="65b3" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated"><strong class="kq hv">在线超参数调整→ </strong>由于α随时间变化，因此不能使用典型的超参数调整技术，因为可能值的空间是时间步长数的指数形式(α = (α (1)，α(T))∈4 tk1，<br/>其中4 k表示k维单纯形)。</p><p id="eefe" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">为了解决这个问题，我们使用<strong class="kq hv">基于人口的培训</strong> (PBT)。</p><h2 id="e84f" class="ml jk hu bd jl mm mn mo jp mp mq mr jt kx ms mt jv lb mu mv jx lf mw mx jz my dt translated">基于人口的培训和M&amp;M</h2><p id="6d7b" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated"><strong class="kq hv">基于群体的训练</strong> (PBT)保持并行训练的<strong class="kq hv">代理</strong>的群体，以便在训练时随着时间优化超参数，并周期性地相互询问以检查它们相对于其他代理做得如何。表现不佳的代理复制较强代理的权重(神经网络参数),并对其超参数进行局部修改。</p><p id="fcc6" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">PBT在单次训练中修改超参数的能力使得发现强大的自适应策略成为可能，例如<em class="np">自动调整学习率退火计划</em>。</p><blockquote class="lk"><p id="9205" class="ll lm hu bd ln lo lp lq lr ls lt lj ek translated">这样，表现不佳的代理被用来<strong class="ak">探索</strong>超参数空间。</p></blockquote><p id="da05" class="pw-post-body-paragraph ko kp hu kq b kr lu iv kt ku lv iy kw kx lw kz la lb lx ld le lf ly lh li lj hn dt translated">所以，我们需要定义<strong class="kq hv">两个函数</strong> →</p><blockquote class="ob oc od"><p id="4540" class="ko kp np kq b kr ks iv kt ku kv iy kw oe ky kz la of lc ld le og lg lh li lj hn dt translated"><strong class="kq hv"> eval → </strong>它测量当前代理的强度</p><p id="915d" class="ko kp np kq b kr ks iv kt ku kv iy kw oe ky kz la of lc ld le og lg lh li lj hn dt translated"><strong class="kq hv">探索</strong> →定义如何激活超参数。</p></blockquote><h2 id="8aa3" class="ml jk hu bd jl mm mn mo jp mp mq mr jt kx ms mt jv lb mu mv jx lf mw mx jz my dt translated">注意:请记住，PBT试剂是混合试剂，它已经是成分试剂的混合物。</h2><p id="0075" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">现在，我们应该使用下面提到的两种方案中的一种，这取决于我们感兴趣的问题的特征。</p><ol class=""><li id="ea2a" class="oh oi hu kq b kr ks ku kv kx oj lb ok lf ol lj om on oo op dt translated">如果模型通过从简单模型切换到更复杂的模型而提高了性能，那么</li></ol><p id="f18a" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated"><strong class="kq hv"> a ) </strong>为<strong class="kq hv"> eval </strong>提供混合策略的性能(即k集以上的奖励)。</p><p id="66a2" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated"><strong class="kq hv"> b ) </strong>对于<strong class="kq hv">探索</strong>函数，对于<strong class="kq hv"> α </strong>，我们随机增加或减去一个固定值(在0和1之间截断)。因此，一旦切换到更复杂的模式有了显著的好处——PBT就会自动完成。</p><p id="f937" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">2.通常，我们希望从一个不受约束的架构切换到某个特定的、受到严重约束的架构(在这种情况下，切换可能不会带来明显的性能优势)。</p><p id="61b5" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">当从组成的单任务策略中训练多任务策略时，我们可以使<strong class="kq hv"> eval </strong>成为一个独立的评估作业，它只查看具有<strong class="kq hv"> α K = 1 </strong>的代理的性能。</p><p id="1043" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">这样，我们可以直接优化目标模型的最终性能，但代价是PBT需要额外的评估。</p></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><h1 id="df26" class="jj jk hu bd jl jm mg jo jp jq mh js jt ja mi jb jv jd mj je jx jg mk jh jz ka dt translated">实验</h1><p id="7091" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">让我们测试并分析一下我们的新M&amp;M方法是否在所有情况下都能正确工作。我们现在将考虑3组强化学习实验，它们可以扩展到大而复杂的动作空间、复杂的代理体系结构和学习多任务策略。</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="fe ff oq"><img src="../Images/d48f5dbd63f09bfe94b43ffd0afac592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wpNCl0GWXkzawZ-XLclscA.jpeg"/></div></div></figure><p id="0f19" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">在整个过程中，我们在0附近初始化α，并通过<br/>时间分析其适应性。</p><blockquote class="ob oc od"><p id="2295" class="ko kp np kq b kr ks iv kt ku kv iy kw oe ky kz la of lc ld le og lg lh li lj hn dt translated"><strong class="kq hv">注意</strong> <strong class="kq hv"> → </strong>即使在实验部分中我们使用K = 2，实际的课程也可能会经历无限多的因素，这是π 1和π 2混合的结果。</p></blockquote><p id="d460" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">除非另有说明，eval函数返回控制策略的最后<strong class="kq hv"> 30集</strong>的平均奖励。</p><p id="5357" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">DeepMind实验室的环境套件为RL代理提供了一系列具有挑战性的3D、基于第一人称视角的任务(请参见附录)。代理感知<strong class="kq hv">基于96 × 72像素的RGB观察，速度为60 fps，可以移动、旋转、跳跃和标记内置机器人</strong>。</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="fe ff or"><img src="../Images/a8ef63b163f8227c30dae0ce190e4985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-QH346AgHuGLJdCvaEvLgg.jpeg"/></div></div></figure><h2 id="dd2d" class="ml jk hu bd jl mm mn mo jp mp mq mr jt kx ms mt jv lb mu mv jx lf mw mx jz my dt translated">1.课程超过使用的行动数量</h2><p id="7035" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">在6维向量中表示动作空间是复杂的，其中两个动作组是非常高分辨率的(<em class="np">旋转和向上/向下看动作</em>)，其余四个是低分辨率的动作(<em class="np">向前移动、向后移动或根本不移动、拍摄或不拍摄等</em>)。</p><p id="27a7" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">这里用9个动作构造<strong class="kq hv"> π 1 </strong>，简单策略(小动作空间)。</p><p id="1c97" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">类似于使用对角高斯分布的连续控制的研究，我们使用分解策略π 2 (a 1，a 2，…，a 6 |s) := j=1 π̂ j (a j |s)，我们称之为大动作空间。</p><p id="3770" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated"><strong class="kq hv">为了能够混合这两种策略，我们将π 1动作映射到π 2动作空间中的相应动作上。</strong></p><p id="d035" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">下图显示了代理网络之间的混合值。</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff os"><img src="../Images/baa15ca02750332598b19e86f54fc63d.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*p23awkQ6HcCwaqJsbJBQrg.jpeg"/></div></figure><p id="97e1" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">本文还讨论了共享头和掩码KL技术，但这两种技术都比M&amp;M技术要陈旧。</p><h2 id="df85" class="ml jk hu bd jl mm mn mo jp mp mq mr jt kx ms mt jv lb mu mv jx lf mw mx jz my dt translated">结果</h2><p id="0cda" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">我们看到，与大的动作空间相比，小的动作空间导致更快的学习，但阻碍了最终的表现。</p><blockquote class="lk"><p id="5e17" class="ll lm hu bd ln lo lp lq lr ls lt lj ek translated">应用于这种设置的Mix &amp; Match得到了两个世界的最佳效果，它学习得很快，并且超越了大动作空间的最终表现。</p></blockquote><figure class="nk nl nm nn no kh fe ff paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="fe ff ot"><img src="../Images/721f5a320b2d487a983a668eb05ba49a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e7Ajy2LdFD_3yT67uamuUg.jpeg"/></div></div></figure><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="fe ff ou"><img src="../Images/7867753aa2b3db30685e12ea46be2f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FeHK4oEnMkVJkRnzwErXBg.jpeg"/></div></div><figcaption class="nv nw fg fe ff nx ny bd b be z ek">FIG: Training in action<br/>spaces experiments.</figcaption></figure><p id="3318" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">当绘制α随时间变化的曲线时(图5左侧),我们看到代理在早期完全切换到大的动作空间，从而表明小的动作空间只对学习的初始阶段有用。通过观察代理人通过训练采取的各种行动，可以进一步证实这一点。</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff ov"><img src="../Images/b41324c8809fa0800345d48f7ee5d4a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*xA5aOIOdkJ5xDty-1G9cug.jpeg"/></div></figure><p id="809d" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">图5(右)显示了行动的边际分布如何随时间演变。我们看到新的动作通过训练被解锁，并且进一步看到最终的分布比初始的分布更熵，这意味着更多的熵导致更多的稳定性。</p><h2 id="22f4" class="ml jk hu bd jl mm mn mo jp mp mq mr jt kx ms mt jv lb mu mv jx lf mw mx jz my dt translated">2.基于Agent架构的课程</h2><p id="117e" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">我们用来自经处理的卷积信号的线性投影来代替LSTM。我们共享卷积模块以及策略/价值函数投影。</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff ow"><img src="../Images/ad19f725c4478a33d140854ff8c16e15.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*eU8EtpsFm98j97Q6988FLg.jpeg"/></div></figure><p id="71ea" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">实验集中于各种<strong class="kq hv">导航任务</strong>。</p><p id="630e" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">一方面，反应性策略(可以仅由前馈策略表示)应该合理快速地学习移动和探索，而另一方面，需要循环网络(具有记忆)来最大化最终性能——通过学习导航新的迷宫布局(探索小的对象位置)或避免(寻找)通过迷宫探索的不成功(成功)路径。</p><h2 id="6d4a" class="ml jk hu bd jl mm mn mo jp mp mq mr jt kx ms mt jv lb mu mv jx lf mw mx jz my dt translated">结果</h2><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="fe ff ox"><img src="../Images/ab4a4eb6eabf4fd2303fb89074b3a027.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1MNj6MA-hPCghkCM8HXFuQ.jpeg"/></div></div><figcaption class="nv nw fg fe ff nx ny bd b be z ek">FIG 6</figcaption></figure><p id="7bf2" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">M&amp;M应用于LSTM和FF的过渡，导致最终性能的显著提高。但没有FF的同类产品快。</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff oy"><img src="../Images/55a305c0d049640d548a86ad9c3341f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*xIEXReFcho8faD2lHRPinA.jpeg"/></div><figcaption class="nv nw fg fe ff nx ny bd b be z ek">FIG 7</figcaption></figure><p id="1742" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">图8导致了两个观察结果</p><ol class=""><li id="c2c9" class="oh oi hu kq b kr ks ku kv kx oj lb ok lf ol lj om on oo op dt translated">由于游戏级别的复杂性，绿色曲线切换到LSTM的时间较晚。</li><li id="c9b6" class="oh oi hu kq b kr oz ku pa kx pb lb pc lf pd lj om on oo op dt translated">如果需要，模型(蓝色曲线)能够切换回混合策略。</li></ol><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div class="fe ff pe"><img src="../Images/988fb893e523f71c0479ea0ce2ac2a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*P_pCwpSHVnbqDyiGL9empA.jpeg"/></div></figure><h2 id="5135" class="ml jk hu bd jl mm mn mo jp mp mq mr jt kx ms mt jv lb mu mv jx lf mw mx jz my dt translated">3.多任务课程</h2><p id="564c" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">作为概念的最后证明，我们考虑学习能够同时解决多个RL问题的单个策略的任务。这类任务的基本方法是在混合环境中训练一个模型，或者等价地在多个并行环境中训练一个共享模型。</p><p id="b6af" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">这种类型的训练有两个缺点:</p><p id="7e5f" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">→严重依赖奖励规模，会偏向<br/>高奖励环境。</p><p id="6408" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">→易于训练的环境为模型提供了大量更新，因此也可能使解决方案偏向自己。</p><p id="3bfe" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">让我们来看看结果:</p><figure class="kd ke kf kg fq kh fe ff paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="fe ff pf"><img src="../Images/7209dde1452bed2eec83262fbf4d296a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MfQzBgXkAVQlUsQAV44xAw.jpeg"/></div></div><figcaption class="nv nw fg fe ff nx ny bd b be z ek">FIG 9</figcaption></figure><p id="1fb1" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">一个是<strong class="kq hv">探索目标地点小</strong>，这有高回报和陡峭的初始学习曲线(大量的回报信号来自收集苹果)。</p><p id="10bd" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">在剩下的两个问题中，训练是困难的，代理与其他机器人以及复杂的机制(拿奖金，标记楼层等)交互。)</p><p id="af74" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">多任务解决方案侧重于解决导航任务，而在更具挑战性的问题上表现较差。</p><p id="8af8" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">为了将M&amp;M应用于这个问题，我们为每个环境构建一个代理<br/>(每个代理充当前面章节中的π 1)，然后构建一个集中的“多任务”代理(前面章节中的π 2)。重要的是，代理共享卷积层，但具有独立的LSTMs。</p><p id="3456" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">训练是以多任务的方式进行的，但是每个环境中的控制策略又是特定任务的π i(专家)和π mt(集中代理)的混合。</p><p id="c6ba" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">由于切换到集中策略不再有利，我们使用π mt的<br/>性能(即中央策略)作为PBT的优化标准(eval ),而不是控制策略。</p><p id="bf67" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">我们独立评估混合物和集中剂的性能。图9示出了所提出的方法的每任务性能。人们可以注意到更加一致的表现—</p><blockquote class="lk"><p id="c478" class="ll lm hu bd ln lo lp lq lr ls lt lj ek translated">M智能体学会在两种具有挑战性的激光标签环境中玩得很好，同时稍微牺牲了在单个导航任务中的性能。</p></blockquote><p id="ac62" class="pw-post-body-paragraph ko kp hu kq b kr lu iv kt ku lv iy kw kx lw kz la lb lx ld le lf ly lh li lj hn dt translated">这一成功的原因之一是，知识转移是在政策空间中进行的，这对于报酬比例是不变的。虽然代理人在转换到只使用<br/>中心策略后仍然可以完全专注于高回报环境，但M &amp; M训练中的这种归纳偏差确保了更高的最低分数。</p></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><h1 id="be5a" class="jj jk hu bd jl jm mg jo jp jq mh js jt ja mi jb jv jd mj je jx jg mk jh jz ka dt translated">结论</h1><p id="3f74" class="pw-post-body-paragraph ko kp hu kq b kr ne iv kt ku nf iy kw kx ng kz la lb nh ld le lf ni lh li lj hn dt translated">随着时间的推移，该混合物的成分权重被调整，使得在训练结束时，我们剩下由最复杂的试剂组成的单一活性成分。还有，</p><blockquote class="lk"><p id="4d62" class="ll lm hu bd ln lo lp lq lr ls lt lj ek translated">在复杂环境中提高和加速性能。</p><p id="88f0" class="ll lm hu bd ln lo lp lq lr ls lt lj ek translated">使用混合策略将代理集合绑定在一起，作为一个单一的复合整体。</p><p id="ec11" class="ll lm hu bd ln lo lp lq lr ls lt lj ek translated">信息可以通过共享的经验或共享的架构元素在组件之间共享，也可以通过类似蒸馏的KL匹配损耗来共享。</p></blockquote></div><div class="ab cl lz ma hc mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hn ho hp hq hr"><p id="2742" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">我将每周发布2个帖子，所以不要错过教程。</p><p id="1884" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">所以，跟着我上<a class="ae kb" rel="noopener" href="/@sagarsharma4244"> <strong class="kq hv">中</strong></a>T5】和<a class="ae kb" href="https://twitter.com/SagarSharma4244" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hv">推特</strong> </a> <strong class="kq hv"> </strong>看看类似的帖子吧。</p><p id="1020" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">如果你有任何意见或问题，请写在评论里。</p><p id="518c" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated"><strong class="kq hv">拍一下(多次)！分享一下！跟我来。</strong></p><p id="1b93" class="pw-post-body-paragraph ko kp hu kq b kr ks iv kt ku kv iy kw kx ky kz la lb lc ld le lf lg lh li lj hn dt translated">乐意帮忙。荣誉……..</p><h1 id="467d" class="jj jk hu bd jl jm jn jo jp jq jr js jt ja ju jb jv jd jw je jx jg jy jh jz ka dt translated">你会喜欢的以前的故事:</h1><div class="pg ph fm fo pi pj"><a href="https://towardsdatascience.com/50-tensorflow-js-api-explained-in-5-minutes-tensorflow-js-cheetsheet-4f8c7f9cc8b2" rel="noopener follow" target="_blank"><div class="pk ab ej"><div class="pl ab pm cl cj pn"><h2 class="bd hv fv z el po eo ep pp er et ht dt translated">50 tensor flow . js API 5分钟讲解| TensorFlow.js Cheetsheet</h2><div class="pq l"><h3 class="bd b fv z el po eo ep pp er et ek translated">TensorFlow API Cheetsheet</h3></div><div class="pr l"><p class="bd b gc z el po eo ep pp er et ek translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px km pj"/></div></div></a></div><div class="pg ph fm fo pi pj"><a href="https://towardsdatascience.com/tensorflow-1-9-has-arrived-1e6e9171ce5e" rel="noopener follow" target="_blank"><div class="pk ab ej"><div class="pl ab pm cl cj pn"><h2 class="bd hv fv z el po eo ep pp er et ht dt translated">TensorFlow 1.9已经到来！</h2><div class="pq l"><h3 class="bd b fv z el po eo ep pp er et ek translated">你需要知道的一切…</h3></div><div class="pr l"><p class="bd b gc z el po eo ep pp er et ek translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="py l pu pv pw ps px km pj"/></div></div></a></div><div class="pg ph fm fo pi pj"><a href="https://towardsdatascience.com/tensorflow-on-mobile-tutorial-1-744703297267" rel="noopener follow" target="_blank"><div class="pk ab ej"><div class="pl ab pm cl cj pn"><h2 class="bd hv fv z el po eo ep pp er et ht dt translated">手机上的TensorFlow:教程</h2><div class="pq l"><h3 class="bd b fv z el po eo ep pp er et ek translated">在Android和iOS上</h3></div><div class="pr l"><p class="bd b gc z el po eo ep pp er et ek translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="pz l pu pv pw ps px km pj"/></div></div></a></div><div class="pg ph fm fo pi pj"><a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" rel="noopener follow" target="_blank"><div class="pk ab ej"><div class="pl ab pm cl cj pn"><h2 class="bd hv fv z el po eo ep pp er et ht dt translated">激活函数:神经网络</h2><div class="pq l"><h3 class="bd b fv z el po eo ep pp er et ek translated">Sigmoid，tanh，Softmax，ReLU，Leaky ReLU解释！！！</h3></div><div class="pr l"><p class="bd b gc z el po eo ep pp er et ek translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="qa l pu pv pw ps px km pj"/></div></div></a></div><figure class="kd ke kf kg fq kh"><div class="bz el l di"><div class="qb qc l"/></div></figure></div></div>    
</body>
</html>