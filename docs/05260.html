<html>
<head>
<title>Understanding architecture of LSTM cell from scratch with code.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用代码从零开始理解LSTM细胞的结构。</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4?source=collection_archive---------0-----------------------#2018-06-22">https://medium.com/hackernoon/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4?source=collection_archive---------0-----------------------#2018-06-22</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff hs"><img src="../Images/adbc8fbde207eceae79f0fd0c2746abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zu0wiHJphMlmpbgBSIZxow.jpeg"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek">source:Google</figcaption></figure><div class=""/><p id="c9fa" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在数据序列很重要的情况下，普通的神经网络表现不佳。例如:语言翻译、情感分析、时间序列等等。为了克服这个缺点，发明了rnn。RNN代表“循环神经网络”。一个RNN细胞不仅考虑它的当前输入，而且考虑它之前的RNN细胞的输出，因为它的当前输出。</p><p id="f52a" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">香草RNN现在的状态的简单形式可以表示为:</p><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff ke"><img src="../Images/0fcac18e55b714cdf609e1ad8774bf8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uubYiUNDmhmR5KOPdJKYtQ.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">Representation of simple RNN cell</strong>,source: stanford</figcaption></figure><p id="cdf2" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">RNNs在顺序数据上表现很好，在顺序很重要的任务上表现也很好。</p><h1 id="6cd2" class="kk kl ij bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh dt translated">但是存在许多<strong class="ak">问题</strong> <strong class="ak">与</strong> <strong class="ak">普通</strong> <strong class="ak"> RNNs </strong></h1><blockquote class="li lj lk"><p id="a5b6" class="jg jh ll ji b jj jk jl jm jn jo jp jq lm js jt ju ln jw jx jy lo ka kb kc kd hn dt translated"><strong class="ji ik">消失渐变问题:</strong></p></blockquote><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff lp"><img src="../Images/597b3ae9f6a53b0aeb8bd3fdff6b5398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pjCSzfYFYdxw8d6e1_L7Cg.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">Vanishing Gradient problem 1.tanh 2.derivative of tanh</strong></figcaption></figure><p id="8c5c" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">双曲正切(tanh)在RNNs中主要用作激活函数，激活函数位于[-1，1]，tanh的导数位于[0，1]。在反向传播期间，由于梯度是通过链规则计算的，它具有将这些小数字<strong class="ji ik">n</strong>(rnn架构中使用的tanh的次数)<strong class="ji ik">乘以</strong>的效果，这将最终梯度挤压到几乎为零，因此从权重中减去梯度不会对它们做出任何改变，从而停止模型的训练。</p><blockquote class="li lj lk"><p id="5c64" class="jg jh ll ji b jj jk jl jm jn jo jp jq lm js jt ju ln jw jx jy lo ka kb kc kd hn dt translated"><strong class="ji ik">爆炸渐变问题:</strong></p></blockquote><p id="a95d" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">与消失梯度问题相反，当遵循链式法则时，我们也在每一步与权重矩阵(转置的W)相乘，并且如果值大于1，则将一个大的数与其自身相乘多次会导致一个非常大的数，从而导致梯度的爆炸。</p><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff lq"><img src="../Images/b732193a1158666ee0a85a74ae27ba01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pd-jyEsefH0ILRbY-_zWJw.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">exploding and vanishing gradients, </strong>source: CS231N stanford</figcaption></figure><blockquote class="li lj lk"><p id="e03f" class="jg jh ll ji b jj jk jl jm jn jo jp jq lm js jt ju ln jw jx jy lo ka kb kc kd hn dt translated"><strong class="ji ik">长期依赖问题</strong></p></blockquote><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff lr"><img src="../Images/ed74111377f75c53e1e9cdc29aab973b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a5EbLhyxbPR78PhiV5Esjg.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">Long-term dependency problem, each node represents an rnn cell.</strong>source:Google</figcaption></figure><p id="0a59" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">rnn在处理顺序数据方面表现良好，但当上下文距离较远时，它们会遇到问题。例:我住在法国，我知道____。答案肯定是这里的“<strong class="ji ik">法语</strong>”，但是如果在“<strong class="ji ik">我住在法国</strong>”&amp;“<strong class="ji ik">我知道____ 【T27”)之间有更多的词。RNNs很难预测“法语”。<strong class="ji ik">这就是长期依赖的问题。因此我们来到了LSTMs。</strong></strong></p><h1 id="ac9d" class="kk kl ij bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh dt translated"><strong class="ak">长短期记忆网络</strong></h1><p id="8eb3" class="pw-post-body-paragraph jg jh ij ji b jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz lw kb kc kd hn dt translated">LSTMs是一种特殊的rnn，具有处理长期依赖关系的能力。LSTMs还提供了消失/爆炸梯度问题解决方案。我们将在本文后面讨论。</p><p id="0f42" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">一个简单的<strong class="ji ik"> LSTM </strong>细胞看起来像这样:</p><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff lx"><img src="../Images/a96f33e54532b2e8f57aba7af7c06725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g4jsLedfzsSFtuqVCfQXsw.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">RNN vs LSTM cell representation, </strong>source: stanford</figcaption></figure><p id="e5a2" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">首先，我们需要初始化权重矩阵和偏差项，如下所示。</p><figure class="kf kg kh ki fq hw"><div class="bz el l di"><div class="ly lz l"/></div></figure><blockquote class="li lj lk"><p id="3353" class="jg jh ll ji b jj jk jl jm jn jo jp jq lm js jt ju ln jw jx jy lo ka kb kc kd hn dt translated"><strong class="ji ik">一些关于LSTM细胞的信息</strong></p></blockquote><p id="09c7" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">一个简单的LSTM单元由4个门组成:</p><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff ma"><img src="../Images/64738cf46d924522d208315c8997bcba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_YFtlUJG69dm6QLnFhYBoQ.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">3 LSTM cells connected to each other.</strong> source:Google</figcaption></figure><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mb"><img src="../Images/e941d27cfae5eb741ed16ce9b4fa42cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z4qT1SIp79JZ21x86w_4gA.jpeg"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">LSTM cell visual representation,</strong> source: Google</figcaption></figure><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div class="fe ff mc"><img src="../Images/8b4e8ce726385bdae20dc807d77fc5a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*3T6bGnxE8iCjmTep_nsbFQ.png"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">handy information about gates,</strong> source: Stanford CS231N</figcaption></figure><blockquote class="li lj lk"><p id="56e5" class="jg jh ll ji b jj jk jl jm jn jo jp jq lm js jt ju ln jw jx jy lo ka kb kc kd hn dt translated"><strong class="ji ik">我们来讨论一下盖茨:</strong></p></blockquote><p id="4170" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><strong class="ji ik"/>【遗忘门】 :在得到<strong class="ji ik">前一状态</strong>、<strong class="ji ik"> h(t-1) </strong>的输出后，遗忘门帮助我们决定什么必须从h(t-1)状态中删除，从而只保留相关的内容。它被一个sigmoid函数包围，该函数有助于粉碎[0，1]之间的输入。它表示为:</p><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mb"><img src="../Images/678bdc5a0b34ad583fdaf47655604153.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jk-RZB7DkOFjYyenApkmBA.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">Forget Gate</strong>, src: Google</figcaption></figure><p id="6d2e" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">我们将遗忘门与前一个单元状态相乘，以遗忘前一个状态中不再需要的不必要的内容，如下所示:</p><figure class="kf kg kh ki fq hw"><div class="bz el l di"><div class="ly lz l"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="ak">code for Forget gate.</strong></figcaption></figure><p id="451d" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><strong class="ji ik">·输入门:</strong>在输入门中，我们决定将当前输入的新内容添加到我们当前的单元状态中，并根据我们希望添加的数量进行缩放。</p><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff md"><img src="../Images/dd2aac6d1f75212aa47f884c411b3a9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Jcedb5RcavnLYPR6sScUg.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">Input Gate+Gate_gate</strong>,photo credits: Christopher Olah</figcaption></figure><p id="24c1" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在上面的照片中，<strong class="ji ik"> sigmoid层决定更新哪些值</strong>而<strong class="ji ik"> tanh层为要添加到当前单元格状态</strong>的新候选项创建一个向量。代码如下所示。</p><figure class="kf kg kh ki fq hw"><div class="bz el l di"><div class="ly lz l"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek">Inputgate, Gate_gate</figcaption></figure><p id="9afa" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">为了计算当前单元状态，我们将((input_gate*gate_gate)和forget gate)的输出相加，如下所示。</p><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div class="fe ff me"><img src="../Images/a411c12c8b30c4e1ecf53a244e8922d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*_53wl2n_QrAowGuxpMukKg.png"/></div></figure><figure class="kf kg kh ki fq hw"><div class="bz el l di"><div class="ly lz l"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek">Code for input_gate + gate_gate</figcaption></figure><p id="d05a" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><strong class="ji ik">输出门:</strong>最后我们将决定从我们的细胞状态输出什么，这将由我们的sigmoid函数来完成。</p><p id="ee59" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">我们将输入与tanh相乘以压缩(-1，1)之间的值，然后将其与sigmoid函数的输出相乘，以便我们只输出我们想要的。</p><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mf"><img src="../Images/2c03b7f3b61ca0a7eae340cb55fecc10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*msWGKNRXxcsC90Xz-4B8XQ.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">output Gate, </strong>source:Google</figcaption></figure><figure class="kf kg kh ki fq hw"><div class="bz el l di"><div class="ly lz l"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek">Output gate and present ‘h’ state</figcaption></figure><p id="46a8" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">对我们所做的事情的总体看法。</p><figure class="kf kg kh ki fq hw"><div class="bz el l di"><div class="ly lz l"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek">overall calculation of different gates.</figcaption></figure><p id="f5f9" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">LSTM对消失和爆炸梯度问题的反应如下。与普通RNNs相比，LSTM的后向投影要干净得多</p><figure class="kf kg kh ki fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mg"><img src="../Images/07ba625b7133075107650c82dacf2923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nqR7x5YWO52hDgxNCCGAbQ.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd kj">Gradient flows smoothly during Backprop,</strong>source: CS231N stanford</figcaption></figure><p id="1de2" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">首先，在反向投影期间没有与矩阵W相乘。它是与f(忘记门)的元素式乘法。所以它的时间复杂度较小。</p><ul class=""><li id="b291" class="mh mi ij ji b jj jk jn jo jr mj jv mk jz ml kd mm mn mo mp dt translated">其次，在通过每个LSTM像元进行反向投影的过程中，它会乘以不同的“忘记命运”值，这使得它不太容易出现消失/爆炸渐变。虽然，如果所有遗忘门的值都小于1，它可能遭受消失梯度，但是在实践中，人们倾向于用某个正数初始化偏置项，所以在训练的开始，f(遗忘门)非常接近1，并且久而久之模型可以学习这些偏置项。</li></ul><figure class="kf kg kh ki fq hw"><div class="bz el l di"><div class="ly lz l"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek">derivative w.r.t input vectors</figcaption></figure><ul class=""><li id="10d4" class="mh mi ij ji b jj jk jn jo jr mj jv mk jz ml kd mm mn mo mp dt translated">尽管如此，该模型可能遭受消失梯度问题，但机会非常少。</li></ul><p id="52a1" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><strong class="ji ik"/>本文仅限于LSTM单元的架构，但<strong class="ji ik">你可以在这里</strong>  <strong class="ji ik">看到完整的代码</strong> <a class="ae mq" href="https://github.com/Manik9/LSTMs" rel="noopener ugc nofollow" target="_blank"> <strong class="ji ik">。</strong>代码还实现了一个使用LSTMs从随机输入生成简单序列的例子。</a></p><p id="29e0" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">我使用深度学习工作室尝试了这个程序:</p><p id="3377" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">深度学习工作室自带内置jupyter笔记本和预装深度学习框架，如Tensorflow、Caffe等..所以你只需要点击<strong class="ji ik">笔记本(在左窗格)</strong>在深度学习工作室打开一个jupyter笔记本就可以了！</p><div class="kf kg kh ki fq ab cb"><figure class="mr hw ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><img src="../Images/cc4311839c7786bbc6d1857f4ab909c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*g_u5-xAH4j-AcONk1waD4A.png"/></div></figure><figure class="mr hw mx mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><img src="../Images/3cf14b4b83243093df1660bafa2df5a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*1vJw-pU-bEFaa09opgyzMQ.png"/></div></figure></div><p id="0268" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">特别感谢<strong class="ji ik">克里斯托弗·奥拉</strong>、<strong class="ji ik">斯坦福CS231n </strong>团队。</p><p id="b552" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">如果你喜欢这篇文章，请分享并鼓掌😄。更多关于深度学习的文章请关注我的<a class="ae mq" rel="noopener" href="/@maniksoni653"> <strong class="ji ik"> Medium </strong> </a>和<a class="ae mq" href="https://www.linkedin.com/in/maniksoni/" rel="noopener ugc nofollow" target="_blank"> <strong class="ji ik"> LinkedIn </strong> </a>。</p><p id="07b4" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">感谢阅读。</p><p id="f715" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">圣诞快乐。</p><p id="d5a3" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="0bee" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">更多学习资料和参考资料:</p><div class="ht hu fm fo hv my"><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab ej"><div class="na ab nb cl cj nc"><h2 class="bd ik fv z el nd eo ep ne er et ii dt translated">了解LSTM网络——colah的博客</h2><div class="nf l"><h3 class="bd b fv z el nd eo ep ne er et ek translated">这些循环使得循环神经网络看起来有点神秘。然而，如果你想得更多一点，事实证明…</h3></div><div class="ng l"><p class="bd b gc z el nd eo ep ne er et ek translated">colah.github.io</p></div></div></div></a></div><figure class="kf kg kh ki fq hw"><div class="bz el l di"><div class="nh lz l"/></div></figure></div></div>    
</body>
</html>