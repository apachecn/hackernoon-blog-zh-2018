<html>
<head>
<title>A Hands-On Introduction to Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络实践介绍</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/a-hands-on-introduction-to-neural-networks-6a03afb468b1?source=collection_archive---------4-----------------------#2018-06-22">https://medium.com/hackernoon/a-hands-on-introduction-to-neural-networks-6a03afb468b1?source=collection_archive---------4-----------------------#2018-06-22</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="0c9c" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated"><strong class="ak">在Python中从头开始实现单个神经元</strong></h2></div><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff jj"><img src="../Images/91714d0468c5e5d1b1bf4b1c41e54544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PSEHaQtQ3D6Bq959ytYcDQ.jpeg"/></div></div></figure><p id="50c9" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在过去的十年里，人工智能(AI)已经牢牢地站在了公众的聚光灯下，这在很大程度上归功于<a class="ae kr" href="https://hackernoon.com/tagged/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a> (ML)和人工神经网络(ann)的进步。</p><p id="dbf5" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">但是，伴随着有前途的新技术而来的是一大堆的嗡嗡声，现在这个领域里有着铺天盖地的噪音。这就是为什么我认为回归基础是有用的，实际上<strong class="jx hv">使用Python从零开始实现单个神经元</strong>。</p><h2 id="9c62" class="ks kt hu bd ku kv kw kx ky kz la lb lc ke ld le lf ki lg lh li km lj lk ll lm dt translated">人工神经元</h2><p id="ed05" class="pw-post-body-paragraph jv jw hu jx b jy ln iv ka kb lo iy kd ke lp kg kh ki lq kk kl km lr ko kp kq hn dt translated">在我们开始之前，我首先想快速地讨论一下什么是神经元<em class="ls">和</em>。人工智能的早期支持者注意到，生物神经元能够从大量数据中概念化和学习，并假设在机器中模拟这种神经元可能允许类似的能力。为此，神经元被抽象成一个输入、输出和权重的模型。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff lt"><img src="../Images/3d02fe083f518266ffb82c126155bd96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FMZFxYcIbr5Bsd5i"/></div></div><figcaption class="lu lv fg fe ff lw lx bd b be z ek">Figure 1: Simple neuron model.</figcaption></figure><p id="f08a" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在机器学习术语中，神经元的每个输入(x1，x2，… xn)被称为一个<strong class="jx hv">特征</strong>，每个特征都用一个数字加权，以表示该输入的强度(w1j，w2j，… wnj)。输入的加权和(netj)然后通过一个<strong class="jx hv">激活函数</strong>、<strong class="jx hv">、</strong>，其一般目的是通过根据一个公式将加权和转换成一个新的数来模拟一个生物神经元的“放电率”。尽管我们现在还不需要深入激活函数的机制，这里有一些由Avinash Sharma V 撰写的<a class="ae kr" rel="noopener" href="/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">阅读材料。</a></p><p id="4628" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">因此，如果这就是一个神经元<em class="ls">如何工作</em>，让我们看看它<em class="ls">如何学习</em>。简单来说，<strong class="jx hv">训练</strong>一个神经元指的是迭代地更新与它的每个输入相关联的权重，以便它能够逐步逼近它所获得的数据集中的潜在关系。一旦经过适当的训练，一个神经元可以用来做一些事情，比如正确地将全新的样本——比如说，猫和狗的图像——分类到不同的桶中，就像人一样。在机器学习术语中，这被称为<strong class="jx hv">分类</strong>。</p><h2 id="27a7" class="ks kt hu bd ku kv kw kx ky kz la lb lc ke ld le lf ki lg lh li km lj lk ll lm dt translated">培养</h2><p id="c83c" class="pw-post-body-paragraph jv jw hu jx b jy ln iv ka kb lo iy kd ke lp kg kh ki lq kk kl km lr ko kp kq hn dt translated">为了训练一个简单的分类器，让我们使用公开可用的<a class="ae kr" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html" rel="noopener ugc nofollow" target="_blank"> sklearn乳腺癌数据集</a>，它具有以下属性:</p><pre class="jk jl jm jn fq ma mb mc md aw me dt"><span id="9624" class="ks kt hu mb b fv mf mg l mh mi">+---------------+-----+<br/>| Classes       |   2 |<br/>+---------------+-----+<br/>| Num Samples   | 569 |<br/>+---------------+-----+<br/>| Num Benign    | 357 |<br/>+---------------+-----+<br/>| Num Malignant | 212 |<br/>+---------------+-----+</span></pre><p id="fa8a" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">数据集中的每个样本都是一个乳房肿块的图像，该图像已被转换为一组30个数字(特征)。使用一部分样本来训练我们的神经元，我们将会看到它是否能够将乳腺肿块中<em class="ls">看不见的</em>部分归类为恶性或良性。换句话说，我们需要执行一个<strong class="jx hv">监督的</strong>学习<strong class="jx hv"> </strong>任务，使用明确标记的数据点作为神经元学习相关模式的老师。</p><p id="45ed" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">要运行并修改下面的代码，请查看这里的脚本:<a class="ae kr" href="https://github.com/dguliani/neural-network-tutorials/blob/master/single-layer-perceptron.py" rel="noopener ugc nofollow" target="_blank">单层感知器. py </a></p></div><div class="ab cl mj mk hc ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="hn ho hp hq hr"><p id="f0c5" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">首先，我们加载数据集，随机将恶性和良性样本混在一起，同时保持每个样本都有标签。这是因为我们不希望我们的神经元根据它看到的样本的顺序得出结论——只根据每个样本的特征。</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="mq mr l"/></div></figure><p id="dac1" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">为了训练我们的神经元，我们基本上需要做三件事:</p><blockquote class="ms mt mu"><p id="af39" class="jv jw ls jx b jy jz iv ka kb kc iy kd mv kf kg kh mw kj kk kl mx kn ko kp kq hn dt translated">1.让神经元对样本进行分类。<br/> 2。根据预测的错误程度更新神经元的权重。<br/> 3。重复一遍。</p></blockquote><p id="5419" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">由于神经元本质上只是权重的集合，我们可以使用Python的矩阵操作包<a class="ae kr" href="http://www.numpy.org/" rel="noopener ugc nofollow" target="_blank"> numpy </a>来随机初始化权重向量。初始化的权重数量对应于神经元的特征(输入)数量，因为每个特征在被求和之前被加权。激活是静态功能，因此不需要软件中的特定表示。</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="mq mr l"/></div></figure><h2 id="3cdf" class="ks kt hu bd ku kv kw kx ky kz la lb lc ke ld le lf ki lg lh li km lj lk ll lm dt translated"><strong class="ak">向前传球</strong></h2><p id="3e01" class="pw-post-body-paragraph jv jw hu jx b jy ln iv ka kb lo iy kd ke lp kg kh ki lq kk kl km lr ko kp kq hn dt translated">在训练的第一步，我们要求神经元对训练样本进行预测。这就是所谓的<strong class="jx hv">正向传递</strong>，它包括对输入特征进行加权求和，并通过激活函数传递该和。</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="mq mr l"/></div></figure><p id="9458" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><em class="ls">上面代码片段中的l0 </em>是一个形状为(n_samples * n_features)的特征矩阵。代表我们单个神经元的权重的形状为(n_features * 1)。因此，这两个矩阵之间的矩阵乘法将为每个样本提供所有特征的加权和。(如果你用手试一下，你会发现这并不像听起来那么复杂。)</p><p id="5d3f" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">当通过激活函数时，这些加权和将有效地成为每个训练样本的类别预测。</p><h2 id="e309" class="ks kt hu bd ku kv kw kx ky kz la lb lc ke ld le lf ki lg lh li km lj lk ll lm dt translated">Sigmoid函数</h2><p id="7c54" class="pw-post-body-paragraph jv jw hu jx b jy ln iv ka kb lo iy kd ke lp kg kh ki lq kk kl km lr ko kp kq hn dt translated"><strong class="jx hv"> sigmoid </strong>函数是逻辑函数的一个特例，这里选择它作为我们的激活函数有几个原因:它是<strong class="jx hv">易微分的</strong>、<strong class="jx hv">非线性的</strong>和<strong class="jx hv">有界的</strong>，其形状和定义如下:</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff my"><img src="../Images/37e66295b70facada11dfa683b99daee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FjdLsOTgG0srpkYs"/></div></div><figcaption class="lu lv fg fe ff lw lx bd b be z ek">Figure 2: Sigmoid function shape.</figcaption></figure><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mz"><img src="../Images/c3bbc3ef746dc0b511cda301f3dc892e.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*8MnsSq5PACXQ9F5xAk2evA.png"/></div><figcaption class="lu lv fg fe ff lw lx bd b be z ek">Figure 3: Sigmoid function definition.</figcaption></figure><p id="92ed" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">该函数用一行代码实现，如下所示:</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="mq mr l"/></div></figure><p id="b66f" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">方便的是，一个标准的逻辑函数有一个<a class="ae kr" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank">容易计算的导数</a>的形式</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff na"><img src="../Images/76c73884839de8cd313948c356cbbe9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/0*jkW_Xj3Tpt_zf-Zo"/></div><figcaption class="lu lv fg fe ff lw lx bd b be z ek">Figure 4: Derivative of a Sigmoid Function</figcaption></figure><p id="cd48" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">其中f(x)代表逻辑函数。正如我们将很快看到的，当试图最小化我们神经元预测的误差时，这个特性是非常有用的。</p><p id="d267" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">sigmoid函数的导数实现如下:</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="mq mr l"/></div></figure><h2 id="8e74" class="ks kt hu bd ku kv kw kx ky kz la lb lc ke ld le lf ki lg lh li km lj lk ll lm dt translated"><strong class="ak">梯度下降</strong></h2><p id="c20c" class="pw-post-body-paragraph jv jw hu jx b jy ln iv ka kb lo iy kd ke lp kg kh ki lq kk kl km lr ko kp kq hn dt translated">现在有趣(也棘手)的部分来了——实际上让我们的神经元学习数据集中的潜在关系。现在，我们已经对每个训练样本进行了有界预测，我们可以计算这些预测中的<strong class="jx hv">误差</strong> / <strong class="jx hv">损失</strong>，并更新与该误差成比例的神经元权重。</p><p id="40b9" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">我们将使用<a class="ae kr" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降优化算法</a>进行权重更新。为了使用这种算法，我们需要一个误差函数来表示我们神经元的预测和真实情况之间的差距。该误差函数被定义为均方误差的缩放版本(缩放使得微分变得容易)。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nb"><img src="../Images/b034e4ff08b11602f06064b6f276bf73.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/0*aoyjuGk6cFB-MYJQ"/></div><figcaption class="lu lv fg fe ff lw lx bd b be z ek">Figure 5: Mean-squared-error function.</figcaption></figure><p id="ac74" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在代码中，该均方误差函数实现如下:</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="mq mr l"/></div></figure><p id="b558" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在数学中，梯度是依赖于多个变量的函数的导数向量。回想一下，矢量既有大小也有方向。我们的神经元的错误依赖于输入它的所有权重。所以，梯度是误差相对于每个权重的偏导数的集合。</p><p id="0ab7" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">作为向量，梯度指向函数最大增长率的方向。因此，向梯度的相反方向移动会使函数最小化。如果我们能够计算我们的神经元的误差函数相对于它的每个权重的梯度，我们可以成比例地更新权重以最小化误差。把误差函数想象成一个有脊和谷的表面。通过与梯度相反的下降，我们进入了误差较低的山谷。</p><p id="3f47" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">下面是使用链式法则对误差函数的梯度的简单推导。更严谨的推导可以在这里找到<a class="ae kr" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nc"><img src="../Images/68032c1fb40512f471c7f6b1dad9cbf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/0*ckcusvv_GkJRZ1jK"/></div><figcaption class="lu lv fg fe ff lw lx bd b be z ek">Figure 6: Partial derivative of error with respect to each neuron weight.</figcaption></figure><p id="f8ff" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">这里，<strong class="jx hv"> E </strong>是误差函数，<strong class="jx hv"> wij </strong>是一个特定的权重，<strong class="jx hv"> oj </strong>是神经元的输出，<strong class="jx hv"> netj </strong>是神经元输入的加权和。索引<strong class="jx hv"> i </strong>和<strong class="jx hv"> j </strong>分别对应于权重和神经元。我们将分别计算偏导数的每个因子。</p><p id="8712" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">第一个因素很简单，是误差相对于神经元输出的导数:</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nd"><img src="../Images/8a781e3466a38274df7898c3691e271e.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*w-plertqxmRy_j80-yajKg.png"/></div><figcaption class="lu lv fg fe ff lw lx bd b be z ek">Figure 7: Derivative of output error with respect to neuron output.</figcaption></figure><p id="c75b" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">第二个因素也很简单，是我们在图4中描述的sigmoid函数的导数。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff ne"><img src="../Images/2e42e5a04ef735def21e801b8dede0e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/0*Bk78f45dsFJ2R4Kq"/></div><figcaption class="lu lv fg fe ff lw lx bd b be z ek">Figure 8: Derivative of neuron output with respect to the weighted sum.</figcaption></figure><p id="8e7f" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">第三个也是最后一个因素简化为等于特定神经元的输入。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nf"><img src="../Images/697c6180cff7f25fc889d840c680deba.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/0*NYmLbs86CTIy_gHn"/></div><figcaption class="lu lv fg fe ff lw lx bd b be z ek">Figure 9: Partial derivative of the weighted sum of inputs with respect to each weight.</figcaption></figure><p id="eb09" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在图9中，<strong class="jx hv"> oi </strong>是该神经元的输入向量，在我们的例子中是来自我们训练集的特征。</p><h2 id="52ac" class="ks kt hu bd ku kv kw kx ky kz la lb lc ke ld le lf ki lg lh li km lj lk ll lm dt translated">权重更新规则</h2><p id="a809" class="pw-post-body-paragraph jv jw hu jx b jy ln iv ka kb lo iy kd ke lp kg kh ki lq kk kl km lr ko kp kq hn dt translated">将我们刚刚看到的偏导数与下降联系在一起，为我们提供了更新代表神经元的权重的规则:</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff ng"><img src="../Images/2ff82e3b7e71ca64623d2b7ec3c2fc8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/0*Jgwveyl29yB_e291"/></div><figcaption class="lu lv fg fe ff lw lx bd b be z ek">Figure 10: Weight update rule.</figcaption></figure><p id="bd3f" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">图10显示每个权重将在梯度的负方向上更新，与附加项<strong class="jx hv"> n </strong>成比例。比例因子n决定了我们在更新神经元权重时的步长，有效地控制了神经元的学习速率。我们称之为<strong class="jx hv">学习率</strong>。</p><h2 id="a9c4" class="ks kt hu bd ku kv kw kx ky kz la lb lc ke ld le lf ki lg lh li km lj lk ll lm dt translated">实施权重更新</h2><p id="079d" class="pw-post-body-paragraph jv jw hu jx b jy ln iv ka kb lo iy kd ke lp kg kh ki lq kk kl km lr ko kp kq hn dt translated">下面是我们的单个神经元的梯度计算和权重更新的实现。您可以按照注释找到权重更新规则所需的导数的每一步。</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="mq mr l"/></div></figure><p id="9019" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在训练神经网络时，相同的训练数据通过<a class="ae kr" href="https://hackernoon.com/tagged/network" rel="noopener ugc nofollow" target="_blank">网络</a>运行多次，每次完整的通过被称为一个<strong class="jx hv">时期</strong>。(<a class="ly lz gr" href="https://medium.com/u/165370addbb5?source=post_page-----6a03afb468b1--------------------------------" rel="noopener" target="_blank"> SAGAR SHARMA </a>在这篇文章中很好地解释了为什么我们在神经网络中多次使用相同的数据。)随着每个时期，权重进一步更新以尝试降低误差。对于我们的简单例子，通过反复试验选择历元数和学习速率，观察mse损失减少和收敛。</p><h2 id="b020" class="ks kt hu bd ku kv kw kx ky kz la lb lc ke ld le lf ki lg lh li km lj lk ll lm dt translated">结果</h2><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nh"><img src="../Images/8dcf310025fe1c770873c9e016041a31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m-1afpz4AI-vu2GE-MmHyA.png"/></div></div><figcaption class="lu lv fg fe ff lw lx bd b be z ek">Figure 11: Training results.</figcaption></figure><p id="f0e5" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">图11显示，在数百个时期内，训练和测试数据集的损失减少，准确性增加。接下来，我们通过在同一数据集上训练10个不同的随机初始化的神经元来检查这个训练过程是否是可重复的。在10次训练运行结束时，平均测试准确率为90.49% (s=2.40%)，平均总准确率为90.33% (s=0.304%)。</p><p id="4342" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">如果我们看到训练和测试准确性的巨大差异，或者如果测试损失增加而训练损失减少，我们就有理由相信神经元没有学习隐藏在数据集中的模式。虽然这种验证水平还不足以将该神经元投入生产环境，但有迹象表明该神经元已经在数据集中学习了一种模式。</p><h2 id="cfe8" class="ks kt hu bd ku kv kw kx ky kz la lb lc ke ld le lf ki lg lh li km lj lk ll lm dt translated">结论</h2><p id="cfbd" class="pw-post-body-paragraph jv jw hu jx b jy ln iv ka kb lo iy kd ke lp kg kh ki lq kk kl km lr ko kp kq hn dt translated">我们在这里研究了人工神经网络的最简单形式，即一个由梯度下降驱动的单个神经元。网络可以由许多神经元或其他可训练的过滤器/单元组成，并根据其目的使用各种损失和激活函数。所有这些扩展允许人工神经网络执行广泛的任务，如对象检测、语言翻译、时间序列预测等。</p><p id="7404" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在我们的下一篇文章中，我们将探索单个神经元的局限性，并通过神经元链或<strong class="jx hv">层</strong>更深入地挖掘错误流。通过神经网络的反向误差流允许一组神经元一起收敛于一个解决方案。因此，我们将能够通过我们的神经网络传递更大、更复杂的数据集。</p><h2 id="c0b0" class="ks kt hu bd ku kv kw kx ky kz la lb lc ke ld le lf ki lg lh li km lj lk ll lm dt translated">作者的评论</h2><p id="22c1" class="pw-post-body-paragraph jv jw hu jx b jy ln iv ka kb lo iy kd ke lp kg kh ki lq kk kl km lr ko kp kq hn dt translated">我想对<a class="ly lz gr" href="https://medium.com/u/eca9f7f76ad2?source=post_page-----6a03afb468b1--------------------------------" rel="noopener" target="_blank"> Eli Burnstein </a>、<a class="ly lz gr" href="https://medium.com/u/96feba698dee?source=post_page-----6a03afb468b1--------------------------------" rel="noopener" target="_blank"> William Wen </a>、<a class="ly lz gr" href="https://medium.com/u/11382a5b2c64?source=post_page-----6a03afb468b1--------------------------------" rel="noopener" target="_blank"> Guy Tonye </a>和<a class="ly lz gr" href="https://medium.com/u/1d1995e6d926?source=post_page-----6a03afb468b1--------------------------------" rel="noopener" target="_blank"> Thomas Aston </a>在多次修改中帮助校对和编辑这篇文章表示衷心的感谢。</p><p id="dd9e" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">像往常一样，留下评论并指出你在这里发现的任何错误。我会尽快修复它们！</p><h2 id="79c0" class="ks kt hu bd ku kv kw kx ky kz la lb lc ke ld le lf ki lg lh li km lj lk ll lm dt translated">脚注</h2><ol class=""><li id="5071" class="ni nj hu jx b jy ln kb lo ke nk ki nl km nm kq nn no np nq dt translated">感知机，一种感知和识别的自动机器，作者弗兰克·罗森布拉特。</li><li id="7798" class="ni nj hu jx b jy nr kb ns ke nt ki nu km nv kq nn no np nq dt translated">在我们非常简单的例子中，我们将权重集中在平均值0附近。然而，对于较大的模型，有更好的方法来初始化权重。<a class="ae kr" href="https://towardsdatascience.com/deep-learning-best-practices-1-weight-initialization-14e5c0295b94" rel="noopener" target="_blank">这里</a>是<a class="ly lz gr" href="https://medium.com/u/3dff2ff1664?source=post_page-----6a03afb468b1--------------------------------" rel="noopener" target="_blank"> Neerja Doshi </a>对权重初始化最佳实践的精彩介绍。</li></ol></div><div class="ab cl mj mk hc ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="hn ho hp hq hr"><p id="45a8" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><em class="ls"> Dhruv是</em> <a class="ly lz gr" href="https://medium.com/u/37b22d8cbc82?source=post_page-----6a03afb468b1--------------------------------" rel="noopener" target="_blank"> <em class="ls">互联实验室</em> </a> <em class="ls">的人工智能软件工程师，这是一家产品开发公司，与客户合作，通过软件驱动的产品来推动影响。</em><a class="ae kr" href="http://www.connectedlab.com" rel="noopener ugc nofollow" target="_blank"><em class="ls">www.connectedlab.com</em></a></p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="nw mr l"/></div></figure></div></div>    
</body>
</html>