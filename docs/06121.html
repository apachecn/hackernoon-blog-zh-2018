<html>
<head>
<title>Neural Machine Translation: Using Open-NMT for training a translation model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经机器翻译:使用开放NMT训练翻译模型</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/neural-machine-translation-using-open-nmt-for-training-a-translation-model-1129a3a2a2d3?source=collection_archive---------2-----------------------#2018-07-24">https://medium.com/hackernoon/neural-machine-translation-using-open-nmt-for-training-a-translation-model-1129a3a2a2d3?source=collection_archive---------2-----------------------#2018-07-24</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="63b0" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">学习任何语言对之间翻译的完整指南</h2></div><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff jj"><img src="../Images/c9b630fbd1cfbaede9c2f11668eeacdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P7K66VvjcHJsr8CNR147Ow.png"/></div></div></figure><p id="25e4" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><em class="kr">无耻插件:我们是一个</em> <a class="ae ks" href="https://dataturks.com" rel="noopener ugc nofollow" target="_blank"> <em class="kr">机器学习数据标注平台</em> </a> <em class="kr">让你超级轻松的</em> <a class="ae ks" href="https://dataturks.com" rel="noopener ugc nofollow" target="_blank"> <em class="kr">构建ML数据集</em> </a> <em class="kr">。只需上传数据，邀请您的团队，快速构建数据集。</em></p><p id="ef80" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">这个博客旨在提供一个循序渐进的教程来学习从一种给定的语言到任何目标语言的翻译。我们为手头的任务使用的方法完全是受开源库的激励，开源库的pyTorch实现可以用python语言获得，称为开放NMT(开源神经机器翻译)。它旨在方便深度学习爱好者在机器翻译、摘要、图像到文本转换、形态学等领域实现他们的想法。</p><p id="7945" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">虽然Google Translate、Microsoft等公司有很多高效的翻译系统，但它们要么不是开源的，要么在限制性许可下是封闭的。其他像tensorflow-seq2seq模型这样的库也是为了这个目的而存在的，但是作为研究代码。开放NMT不仅是开源的，而且还提供大量文档化、模块化和可读的代码，用于模型的快速训练和高效运行。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff kt"><img src="../Images/74dd442cbe03f21c6b9f433070508f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*y6bOZ9c1P2Sv0gbkoxkNQw.jpeg"/></div></figure><p id="01fa" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">我们进一步阐述了建立库和使用工具包训练你自己的定制翻译系统的详细指南。这个博客处理从给定的英语文本生成印地语翻译。</p><h1 id="6538" class="ku kv hu bd kw kx ky kz la lb lc ld le ja lf jb lg jd lh je li jg lj jh lk ll dt translated">开放式NMT建筑的简要概述；</h1><p id="9fc2" class="pw-post-body-paragraph jv jw hu jx b jy lm iv ka kb ln iy kd ke lo kg kh ki lp kk kl km lq ko kp kq hn dt translated">开-NMT是根据纪尧姆·克莱因等人的研究，在这里发现了<a class="ae ks" href="http://aclweb.org/anthology/P17-4012" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff lr"><img src="../Images/4caa8acb8b31bd48ba2d80a2a08a5996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQY3yk6XChJa8-tzuNve2w.png"/></div></div></figure><p id="f20a" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">根据该论文，揭示了关于其架构的以下细节:</p><blockquote class="ls lt lu"><p id="45a6" class="jv jw kr jx b jy jz iv ka kb kc iy kd lv kf kg kh lw kj kk kl lx kn ko kp kq hn dt translated">OpenNMT是一个用于训练和部署神经机器翻译模型的完整库。该系统是哈佛大学开发的seq2seq-attn的继任者，为了提高效率、可读性和通用性，已经完全重写。它包括普通的NMT模型，以及对注意力、门控、堆叠、输入馈送、正则化、波束搜索和所有其他为最先进的性能所必需的选项的支持。</p><p id="4e07" class="jv jw kr jx b jy jz iv ka kb kc iy kd lv kf kg kh lw kj kk kl lx kn ko kp kq hn dt translated">主系统在Lua/Torch数学框架中实现，并且可以使用Torch的内部标准神经网络组件轻松扩展。脸书研究所的Adam Lerer也用同样的API对它进行了扩展，以支持Python/PyTorch框架。</p></blockquote><h1 id="a26d" class="ku kv hu bd kw kx ky kz la lb lc ld le ja lf jb lg jd lh je li jg lj jh lk ll dt translated">所需模块的设置</h1><p id="55ac" class="pw-post-body-paragraph jv jw hu jx b jy lm iv ka kb ln iy kd ke lo kg kh ki lp kk kl km lq ko kp kq hn dt translated">训练你的自定义翻译系统所需要的主要软件包本质上是pyTorch，其中实现了开放NMT模型。</p><p id="fb61" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">当然，第一步是克隆OpenNMT-py存储库:</p><pre class="jk jl jm jn fq ly lz ma mb aw mc dt"><span id="d538" class="md kv hu lz b fv me mf l mg mh">git clone https://github.com/OpenNMT/OpenNMT-py<br/>cd OpenNMT-py</span></pre><p id="c66b" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">这里有一个requirements.txt文件来收集所有需要的包:</p><pre class="jk jl jm jn fq ly lz ma mb aw mc dt"><span id="0830" class="md kv hu lz b fv me mf l mg mh">six<br/>tqdm<br/>torch&gt;=0.4.0<br/>git+<a class="ae ks" href="https://github.com/pytorch/text" rel="noopener ugc nofollow" target="_blank">https://github.com/pytorch/text</a><br/>future</span></pre><p id="4163" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">由于PyTorch一直在不断发展，我们建议派生PyTorch 0.4版本，以确保代码库的稳定性能。</p><p id="be64" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">运行以下命令自动收集先决条件依赖项:</p><pre class="jk jl jm jn fq ly lz ma mb aw mc dt"><span id="af24" class="md kv hu lz b fv me mf l mg mh">pip install -r requirements.txt</span></pre><h1 id="bb8b" class="ku kv hu bd kw kx ky kz la lb lc ld le ja lf jb lg jd lh je li jg lj jh lk ll dt translated">收集数据集</h1><p id="fcde" class="pw-post-body-paragraph jv jw hu jx b jy lm iv ka kb ln iy kd ke lo kg kh ki lp kk kl km lq ko kp kq hn dt translated">数据集包括源和目标语言文件的并行语料库，每行包含一个句子，使得每个标记由空格分隔。</p><p id="1ab3" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">对于我们的教程，我们使用存储在不同文件中的英语和印地语句子的平行语料库。这些数据是从各种来源收集来的，然后加以综合。然后重新排列数据，以创建一组文件，如下所示:</p><ul class=""><li id="908e" class="mi mj hu jx b jy jz kb kc ke mk ki ml km mm kq mn mo mp mq dt translated"><code class="eh mr ms mt lz b">src-train.txt : Training file containing 10000 English (Source Language) sentences</code></li><li id="4a0c" class="mi mj hu jx b jy mu kb mv ke mw ki mx km my kq mn mo mp mq dt translated"><code class="eh mr ms mt lz b">tgt-train.txt : Training file containing 10000 Hindi (Target Language) sentences</code></li><li id="00d8" class="mi mj hu jx b jy mu kb mv ke mw ki mx km my kq mn mo mp mq dt translated"><code class="eh mr ms mt lz b">src-val.txt : Validation data consisting of 1000 English (Source Language) sentences</code></li><li id="c6d3" class="mi mj hu jx b jy mu kb mv ke mw ki mx km my kq mn mo mp mq dt translated"><code class="eh mr ms mt lz b">tgt-val.txt : Validation data consisting of 1000 Hindi (Target Language) sentences</code></li><li id="d092" class="mi mj hu jx b jy mu kb mv ke mw ki mx km my kq mn mo mp mq dt translated"><code class="eh mr ms mt lz b">src-test.txt : Test Evaluation data consisting of 1000 English (Source Language) sentences</code></li><li id="a4fd" class="mi mj hu jx b jy mu kb mv ke mw ki mx km my kq mn mo mp mq dt translated"><code class="eh mr ms mt lz b">tgt-test.txt : Test Evaluation data consisting of 1000 Hindi (Target Language) sentences</code></li></ul><p id="29ab" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">以上所有文件都放在/data目录中。</p><p id="b500" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">注意:在本教程中，我们使用了有限的数据进行解释和实验。然而，建议使用包含数百万个句子的大型语料库，以确保独特单词的大量词汇，从而更好地学习和接近人类的翻译。</p><p id="f50f" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">验证数据用于评估模型的每一步，以确定收敛点。它通常应该包含最多5000个句子。</p><p id="2af9" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">以下示例显示了文本数据在相应文件中的排列方式:</p><pre class="jk jl jm jn fq ly lz ma mb aw mc dt"><span id="4106" class="md kv hu lz b fv me mf l mg mh">Source Files :<br/>They also bring out a number of Tamil weekly newspapers.<br/>They are a hard — working people and most of them work as labourers.<br/>Tamil films are also shown in the local cinema halls.<br/>There are quite a large number of Malayalees living here.</span><span id="d43f" class="md kv hu lz b fv mz mf l mg mh">Target Files :</span><span id="359f" class="md kv hu lz b fv mz mf l mg mh">तमिल भाषा में वे अनेक समाचार पत्र व पत्रिकाएं भी निकालते हैं .<br/>ये लोग काफी परिश्रमी हैं , अधिकांश लोग मजदूरी करते हैं .<br/>स्थानीय सिनेमा हालों में तमिल चलचित्रों का प्रदर्शन अक्सर किया जाता है .<br/>मलयालम लोगों की बहुत बडी संख्या है .</span></pre><h1 id="de5b" class="ku kv hu bd kw kx ky kz la lb lc ld le ja lf jb lg jd lh je li jg lj jh lk ll dt translated">预处理文本数据:</h1><p id="92af" class="pw-post-body-paragraph jv jw hu jx b jy lm iv ka kb ln iy kd ke lo kg kh ki lp kk kl km lq ko kp kq hn dt translated">执行以下命令来预处理训练和验证数据，提取用于训练的特征，并为模型生成词汇文件。</p><pre class="jk jl jm jn fq ly lz ma mb aw mc dt"><span id="5c35" class="md kv hu lz b fv me mf l mg mh">python preprocess.py -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo</span></pre><h1 id="80bf" class="ku kv hu bd kw kx ky kz la lb lc ld le ja lf jb lg jd lh je li jg lj jh lk ll dt translated">培训翻译模型:</h1><p id="50c3" class="pw-post-body-paragraph jv jw hu jx b jy lm iv ka kb ln iy kd ke lo kg kh ki lp kk kl km lq ko kp kq hn dt translated">训练的主要命令使用起来非常简单。本质上，它接受一个数据文件和一个保存文件作为输入。</p><p id="69b2" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">使用的默认模型总结如下:</p><pre class="jk jl jm jn fq ly lz ma mb aw mc dt"><span id="500e" class="md kv hu lz b fv me mf l mg mh">NMTModel(<br/>  (encoder): RNNEncoder(<br/>    (embeddings): Embeddings(<br/>      (make_embedding): Sequential(<br/>        (emb_luts): Elementwise(<br/>          (0): Embedding(20351, 500, padding_idx=1)<br/>        )<br/>      )<br/>    )<br/>    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)<br/>  )<br/>  (decoder): InputFeedRNNDecoder(<br/>    (embeddings): Embeddings(<br/>      (make_embedding): Sequential(<br/>        (emb_luts): Elementwise(<br/>          (0): Embedding(20570, 500, padding_idx=1)<br/>        )<br/>      )<br/>    )<br/>    (dropout): Dropout(p=0.3)<br/>    (rnn): StackedLSTM(<br/>      (dropout): Dropout(p=0.3)<br/>      (layers): ModuleList(<br/>        (0): LSTMCell(1000, 500)<br/>        (1): LSTMCell(500, 500)<br/>      )<br/>    )<br/>    (attn): GlobalAttention(<br/>      (linear_in): Linear(in_features=500, out_features=500, bias=False)<br/>      (linear_out): Linear(in_features=1000, out_features=500, bias=False)<br/>      (softmax): Softmax()<br/>      (tanh): Tanh()<br/>    )<br/>  )<br/>  (generator): Sequential(<br/>    (0): Linear(in_features=500, out_features=20570, bias=True)<br/>    (1): LogSoftmax()<br/>  )<br/>)</span></pre></div><div class="ab cl na nb hc nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="hn ho hp hq hr"><pre class="ly lz ma mb aw mc dt"><span id="340e" class="md kv hu lz b fv nh ni nj nk nl mf l mg mh">python train.py -data data/demo -save_model demo-model</span></pre><p id="856a" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">上面的命令将运行一个默认模型，包括一个两层LSTM，拥有500个隐藏单元，用于编码器和解码器。要指定将GPU用于训练，请在上面的命令中指定-gpuid参数(比如-gpuid 1用于指定用法og GPU 1)。</p><p id="13ca" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">典型地，默认模型持续到100000个时期，因此每5000个时期后保存一个检查点。所以如果你的模型收敛了，验证精度更早的达到一个稳定点，你就可以停止进一步的训练，使用之前保存的检查点。</p><h1 id="c628" class="ku kv hu bd kw kx ky kz la lb lc ld le ja lf jb lg jd lh je li jg lj jh lk ll dt translated">翻译您自己的数据:</h1><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nm"><img src="../Images/9ceff69b068d79314238b350596ee8f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*CJRabEYd5PgFy-p9JWhaBw.jpeg"/></div></figure><p id="faf4" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">可以执行下面的命令来对源语言(英语)中的看不见的文本执行推断步骤，并生成相应的预测翻译:</p><pre class="jk jl jm jn fq ly lz ma mb aw mc dt"><span id="0aef" class="md kv hu lz b fv me mf l mg mh">python translate.py -model demo-model_XYZ.pt -src data/src-test.txt -output pred.txt -replace_unk -verbose</span></pre><p id="d7de" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">这将生成翻译后的输出，并将预测存储到名为<code class="eh mr ms mt lz b">pred.txt</code>的文件中。</p><p id="6fad" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">该模型在NVIDIA GEFORCE 2GB GPU上训练了10000个纪元。在CPU上进行训练将需要非常高的计算成本，因此建议使用高端GPU以更快的速度训练具有大量数据的模型。</p><h1 id="f1fe" class="ku kv hu bd kw kx ky kz la lb lc ld le ja lf jb lg jd lh je li jg lj jh lk ll dt translated">模型的预测示例:</h1><p id="a568" class="pw-post-body-paragraph jv jw hu jx b jy lm iv ka kb ln iy kd ke lo kg kh ki lp kk kl km lq ko kp kq hn dt translated">下面显示的是在训练模型后为相应的英语句子生成的印地语翻译的几个示例。</p><pre class="jk jl jm jn fq ly lz ma mb aw mc dt"><span id="f804" class="md kv hu lz b fv me mf l mg mh">Trees are capable of absorbing more of carbon dioxide, thus maintaining equilibrium in the air composition .</span><span id="6d0e" class="md kv hu lz b fv mz mf l mg mh">PREDICTED : पेडों में कार्बन डाईआक्साइड के बुरे लोग इस प्रकार पेड - पौधे का प्रयोग करने के लिए मौजूद हैं .<br/><br/>He has hope that the gods will throw good things from the heavens , upon them .</span><span id="67e3" class="md kv hu lz b fv mz mf l mg mh">PREDICTED :वे उमीद है कि वे घर से कुछ नहीं बची हैं .</span><span id="5198" class="md kv hu lz b fv mz mf l mg mh">The Buddhist temple , the Dalai Lama Palace and dispensary of Tibet are tourist attractions here .</span><span id="5e12" class="md kv hu lz b fv mz mf l mg mh">PREDICTED :यहां का बौद्ध मंदिर दलाई लामा का आवास तथा तिब्बती औषधालय स्थानिय लोगो में मिलता है .</span><span id="e6e1" class="md kv hu lz b fv mz mf l mg mh">He lets the hair grow long.</span><span id="3a9f" class="md kv hu lz b fv mz mf l mg mh">PREDICTED : वह अपने बढा लेता है .</span></pre><p id="46ed" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">如上所述，由于用于任何实时翻译的训练数据较少，预测还不够好。为了执行接近真实世界的翻译，该模型必须在大量词汇和大约一百万个句子上进行训练，这将同时涉及硬件要求和训练时间方面的大量计算成本。</p><h1 id="1df9" class="ku kv hu bd kw kx ky kz la lb lc ld le ja lf jb lg jd lh je li jg lj jh lk ll dt translated">评估您训练的模型:</h1><h2 id="001e" class="md kv hu bd kw nn no np la nq nr ns le ke nt nu lg ki nv nw li km nx ny lk nz dt translated">双语评估替角分数</h2><p id="fe22" class="pw-post-body-paragraph jv jw hu jx b jy lm iv ka kb ln iy kd ke lo kg kh ki lp kk kl km lq ko kp kq hn dt translated">双语评估替角分数(BLEU Score)是指通过将生成的句子与参考句子进行比较来评估机器翻译系统的评估指标。</p><p id="b491" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在该比较中完全匹配导致BLEU分数为1.0，而完全不匹配导致BLEU分数为0.0。</p><p id="226d" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">BLEU评分是一个普遍适用的评估翻译模型的标准，因为它独立于语言，易于解释，并且与人工评估高度相关。</p><p id="0872" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">BLEU评分是在Kishore Papineni等人进行的一项研究中提出的。<a class="ae ks" href="http://www.aclweb.org/anthology/P02-1040.pdf" rel="noopener ugc nofollow" target="_blank"> BLEU:一种自动评估机器翻译的方法</a>。</p><p id="6c12" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在对与参考文本中的n元语法匹配的候选翻译中的n元语法进行计数之后，生成BLEU分数。在这种比较中不考虑词序。</p><p id="c220" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">那么我们如何定义一个n-gram呢？假设一个一元单词或一元单词表示每个单独的单词，一个二元单词表示每一对单词。</p><p id="1b59" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">计算BLEU分数的代码，给定你预测的候选文件和GitHub资源库中给定的参考文件，博客末尾提供了whic的链接。</p><p id="88c3" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">下面是我们如何运行代码来评估模型:</p><pre class="jk jl jm jn fq ly lz ma mb aw mc dt"><span id="c429" class="md kv hu lz b fv me mf l mg mh">python calculatebleu.py "pred.txt" "tgt-test.txt"</span></pre><p id="9870" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">其中pred.txt是我们的候选预测翻译文件，tgt-test.txt是包含目标语言的实际翻译的文件。</p><p id="95f9" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">由于我们用10k个句子生成的数据词汇仅包含几千个单词，我们在预测中得到的BLEU分数相当差(0.025)。</p><p id="e6e5" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">注意:由于我们的主要目的集中在阐述开放NMT的使用，我们只使用了一个小数据集，这就是为什么我们的预测翻译结果的评估是一个糟糕的BLEU分数。大约0.5的BLEU分数意味着相当好的翻译。通过增加几千个例子来增加训练词汇，从而提高分数。</p><p id="a1d3" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">然而，Open-NMT允许我们在任何一对语言之间训练我们自己的自定义翻译器模型，并且使用起来非常方便。</p><p id="400b" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">用于生成BLEU分数的代码和用于训练我们的模型的数据集已经在这里提供<a class="ae ks" href="https://github.com/DataTurks-Engg/Neural_Machine_Translation" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="f0ff" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">如果您有任何疑问或建议，我很乐意倾听。请在abhishek.narayanan@dataturks.com给我写信。</p><p id="ad0b" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><em class="kr">无耻插件:我们是一个数据注释平台，让你建立ML数据集超级容易。只需上传数据，邀请您的团队，快速构建数据集。</em> <a class="ae ks" href="https://dataturks.com/index.php" rel="noopener ugc nofollow" target="_blank"> <strong class="jx hv"> <em class="kr">快来看看吧！</em> </strong> </a></p></div></div>    
</body>
</html>