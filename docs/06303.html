<html>
<head>
<title>Getting Ramped-Up on Airflow with MySQL → S3 → Redshift</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过MySQL → S3 →红移加速气流</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/getting-ramped-up-on-airflow-with-mysql-s3-redshift-defcc4522c8c?source=collection_archive---------5-----------------------#2018-07-30">https://medium.com/hackernoon/getting-ramped-up-on-airflow-with-mysql-s3-redshift-defcc4522c8c?source=collection_archive---------5-----------------------#2018-07-30</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="9ba3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我最近以数据工程师的身份加入了<a class="ae jp" href="https://www.plaid.com" rel="noopener ugc nofollow" target="_blank"> Plaid </a>，并开始使用<a class="ae jp" rel="noopener" href="/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8"> Airflow </a>，这是一个我们用来管理内部ETL管道的工作流工具。如果你完全不熟悉Airflow，可以把它想象成一个界面更好的升级版crontab。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff jq"><img src="../Images/a620e3ebebc01dbab2a571fc9095feca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zd-Z796er5lxaFW81EaWGA.png"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">It’s fast! It’s flexible! It’s free! It’s Airflow!</figcaption></figure><p id="1e90" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在我加入的时候，Plaid正在迁移到用于可视化SQL查询的<a class="ae jp" href="https://www.periscopedata.com/" rel="noopener ugc nofollow" target="_blank"> Periscope数据</a>,我的直接任务变成了获取更多的数据，人们依赖这些数据来分析我们新生的<a class="ae jp" href="https://hackernoon.com/tagged/redshift" rel="noopener ugc nofollow" target="_blank">红移</a>集群，即我们从Periscope查询的数据仓库。我的同事花了一些时间将我们的MongoDB表收集到Redshift中，方法是跟踪操作日志并在Spark中进行一些后处理，以及在S3解析一堆日志转储以创建有意义的分析表。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff kg"><img src="../Images/1298d986d99a6a0e340b7a69f59a547e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Dt4pTxhZXWN8CtwF"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">Plaid ETL pipeline circa early 2018</figcaption></figure><h2 id="2dc6" class="kh ki hu bd kj kk kl km kn ko kp kq kr jc ks kt ku jg kv kw kx jk ky kz la lb dt translated">动机</h2><p id="4c25" class="pw-post-body-paragraph ir is hu it b iu lc iw ix iy ld ja jb jc le je jf jg lf ji jj jk lg jm jn jo hn dt translated">我的同事好心地给我留了一个更简单的任务，帮助我适应air flow——定期将数据从MySQL转移到Redshift。我们最近开始在RDS上使用Amazon Aurora实例，需要从RDS中获取数据并将其加载到Redshift中，以建立这些新数据集的KPI。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff kg"><img src="../Images/830d99de687d8278a91ba557da75a1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0hMmiPfougfcSfmz"/></div></div></figure><p id="16b8" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们的目标是在一周内完成这个练习，所以我们给自己设定了一些限制:</p><ol class=""><li id="7d28" class="lh li hu it b iu iv iy iz jc lj jg lk jk ll jo lm ln lo lp dt translated">我们将使用MySQL的选择s3的能力和Redshift的卸载命令。</li><li id="c29d" class="lh li hu it b iu lq iy lr jc ls jg lt jk lu jo lm ln lo lp dt translated">我们每天只对每个表执行一次完整的表复制。由于复制我们最大的表只需要几个小时，我们决定接受每天晚上提取所有数据，这样更容易实现，并且对两个数据源中的变化或不一致更有弹性。</li><li id="b8ab" class="lh li hu it b iu lq iy lr jc ls jg lt jk lu jo lm ln lo lp dt translated">我们将致力于获取最重要的数据，而不是担心像深度嵌套的json列或存储在数据库中的二进制图像文件这样的细节。</li></ol><h2 id="5f7c" class="kh ki hu bd kj kk kl km kn ko kp kq kr jc ks kt ku jg kv kw kx jk ky kz la lb dt translated">设计工作流程</h2><p id="dd66" class="pw-post-body-paragraph ir is hu it b iu lc iw ix iy ld ja jb jc le je jf jg lf ji jj jk lg jm jn jo hn dt translated">我们想设计一个从MySQL到Redshift的转换，并且知道必须有模式的转换。幸运的是，AWS提供了一个比较MySQL和Redshift类型的资源<a class="ae jp" href="https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html" rel="noopener ugc nofollow" target="_blank">。</a>我们可以使用一个简单的查询将模式从MySQL中提取出来:</p><pre class="jr js jt ju fq lv lw lx ly aw lz dt"><span id="aace" class="kh ki hu lw b fv ma mb l mc md"><strong class="lw hv">select</strong> <br/>  table_name, column_name, data_type, ordinal_position, character_maximum_length <br/><strong class="lw hv">from</strong> <br/>  information_schema.columns <br/><strong class="lw hv">where</strong> <br/>  table_schema = database();</span></pre><p id="6589" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们手动处理了一些类型，例如，我们不是移动二进制数据，而是检测一个二进制类型，并返回一个关于列是null还是非null的布尔值，以避免必须通过网络复制大量二进制数据，这对分析是不可用的。我们将该功能打包到一些生成翻译配置的python脚本中。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff me"><img src="../Images/8107382363492b87b63cfd29b6e0e507.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Nx5B0xgn3StVNWDS"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">Example MySQL configuration</figcaption></figure><p id="204d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然后，我们创建dag_mysql_processor.py来接收这些数据库配置并生成相关的dag。它遍历每个条目，并使用我们使用<a class="ae jp" href="https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html" rel="noopener ugc nofollow" target="_blank"> AWS文档</a>作为指南编写的一系列SQL模板生成相应的步骤。例如:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff kg"><img src="../Images/ac1301a88d84f87953dbdd32d82c0a5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ikFXRIHAMiyyWScJ"/></div></div></figure><p id="21af" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">结果是一批Airflow DAGs，MySQL数据库中的每个表都有一个。目前，我们每天运行一次这些Dag，通过每天完全重建一次表，为我们当前的使用情形提供了足够好的延迟。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff mf"><img src="../Images/f2883fee90b3c304d7f51f78b7f3914d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bslSQlbJWpEU6gLVQEcBhQ.png"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">Our linear DAG pulling data from MySQL to S3 to Redshift</figcaption></figure><h1 id="b125" class="mg ki hu bd kj mh mi mj kn mk ml mm kr mn mo mp ku mq mr ms kx mt mu mv la mw dt translated">道路上的颠簸</h1><p id="cb02" class="pw-post-body-paragraph ir is hu it b iu lc iw ix iy ld ja jb jc le je jf jg lf ji jj jk lg jm jn jo hn dt translated">第一点麻烦来自尝试热插拔。我们希望在替换旧数据之前确保表看起来是正确的，所以我们添加了一个步骤来验证行数。在我们的DAG的第一个版本中，我们将每个语句作为一个单独的气流任务来执行，但是表格偶尔会消失。将交换步骤合并到单个事务性任务中，可以防止我们的全表替换方式出现任何表停机。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff mx"><img src="../Images/e5e8ada6c8f6b0a5bf2cd9a8841718fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NvjBzDLxrkNvLFWPAhj8OQ.png"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">V1 of the project had a race condition in dropping and recreating the table.</figcaption></figure><p id="eb32" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们预计完整复制会很麻烦，因为我们的一些表有几十亿行，并且全局“成功或失败”(只进行完整表复制的结果)将很难实时恢复。我们在一些最大的表上运行了一些实验，以了解错误可能发生在哪里，并且<code class="eh my mz na lw b">stl_load_errors</code>很快成为我们的首选调试<a class="ae jp" href="https://hackernoon.com/tagged/tool" rel="noopener ugc nofollow" target="_blank">工具</a>。</p><pre class="jr js jt ju fq lv lw lx ly aw lz dt"><span id="54b6" class="kh ki hu lw b fv ma mb l mc md"><strong class="lw hv">select</strong> * <strong class="lw hv">from</strong> stl_load_errors <strong class="lw hv">order by</strong> starttime <strong class="lw hv">desc</strong> <strong class="lw hv">limit</strong> 1;</span></pre><p id="d7f6" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们在将列从MySQL类型转换为红移类型时遇到了一些问题。通过添加一些逻辑来说明转换过程中的特定类型，并说明我们的模式中的模式，我们能够解决很多这样的问题，我们能够基于列名和类型进行匹配。</p><pre class="jr js jt ju fq lv lw lx ly aw lz dt"><span id="c035" class="kh ki hu lw b fv ma mb l mc md">potential_binary_column = <br/>  <strong class="lw hv">'md5' in </strong>column_name <strong class="lw hv">or 'uuid' in </strong>column_name</span></pre><p id="f879" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">错误的另一个罪魁祸首是数据中出现的换行符，这会导致一行被分成两行(或更多行),然后无法解析。这里的解决方案是在等式的两边使用一致且有目的的分隔符。来自mysql:</p><pre class="jr js jt ju fq lv lw lx ly aw lz dt"><span id="b4c8" class="kh ki hu lw b fv ma mb l mc md"><strong class="lw hv">select <br/>  </strong>{{ info.columns }} <br/><strong class="lw hv">from <br/>  </strong>{{ info.table_name }}<br/><strong class="lw hv">INTO </strong>OUTFILE s3 '{{ info.s3_path }}'<strong class="lw hv"><br/></strong>FIELDS TERMINATED <strong class="lw hv">BY ',' </strong>enclosed <strong class="lw hv">by '"' </strong>escaped <strong class="lw hv">by '\\'<br/></strong>LINES TERMINATED <strong class="lw hv">BY '\n'<br/></strong>MANIFEST <strong class="lw hv">ON<br/></strong>OVERWRITE <strong class="lw hv">ON<br/></strong>;</span></pre><p id="65c1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">并将它与红移查询配对</p><pre class="jr js jt ju fq lv lw lx ly aw lz dt"><span id="e9c2" class="kh ki hu lw b fv ma mb l mc md"><strong class="lw hv">TRUNCATE</strong> "{{ info.schema_name }}"."{{ info.table_name }}";<br/><strong class="lw hv">COPY</strong> "{{ info.schema_name }}"."{{ info.table_name }}" <strong class="lw hv">from </strong>'{{ info.s3_path }}.manifest'<strong class="lw hv"><br/></strong>iam_role '{{ PLAID_REDSHIFT_ROLE }}'<strong class="lw hv"><br/>DELIMITER</strong> <strong class="lw hv">','<br/>TRUNCATECOLUMNS<br/>EMPTYASNULL<br/>ACCEPTINVCHARS<br/>IGNOREBLANKLINES<br/>NULL AS '\\N'<br/>MANIFEST<br/>REMOVEQUOTES<br/>STATUPDATE ON<br/>MAXERROR </strong>{{ info.max_error }}<strong class="lw hv"><br/>ESCAPE<br/></strong>;</span></pre><h1 id="8ca2" class="mg ki hu bd kj mh mi mj kn mk ml mm kr mn mo mp ku mq mr ms kx mt mu mv la mw dt translated">新一天的黎明——实习生来了</h1><p id="81e8" class="pw-post-body-paragraph ir is hu it b iu lc iw ix iy ld ja jb jc le je jf jg lf ji jj jk lg jm jn jo hn dt translated">代替硬编码的模式文件，我们希望将配置文件移动到一个持久存储层，我们计划称之为<code class="eh my mz na lw b">dbdb</code>，或数据库-数据库——这是我们从在Square工作过类似系统的工程团队成员那里继承的名字。</p><p id="452f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">大约在迈克尔·特鲁特作为软件工程实习生加入我们这个夏天的时候，我们收到了不要称之为<code class="eh my mz na lw b">dbdb</code>的反馈。他确定了对Plaid的需求，以支持将CSV特别上传到我们的Redshift集群，我们同意将CSV上传、DBDB和其他计划中的ETL改进合并到更广泛的数据仓库管理之下。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff kg"><img src="../Images/c70ecc1f1a267fdd7245958e1bc43f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8Iv1PLLUhgAXF07H"/></div></div></figure><p id="530f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这为我们的开发团队带来了立竿见影的效果。当我们第一次开始使用Airflow时，有几个Dag由一对配置文件管理，但是几个MySQL数据库的增加和用例套件的增加导致团队不得不打开几个pull请求来进行配置更改。相反，将状态存储在数据仓库管理器中使我们能够更容易地修改系统——在从上游数据库添加或删除表和列时自动添加和删除表和列，并添加自定义功能，如设置红移排序和分布键，以及为我们的数据库接收部署更好的方法。</p><p id="5adc" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们的一些表有数十亿行，其中许多表要么是“仅追加”的，要么是“滑动窗口”的更新。对于只追加表，众所周知的“低挂果实”是只从数据库中查询新行。对于其他表，更新的行通常是在过去几天内创建的，因此混合使用完全更新、部分更新和增量更新是有意义的。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff kg"><img src="../Images/19ea9e5ffcd35c6d3e9288d23f6662fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fHWtykoaK8rucWuS"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">Iterative loading, easily controlled maintaining state in the Data Warehouse Manager</figcaption></figure><h1 id="4f63" class="mg ki hu bd kj mh mi mj kn mk ml mm kr mn mo mp ku mq mr ms kx mt mu mv la mw dt translated">替代解决方案</h1><p id="e832" class="pw-post-body-paragraph ir is hu it b iu lc iw ix iy ld ja jb jc le je jf jg lf ji jj jk lg jm jn jo hn dt translated">我们尝试使用现有的解决方案来节省我们的工程工作。这次调查中我们最喜欢的工具是<a class="ae jp" href="https://www.matillion.com/" rel="noopener ugc nofollow" target="_blank"> Matillion </a>，如果你正在寻找一个拖放式的数据管道工具，我肯定会去看看。他们提供了一个Amazon机器映像(AMI ),您可以将它部署在AWS基础设施中，并为这类问题提供强大的支持。最终，我们发现我们的用例需要太多的外部处理，因此Matillion不是我们的正确解决方案。</p><p id="8008" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果你正在寻找云提供商，我们已经成功地使用<a class="ae jp" href="https://www.stitchdata.com/" rel="noopener ugc nofollow" target="_blank">缝合数据</a>用于数据处理管道，<a class="ae jp" href="https://aws.amazon.com/blogs/aws/fast-easy-free-sync-rds-to-redshift/" rel="noopener ugc nofollow" target="_blank">亚马逊</a>的这篇文章推荐了其他供应商，<a class="ae jp" href="https://blog.panoply.io/how-to-move-your-mysql-to-amazon-redshift" rel="noopener ugc nofollow" target="_blank"> Panoply </a>推荐了使用mysqldump的类似方法。</p><h1 id="37f1" class="mg ki hu bd kj mh mi mj kn mk ml mm kr mn mo mp ku mq mr ms kx mt mu mv la mw dt translated">结论</h1><p id="5dd6" class="pw-post-body-paragraph ir is hu it b iu lc iw ix iy ld ja jb jc le je jf jg lf ji jj jk lg jm jn jo hn dt translated">Airflow一直是我们的可靠工具，也是我们内部ETL工作的重要组成部分。Plaid可以处理许多不同的数据源，对于非敏感数据集+第三方数据来说,<a class="ae jp" href="https://www.stitchdata.com/" rel="noopener ugc nofollow" target="_blank"> Stitch </a>和<a class="ae jp" href="https://segment.com/" rel="noopener ugc nofollow" target="_blank"> Segment </a>在构建数据工作流中发挥了重要作用。对于我们在本地安全保护伞下的数据，Airflow已经证明了自己是可靠的、信息丰富的，并且可以被在Plaid上增加数据摄取的团队新成员访问。</p><p id="a92e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">Plaid拥有跨越数十亿行的蓬勃发展的数据生态系统。如果你是一个气流老手或生活和呼吸ETL管道，我们很乐意聊天！我们特别喜欢开源我们的工作:如果你喜欢处理这类问题，或者认为这个解决方案可以帮助你自己的用例，我们很乐意在下面的评论中联系你。</p><figure class="jr js jt ju fq jv"><div class="bz el l di"><div class="nb nc l"/></div></figure></div></div>    
</body>
</html>