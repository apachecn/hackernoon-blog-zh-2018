<html>
<head>
<title>Various Optimisation Techniques and their Impact on Generation of Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">各种优化技术及其对单词嵌入生成的影响</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/various-optimisation-techniques-and-their-impact-on-generation-of-word-embeddings-3480bd7ed54f?source=collection_archive---------13-----------------------#2018-08-06">https://medium.com/hackernoon/various-optimisation-techniques-and-their-impact-on-generation-of-word-embeddings-3480bd7ed54f?source=collection_archive---------13-----------------------#2018-08-06</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/1fc2910512548be2df27aef7258b0eef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RPByKYP_YvHWwqPDoO9fAg.png"/></div></div></figure><p id="6a52" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><em class="ka">无耻插件:我们是一个</em> <a class="ae kb" href="https://dataturks.com" rel="noopener ugc nofollow" target="_blank"> <em class="ka">机器学习数据标注平台</em> </a> <em class="ka">让你超级轻松的</em> <a class="ae kb" href="https://dataturks.com" rel="noopener ugc nofollow" target="_blank"> <em class="ka">构建ML数据集</em> </a> <em class="ka">。只需上传数据，邀请您的团队，快速构建数据集。</em></p><p id="59b4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">欢迎来到关于机器学习及其应用的五个系列教程的第三部分。来看看<a class="ae kb" href="http://dataturks.com" rel="noopener ugc nofollow" target="_blank"><strong class="je hv"><em class="ka">data Turks</em></strong></a>，一个让你的ML生活更简单更顺畅的数据标注工具。</p><p id="9d43" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">单词嵌入</strong>是分配给单词的<strong class="je hv">矢量表示</strong>，它们具有相似的上下文用法。你可能会说单词嵌入有什么用？嗯，如果我在谈论梅西，并立即知道上下文是足球…这是怎么发生的？我们的大脑有联想记忆，我们把梅西和足球联系在一起…</p><p id="fb63" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了达到同样的目的，也就是把相似的词组合起来，我们使用嵌入。嵌入最初是从一种热编码方法开始的，在这种方法中，文本中的每个单词都用一个长度等于词汇表中唯一单词数量的数组来表示。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff kc"><img src="../Images/53331cb416b69fa28ec4b7bb21b53b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieHW2WB8oBZJ-v30bMgQog.png"/></div></div></figure><blockquote class="kh ki kj"><p id="5a7c" class="jc jd ka je b jf jg jh ji jj jk jl jm kk jo jp jq kl js jt ju km jw jx jy jz hn dt translated">例句:句子1:芒果是黄色的。苹果是红色的。</p></blockquote><p id="211d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">独特的单词是{The，mangoes，are，yellow，apples，red}。因此，句子1将表示为[1，1，1，1，0，0] &amp;句子2将表示为[1，0，1，0，1，1]。</p><p id="6abd" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这种方法适用于小数据集，但不适用于非常大的数据集。因此，为此实现了几个n元模型。在本教程中，我们将不探讨这个领域。感兴趣的主题是用于生成单词嵌入的word2vec模型。这涵盖了机器学习的很多概念。我们将学习单隐层神经网络、嵌入和各种优化技术。</p><p id="29b0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">任何机器学习算法都需要三个领域携手合作。它们是分类器的<strong class="je hv">表示</strong>、假设的<strong class="je hv">评估</strong>和模型的<strong class="je hv">优化</strong>以获得更高的精确度。</p><p id="f38d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在<strong class="je hv"> word2vec模型</strong>中，我们有一个大小为N的单一隐藏分层神经网络，用于获得维度N中的单词嵌入。可视化嵌入的方法如下…</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff kn"><img src="../Images/70acedde766255842350113467c63b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sXNXYfAqfLUeiDXPCo130w.png"/></div></div></figure><h2 id="7d6c" class="ko kp hu bd kq kr ks kt ku kv kw kx ky jn kz la lb jr lc ld le jv lf lg lh li dt translated"><strong class="ak">让我们了解一下各种术语… </strong></h2><p id="4d9e" class="pw-post-body-paragraph jc jd hu je b jf lj jh ji jj lk jl jm jn ll jp jq jr lm jt ju jv ln jx jy jz hn dt translated"><strong class="je hv">连续词袋模型(Continuous Bag of Words Model)——CBOW:</strong>由托马斯·米科洛夫(Tomas Mikolov)在其论文中提出，该模型假设每个语境只考虑一个词。因此，给定一个上下文单词，该模型将预测一个目标单词。设词汇量为V</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div class="fe ff lo"><img src="../Images/f52fde4d57816ed96ff2646a8697b671.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*MpdschOuQr6JW7NRR839hg.png"/></div><figcaption class="lp lq fg fe ff lr ls bd b be z ek">CBOW model with only one word in context</figcaption></figure><p id="00ce" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">输入层和输出层之间的权重矩阵可以由V*N矩阵表示。矩阵的每一行代表每个单词的嵌入向量。注意，这种情况下的激活函数是一个l <strong class="je hv">线性函数</strong>。目标函数是在给定输入上下文单词的情况下观察实际输出单词的条件概率。我们需要最大化目标函数，也就是最大化给定上下文的单词预测…简单对！</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div class="fe ff lt"><img src="../Images/3fa4a5daabc80679cdf8693077d30763.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*qey7p82rdzSo2YQ0cR8hrA.png"/></div></figure><p id="64e7" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">CBOW也有一个多单词上下文，其中不是在上下文中有一个单词，而是取某个窗口大小的单词长度的平均值，然后将其作为输入发送到神经网络。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div class="fe ff lu"><img src="../Images/2f1f304cae35b69f97839ac64dfdd5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*0ie8pwQn-Yp61GeM5NjqtA.png"/></div></figure><p id="4c9c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">跳克模型</strong></p><p id="b27e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">Mikolov等人引入的skip gram模型与CBOW模型相反。目标单词现在在输入层，上下文单词在输出层。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div class="fe ff lv"><img src="../Images/78e5aaf33dfe811cc4a65fa3f125d3bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*tmnTV-dCIu2tH9QFUOiXUw.png"/></div></figure><p id="40c6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">目标函数是给定上下文单词的目标单词组中输出单词的概率。W_O，c是第c组输出字中的实际输出字。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div class="fe ff lw"><img src="../Images/47b6f00ff3b5156db9034171bc3fd71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*6xtesN8uvDwLDB9fkq2msA.png"/></div><figcaption class="lp lq fg fe ff lr ls bd b be z ek">Objective function</figcaption></figure><p id="7889" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">Word2vec模型实现了skip-gram，现在…我们来看看代码。Gensim还提供了word2vec更快的实现…我们将查看Word2Vec的源代码。让我们导入所有需要的库和nltk.corpus中可用的数据集。</p><figure class="kd ke kf kg fq iv"><div class="bz el l di"><div class="lx ly l"/></div></figure><p id="8ce0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">让我们对数据集进行预处理，去掉不常用的单词，并将它们标记为UNK记号。</p><figure class="kd ke kf kg fq iv"><div class="bz el l di"><div class="lx ly l"/></div></figure><p id="c6bf" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">实现skip gram模型是下一部分。</p><figure class="kd ke kf kg fq iv"><div class="bz el l di"><div class="lx ly l"/></div></figure><p id="070b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">训练跳格模型导致模型理解语言结构。</p><figure class="kd ke kf kg fq iv"><div class="bz el l di"><div class="lx ly l"/></div></figure><p id="64d9" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">让我们想象一下嵌入。</p><figure class="kd ke kf kg fq iv"><div class="bz el l di"><div class="lx ly l"/></div></figure><p id="7e51" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">优化用于改进所获得的嵌入。让我们回顾一下我们知道并使用的各种技术。由于在介质上输入数学的限制，我建议你通过<a class="ae kb" href="http://ruder.io/optimizing-gradient-descent/" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div class="fe ff lz"><img src="../Images/71734e0a7618f7c2348e4dcf4e1154f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*_YF5R0ifnSmKiXsODDYtYQ.png"/></div><figcaption class="lp lq fg fe ff lr ls bd b be z ek">Results for comparison of various optimisers</figcaption></figure><p id="862e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">因此，我们可以得出结论，RMSProp和Adam这两个最先进的工具并不能很好地在这些模型上工作。另一方面，近端Adagrad和SGD的效果非常好。让我们看看近端Adagrad和SGD的结果。</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ma"><img src="../Images/8e00ff0413d792753f95db2f433ae9c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i7dQ2v2QelEZeFb3fk17eA.png"/></div></div><figcaption class="lp lq fg fe ff lr ls bd b be z ek">Proximal Adaptive Gradient Descent Optimizer</figcaption></figure><p id="5636" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">检查那些经常出现在一起的单词，它们在图片上的位置是否足够近。此外…比较数字的位置…在两个图像中…相应地决定哪一个是更好的！</p><figure class="kd ke kf kg fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ma"><img src="../Images/46f4fba7014e31508b74127819f16570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6o0tP84ySiZW3_Qi5OF7LQ.png"/></div></div><figcaption class="lp lq fg fe ff lr ls bd b be z ek">Stochastic Gradient Descent Optimizer</figcaption></figure><p id="de79" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这是五部分系列中的第三部分……对接下来的两部分感到兴奋……在<em class="ka">lalith@dataturks.com分享你的想法和反馈。</em></p></div></div>    
</body>
</html>