<html>
<head>
<title>The reason behind moving in the direction opposite to the Gradient</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">向与梯度相反的方向移动的原因</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/the-reason-behind-moving-in-the-direction-opposite-to-the-gradient-f9566b95370b?source=collection_archive---------10-----------------------#2018-08-15">https://medium.com/hackernoon/the-reason-behind-moving-in-the-direction-opposite-to-the-gradient-f9566b95370b?source=collection_archive---------10-----------------------#2018-08-15</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="ae02" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在这篇文章中，我将给出一个<a class="ae jp" href="https://hackernoon.com/tagged/mathematical" rel="noopener ugc nofollow" target="_blank">梯度下降的数学</a>直觉，以及在与梯度相反的方向上移动的原因。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff jq"><img src="../Images/702f2db1e5a2a19cff7726267476a666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8wpL_9SdB1rRuN8ll-u5hA.png"/></div></div></figure><p id="d963" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我假设有偏导数、<a class="ae jp" href="https://hackernoon.com/tagged/vector-algebra" rel="noopener ugc nofollow" target="_blank">、向量代数</a>、泰勒级数和神经网络的基本知识。</p><p id="a498" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">所以让我们开始吧。</p><blockquote class="kc kd ke"><p id="a86c" class="ir is kf it b iu iv iw ix iy iz ja jb kg jd je jf kh jh ji jj ki jl jm jn jo hn dt translated">在神经网络中，我们总是需要从我们拥有的数据中学习权重和偏差，以便我们达到损失函数的绝对最小值，因此我们需要有一种原则方法来达到损失函数的绝对最小值。</p></blockquote><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff kj"><img src="../Images/b233edf65fc801d8f62c38c3b3b13894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_MFSOxyvx5xd6FQrRP0oaQ.png"/></div></div><figcaption class="kk kl fg fe ff km kn bd b be z ek">Error Surface</figcaption></figure><p id="adbc" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们的主要目标是导航通过误差表面，以便到达误差小于或接近零的点。</p><p id="7f4d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">让我们假设θ <strong class="it hv"> = [ </strong> w，b <strong class="it hv"> ] </strong>其中w &amp; b分别是权重和偏差。θ是误差曲面上的任意点。从w开始&amp; b是随机初始化的，这是我们的起点。</p><p id="5b85" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">θ <strong class="it hv"> </strong>是参数w和b的向量，使得θ <strong class="it hv"> </strong> ∈ R <strong class="it hv"> </strong>。</p><p id="8535" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">让我们假设δθ<strong class="it hv">=[</strong>δw，δb]其中δw&amp;δb是我们对权重和偏差所做的改变，使得我们朝着减少损失的方向移动，并停留在误差较小的地方。δθ是减少损失方向的向量。</p><p id="45b2" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">δθ是参数δw和δb的向量，使得δθ<strong class="it hv"/>∈R<strong class="it hv"/>。</p><p id="f2b2" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在我们需要从θ移动到θ+δθ，这样我们就朝着损失最小的方向移动。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff ko"><img src="../Images/e400c1dba7962eda7cd40bcf4d04757c.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*fxGeda8COOPWXs2Nts_KAw.png"/></div><figcaption class="kk kl fg fe ff km kn bd b be z ek">Loss Function</figcaption></figure><p id="064c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果我们把δθ<strong class="it hv"/>加到<strong class="it hv"> </strong> θ <strong class="it hv"> </strong>上，我们就得到一个新的矢量。</p><p id="b1b0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">设新向量为<strong class="it hv"> </strong> θ <em class="kf"> new。</em></p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kp"><img src="../Images/6c6b28e5bd94e97d537a69b335571dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*9vrqRZ4nIZGu8GYycpvhGg.png"/></div></figure><p id="4ed1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此从上图可以看出，矢量θ <strong class="it hv"> </strong>是<strong class="it hv"> </strong>向矢量δθ的方向移动。</p><p id="90f0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">但是如果我们不朝着δθ<strong class="it hv"/>大步前进会更好，尽管<strong class="it hv"> </strong>我们有兴趣朝着那个方向前进。如果我们大步前进，我们就有可能错过损失函数的绝对最小值。</p><p id="9adb" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，我们向δθ迈一小步。这由标量“η”决定。</p><p id="9251" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">标量“η”称为学习率。<strong class="it hv"> </strong> η一般小于1。因此，我们将向δθ方向移动，并按η的比例缩小。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kq"><img src="../Images/0046bd3f6ec8b2929313e6f354e41d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*PlFfGUOorp9qN_98fa_SWA.png"/></div></figure><p id="b750" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此θ <em class="kf"> new = </em> θ + η。δθ其中δθ是减少损耗的方向。</p><p id="cae5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">所以我们从θ的随机值开始。然后我们向δθ方向移动，这确保了我们的损耗减少。我们需要以循环的方式来达到全球最低。</p><p id="bd67" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">但是什么是δθ呢？δθ的正确值是多少？</p><p id="0c4b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">坚持住，我们会找到的。</p><p id="bd94" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">所以上面这个问题的答案来自泰勒级数。</p><p id="d5fe" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了简单起见，让我们假设δθ= u。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff kr"><img src="../Images/c27bb7f5f2e453d014c2407966b79c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X1YhQkv_Ez84sSo5YNDV_A.png"/></div></div><figcaption class="kk kl fg fe ff km kn bd b be z ek">Taylor Series</figcaption></figure><p id="2b3a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">泰勒级数告诉我们的是，如果我们在θ的某个值，我们对θ的值做一个小的改变，那么损失函数的新值是什么。</p><p id="0e20" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">L(θ)称为损失函数。</p><p id="835d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这里∇L(θ) = [∂L(θ)/∂w，∂L(θ)/∂b]是梯度向量，它是关于θ分量的偏导数的集合。</p><p id="161e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">η的值通常取为小于1。还有那个η &lt;&lt;&lt;1. So we might as well ignore the higher order terms.</p><p id="e58d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">And we end up with the equation as below.</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff ks"><img src="../Images/5f0e01d392b03c0f341bb5d2774b94fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YmFWZLHaOLZhjQ38D0hNAg.png"/></div></div></figure><p id="86df" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">So we have some value of θ and we want to move away from that direction such that the new loss L(θ+ηu) is less than the old loss L(θ).</p><p id="f7ee" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">So a desired value for “ u ” is obtained when the following condition holds.</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff kt"><img src="../Images/c13f282d5c7d083bf2701123401d08f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ohNyE3z2AMcISWyZ2-a2Ew.png"/></div></div></figure><p id="7ec6" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">This implies,</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff ku"><img src="../Images/7a6348e89d1c97296d3df234db2b5526.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*YX_RdyMdVXXHZB9UWt-bRA.png"/></div></figure><p id="0ebd" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">This condition should hold for the vector u that we are trying to choose so that we can be sure that we have chosen a good value for “ u ”. A good value of “ u ” can be obtained if the loss of the new step is less than the loss of the previous step.</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kv"><img src="../Images/42012c91d18b9daafd90a4ac74b0a52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*IMfuRxecGO2h5_e66K7xJQ.png"/></div></figure><p id="3a84" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">So the range is ± ||u|| || ∇L(θ)|| .This is just the dot product of u and ∇L(θ).</p><p id="0181" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">Let us assume β as the angle between the vector u and the gradient vector ∇L(θ).Then we know that,</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kw"><img src="../Images/44aee8ebf8d9d89b1d9b08c8cccdef69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*C-i77eCvWopv68OfglmnjQ.png"/></div></figure><p id="a5f3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">And assuming k = ||u|| || ∇L(θ)|| the inequalities simplify to,</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff kx"><img src="../Images/23faecf78883d03d0055366003761a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*xQ28WGjKKFtPDaWmvYIunw.png"/></div></figure><p id="b7f4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">We want the difference between the new loss and old loss to be as negative as possible. The more negative it is, the more the loss will decrease. So the dot product of the vector u and ∇L(θ) should be as negative as possible and so should be equal to <strong class="it hv"> -k </strong>。这种情况下，cos(β)的值应等于-1。</p><p id="402d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这是符合条件的https://onlinecourses.nptel.ac.in/noc18_cs41)&lt; 0.</p><p id="72ad" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">The value of cos(β) will be equal to -1 when the angle between the vector u and the gradient is 180°.</p><p id="07aa" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">The direction “u” that we intend to move in should be at 180° with respect to the gradient.</p><p id="4de4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">And this is the reason why need to move in the direction opposite to the Gradient.</p><p id="fb0d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">So what the gradient descent rule tells us is that if we are at a particular value of θ and if we want to move to a new value of θ such that the new loss is less than the current loss then we should move in the direction opposite to the gradient.</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff ky"><img src="../Images/33a4756e22de4c5feb27edd3d16d6c9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r3v-CAWYMaHQemsWnqVq8w.png"/></div></div></figure><p id="961a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">Now these are the parameter update equations. And they follow what we have discussed so far.</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff kz"><img src="../Images/213642e2ea4bfd852c0d8e41136d44a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*njzlK67o0CTO8Yu-l4RX0w.png"/></div></div></figure><p id="a995" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">I guess this article would have given you a mathematical intuition of gradient descent and the reason behind moving in the direction opposite to the gradient. Any feedback will be appreciated.</p><p id="09a0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">Citation:</p><ol class=""><li id="d96e" class="la lb hu it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li dt translated"><a class="ae jp" href="https://onlinecourses.nptel.ac.in/noc18_cs41/unit?unit=10&amp;lesson=38" rel="noopener ugc nofollow" target="_blank"/></li></ol><figure class="jr js jt ju fq jv"><div class="bz el l di"><div class="lj lk l"/></div></figure></div></div>    
</body>
</html>