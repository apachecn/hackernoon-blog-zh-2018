<html>
<head>
<title>Exploding And Vanishing Gradient Problem: Math Behind The Truth</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">爆炸和消失的梯度问题:真相背后的数学</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/exploding-and-vanishing-gradient-problem-math-behind-the-truth-6bd008df6e25?source=collection_archive---------3-----------------------#2018-08-17">https://medium.com/hackernoon/exploding-and-vanishing-gradient-problem-math-behind-the-truth-6bd008df6e25?source=collection_archive---------3-----------------------#2018-08-17</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff hs"><img src="../Images/79ad043a4f0e0daa7520999a72fc2363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vaOqtwi2XYGiIOhyxOOlHw.jpeg"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd ih">Gamma Ray burst</strong>! source: Google</figcaption></figure><div class=""/><p id="e1c2" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">你好，星尘！今天我们将看到爆炸和消失梯度问题背后的数学原因，但首先让我们简单地理解这个问题。</p><p id="cca8" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">通常，当我们使用梯度下降通过反向投影训练深度模型时，我们计算输出w.r.t .到权重矩阵的梯度，然后从相应的权重矩阵中减去它，以使其(矩阵的)值更准确，从而给出正确的输出</p><p id="5502" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jj il"> <em class="kf">但是如果梯度变得可以忽略不计呢？</em> </strong></p><p id="60d1" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">当梯度变得可以忽略时，从原始矩阵中减去它没有任何意义，因此模型停止学习。这个问题被称为消失梯度问题。</p><p id="7354" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们将首先在头脑中想象这个问题。我们将使用具有1、2、4和5个隐藏层的MNIST(你知道这一点)数据集来训练深度学习模型，并查看使用不同架构对输出的影响(准确性并不总是增加！😵).</p><figure class="kh ki kj kk fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff kg"><img src="../Images/e44c335b6d3a2d822b0f035e378c737d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T1iZliJp9yZVb1TlsE-YUw.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd ih">DNN architecture with 3 hidden layers</strong></figcaption></figure><p id="e83f" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">你可以在这里获得完整的代码。对于这篇文章，我只是使用代码的快照。我用过 <strong class="jj il"> <em class="kf">深度学习工作室的Jupyter lab </em> </strong> <em class="kf">来执行代码。如果你不知道这个令人敬畏的深度学习工具，可以看看我关于这个</em>的文章。</p><div class="ht hu fm fo hv kl"><a href="https://towardsdatascience.com/iris-genus-classification-deepcognition-azure-ml-studio-4b930f54435a" rel="noopener follow" target="_blank"><div class="km ab ej"><div class="kn ab ko cl cj kp"><h2 class="bd il fv z el kq eo ep kr er et ij dt translated">鸢尾属分类|DeepCognition| Azure ML studio</h2><div class="ks l"><h3 class="bd b fv z el kq eo ep kr er et ek translated">界:植物界分支:被子植物目:天冬目:鸢尾科亚科:环烯醚萜族:环烯醚萜属:鸢尾</h3></div><div class="kt l"><p class="bd b gc z el kq eo ep kr er et ek translated">towardsdatascience.com</p></div></div><div class="ku l"><div class="kv l kw kx ky ku kz ib kl"/></div></div></a></div><div class="ht hu fm fo hv kl"><a href="https://deepcognition.ai/" rel="noopener  ugc nofollow" target="_blank"><div class="km ab ej"><div class="kn ab ko cl cj kp"><h2 class="bd il fv z el kq eo ep kr er et ij dt translated">主页</h2><div class="ks l"><h3 class="bd b fv z el kq eo ep kr er et ek translated">我们想邀请您在3月26日至29日的GPU技术大会上加入Deep Cognition的团队，展位号为1035…</h3></div><div class="kt l"><p class="bd b gc z el kq eo ep kr er et ek translated">deepcognition.ai</p></div></div><div class="ku l"><div class="la l kw kx ky ku kz ib kl"/></div></div></a></div><blockquote class="lb lc ld"><p id="58ce" class="jh ji kf jj b jk jl jm jn jo jp jq jr le jt ju jv lf jx jy jz lg kb kc kd ke hn dt translated"><strong class="jj il">带1个隐藏层的模型。</strong></p></blockquote><p id="5a22" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">第1行:784表示输入神经元，30表示隐藏层1中的神经元，10表示输出的数量。</p><figure class="kh ki kj kk fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff lh"><img src="../Images/0cbfb693085d28546ac7fc1e4a23da77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IiQ6dElYaW8Hp7iPXFWFNw.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd ih">Accuracy of the model with 1 hidden layer.</strong></figcaption></figure><p id="039d" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="kf">这里的术语‘第I个’隐藏层的权重矩阵的长度</em><strong class="jj il"><em class="kf"/></strong><em class="kf">是第一个隐藏层的权重矩阵的大小。可以认为是特定隐藏层学习特征的速度(大致)。<br/>我们将使用这个术语来比较不同模型的不同隐藏层的速度。</em></p><p id="88e5" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jj il">第一个模型中第一个隐藏层的速度:0.103165(记住这个！)</strong></p><blockquote class="lb lc ld"><p id="0c76" class="jh ji kf jj b jk jl jm jn jo jp jq jr le jt ju jv lf jx jy jz lg kb kc kd ke hn dt translated"><strong class="jj il">有3个隐藏层的模型:</strong></p></blockquote><figure class="kh ki kj kk fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff li"><img src="../Images/76c204701e6ab4ef101f2bed0b55f4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fAHTgJn527zEhbnrn3iKfg.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd ih">DNN with 3 hidden layers.</strong></figcaption></figure><p id="fa50" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">观察结果:</p><ul class=""><li id="8d44" class="lj lk ik jj b jk jl jo jp js ll jw lm ka ln ke lo lp lq lr dt translated">第一个隐藏层的学习速度:0.09983(小于先前模型的第一个隐藏层的速度)。</li><li id="ef21" class="lj lk ik jj b jk ls jo lt js lu jw lv ka lw ke lo lp lq lr dt translated">第I层的学习速度一般大于第(i+1)层。</li></ul><blockquote class="lb lc ld"><p id="e193" class="jh ji kf jj b jk jl jm jn jo jp jq jr le jt ju jv lf jx jy jz lg kb kc kd ke hn dt translated">让我们继续到MNIST的4层和5层</p></blockquote><div class="kh ki kj kk fq ab cb"><figure class="lx hw ly lz ma mb mc paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><img src="../Images/3f2b2003802e3cd0d8cedf0ceee5beb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*DB2Df8LELbip4TqLSwIQYA.png"/></div></figure><figure class="lx hw md lz ma mb mc paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><img src="../Images/eca3845d3507791154a0c44b2ea98cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*9a6Cnrv-SbgrW8T0lE3t4g.png"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek me di mf mg">LEft<strong class="bd ih"> :MNIST with 4 hidden layers, </strong>Right<strong class="bd ih">:MNIST with 5 hidden layers.</strong></figcaption></figure></div><p id="1390" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">第I个隐藏层的学习速度随着我们拥有更多更深的模型(即具有更多隐藏层的模型)而不断降低。</p><p id="5afc" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在5个隐藏层中，我们甚至失去了模型的准确性。</p><blockquote class="lb lc ld"><p id="7414" class="jh ji kf jj b jk jl jm jn jo jp jq jr le jt ju jv lf jx jy jz lg kb kc kd ke hn dt translated"><strong class="jj il">数学上的原因。</strong></p></blockquote><p id="7223" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">考虑一个具有4个隐藏层的神经网络，每个矩阵中有一个神经元。</p><figure class="kh ki kj kk fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mh"><img src="../Images/da0fd77ac6b463a9afbe9e6ec45ff3c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Io6GzudqlSN8AIIA1uNw0A.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd ih">Neural Network</strong></figcaption></figure><p id="e325" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">上述神经网络的计算图为:</p><figure class="kh ki kj kk fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mi"><img src="../Images/5e40a5d2ce216adeacebda1fdcc1e1d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UATNEUQ0dKbvDdI79y3z9A.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd ih">Forward Propagation</strong></figcaption></figure><p id="ec99" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在前向传播中，我们只是将输入与权重矩阵相乘，并添加偏差，如上所示。然后我们找到输出的sigmoid。</p><figure class="kh ki kj kk fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mj"><img src="../Images/d12a95f3b130a2fed1c50a84414999c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aVaJIAm2hKo_6YAWY8Y9wA.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd ih">Backpropagation.</strong></figcaption></figure><p id="581b" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在反向投影过程中，为了使我们的输出更加精确，我们找到了输出相对于不同权重矩阵的导数。假设我们想求C(输出)w.r.t权矩阵(b1)的导数。</p><p id="b2b3" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这将包括的术语有:</p><figure class="kh ki kj kk fq hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff mh"><img src="../Images/da0fd77ac6b463a9afbe9e6ec45ff3c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Io6GzudqlSN8AIIA1uNw0A.png"/></div></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd ih">Neural Network</strong></figcaption></figure><p id="c9b9" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">乙状结肠'(z1)，乙状结肠'(z2)..etc不到1/4。因为sigmoid函数的导数小于1/4。见下文。使用高斯方法将加权矩阵w1、w2、w3、w4初始化为具有平均值0和标准偏差1。因此||w(i)||小于1。因此，在导数中，我们将小于1和1/4的项相乘。因此，将这样的小项相乘大量次，我们得到非常小的梯度，这使得模型几乎停止学习。</p><p id="297d" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">原因在于，如果我们具有比起始隐藏层更深的模型，则学习速度会更低:在反向投影期间，当我们到达起始隐藏层时，我们移动得更深，因此涉及更多这样的项，这使得梯度变小。</p><figure class="kh ki kj kk fq hw fe ff paragraph-image"><div class="fe ff mk"><img src="../Images/f6b7df5dc1c1504c774de26e07da7d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*cBzn6qn0lqcu0FB3SHYWFA.png"/></div><figcaption class="id ie fg fe ff if ig bd b be z ek"><strong class="bd ih">Read it!</strong></figcaption></figure><p id="e049" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">类似于爆炸梯度的情况，如果我们用非常大的值初始化我们的权重矩阵，那么导数将非常大，因此模型将具有非常不稳定的训练。</p><p id="9e4e" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">感谢阅读..伙计们。</p><p id="f3b6" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">如果你觉得这篇文章有帮助👏分享一下。</p><p id="0995" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在<strong class="jj il"> LinkedIn </strong>和<strong class="jj il"> medium </strong>上关注我，订阅<strong class="jj il">我的YouTube频道</strong>:</p><div class="ht hu fm fo hv kl"><a href="https://www.linkedin.com/in/maniksoni/" rel="noopener  ugc nofollow" target="_blank"><div class="km ab ej"><div class="kn ab ko cl cj kp"><h2 class="bd il fv z el kq eo ep kr er et ij dt translated">Manik Soni -机器学习实习生-Ace2three.com印度信息技术有限公司负责人| LinkedIn</h2><div class="ks l"><h3 class="bd b fv z el kq eo ep kr er et ek translated">查看Manik Soni在全球最大的职业社区LinkedIn上的个人资料。Manik有3个工作列在他们的…</h3></div><div class="kt l"><p class="bd b gc z el kq eo ep kr er et ek translated">www.linkedin.com</p></div></div><div class="ku l"><div class="ml l kw kx ky ku kz ib kl"/></div></div></a></div><div class="ht hu fm fo hv kl"><a rel="noopener follow" target="_blank" href="/@maniksoni653"><div class="km ab ej"><div class="kn ab ko cl cj kp"><h2 class="bd il fv z el kq eo ep kr er et ij dt translated">马尼克索尼培养基</h2><div class="ks l"><h3 class="bd b fv z el kq eo ep kr er et ek translated">阅读媒介上的Manik Soni的作品。机器学习研究员。每天，Manik Soni和成千上万的其他人…</h3></div><div class="kt l"><p class="bd b gc z el kq eo ep kr er et ek translated">medium.com</p></div></div><div class="ku l"><div class="mm l kw kx ky ku kz ib kl"/></div></div></a></div><p id="397e" class="pw-post-body-paragraph jh ji ik jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jj il">订阅我的YouTube频道:</strong></p><div class="ht hu fm fo hv kl"><a href="https://www.youtube.com/channel/UCNrS8D0rHKh8k2SU8oDY4Hg?view_as=subscriber?&amp;ab_channel=AIandTheDeepLearner" rel="noopener  ugc nofollow" target="_blank"><div class="km ab ej"><div class="kn ab ko cl cj kp"><h2 class="bd il fv z el kq eo ep kr er et ij dt translated">人工智能与深度学习者:人工智能_日常</h2><div class="kt l"><p class="bd b gc z el kq eo ep kr er et ek translated">www.youtube.com</p></div></div><div class="ku l"><div class="mn l kw kx ky ku kz ib kl"/></div></div></a></div></div></div>    
</body>
</html>