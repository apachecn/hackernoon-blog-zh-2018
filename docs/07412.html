<html>
<head>
<title>Machine Learning Model Pipelines: Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习模型管道:第二部分</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/machine-learning-model-pipelines-part-ii-23ebd1e6b714?source=collection_archive---------12-----------------------#2018-08-31">https://medium.com/hackernoon/machine-learning-model-pipelines-part-ii-23ebd1e6b714?source=collection_archive---------12-----------------------#2018-08-31</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><figure class="ht hu fm fo hv hw fe ff paragraph-image"><div role="button" tabindex="0" class="hx hy di hz bf ia"><div class="fe ff hs"><img src="../Images/66b21c2cad623571cb40cdfdfee493b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PRae-Y3rsmO5t-mbUfpSDA.jpeg"/></div></div></figure><div class=""/><p id="4cd0" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><a class="ae ka" rel="noopener" href="/@vishvananda/machine-learning-model-pipelines-part-i-e138b7a7c1ef">本系列的第一部分</a>使用了一个名为Classy的应用程序的真实示例后端来展示ML模型管道的优势。它解释了将组件排序到管道中的三种架构。</p><p id="d3ef" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这篇文章中，我们探索用<a class="ae ka" href="https://oracle.github.io/graphpipe" rel="noopener ugc nofollow" target="_blank"> GraphPipe </a>和go实现这些管道的本质细节。为了保持代码简单，我们将从用玩具模型实现三个基本架构开始，然后展示第一篇文章中经典后端的完整实现。</p><h2 id="1d30" class="kb kc if bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">基本设置</h2><p id="161d" class="pw-post-body-paragraph jc jd if je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">你可以在<a class="ae ka" href="http://github.com/vishvananda/pipeline" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到所有的代码示例，但是玩具模型的相关代码包含在下面。为了避免您自己构建示例，我还将包含所有客户端和服务器的容器上传到了<a class="ae ka" href="https://hub.docker.com/r/vishvananda/pipeline/" rel="noopener ugc nofollow" target="_blank"> DockerHub </a>。整篇文章中的灰色文本显示了如何使用这些容器运行示例。</p><p id="ce68" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">首先，让我们在go中设置几个非常简单的GraphPipe服务器。我们所有的服务器示例都将使用相同的main函数；只有<code class="eh lb lc ld le b">apply</code>方法不同。以下是我们所有服务器的导入和主要功能:</p><figure class="lf lg lh li fq hw"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="d62f" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><code class="eh lb lc ld le b">gp.Serve()</code>方法反思了我们的apply方法的类型，并为我们将所有的东西粘在一起。我们的服务器接受一个参数，这个参数是监听的端口。第一个服务器将接受一个二维的浮动数组。它会将数组中的每个浮点值乘以2。以下是第一台服务器的应用方法:</p><figure class="lf lg lh li fq hw"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="0e05" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">第二台服务器非常相似，但它将每个值加1，而不是乘以2:</p><figure class="lf lg lh li fq hw"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="bf33" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在，我们可以在端口3000上运行乘法模型，在端口4000上运行加法模型。您可以运行我的容器化服务器，如下所示:</p><pre class="lf lg lh li fq ll le lm ln aw lo dt"><span id="7dda" class="kb kc if le b fv lp lq l lr ls">SRV=multiply<br/>ID1=`docker run -d --net=host --rm vishvananda/pipeline:$SRV 3000`</span><span id="7c1a" class="kb kc if le b fv lt lq l lr ls">SRV=add<br/>ID2=`docker run -d --net=host --rm vishvananda/pipeline:$SRV 4000`</span></pre><h2 id="39d9" class="kb kc if bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">客户端排序</h2><figure class="lf lg lh li fq hw fe ff paragraph-image"><div class="fe ff lu"><img src="../Images/7319948a9c98ec394a0590e9a9665946.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*AGfq4BvwOXoDPqWVtbDbPA.png"/></div></figure><p id="707d" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">第一部分<a class="ae ka" rel="noopener" href="/@vishvananda/machine-learning-model-pipelines-part-i-e138b7a7c1ef">中讨论的第一种排序是客户端排序。正如所料，这将序列逻辑放入了客户端。我们只需让客户端按顺序执行对乘法和加法模型的请求:</a></p><figure class="lf lg lh li fq hw"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="1edb" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">要运行该示例的容器化版本:</p><pre class="lf lg lh li fq ll le lm ln aw lo dt"><span id="00a0" class="kb kc if le b fv lp lq l lr ls">CLIENT=client-sequencing-client<br/>docker run --net=host --rm vishvananda/pipeline:$CLIENT<br/># Output:<br/># [[0 1] [2 3]]<br/># [[0 2] [4 6]]<br/># [[1 3] [5 7]]</span></pre><p id="194b" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">请注意，我们也可以使用python客户端，它会给出相同的结果:</p><figure class="lf lg lh li fq hw"><div class="bz el l di"><div class="lj lk l"/></div></figure><h2 id="529a" class="kb kc if bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">服务器排序</h2><figure class="lf lg lh li fq hw fe ff paragraph-image"><div class="fe ff lv"><img src="../Images/1570e4f05133199e39fba420c7b4ca88.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*pFBOJMmRbg-bodr1ejfGKg.png"/></div></figure><p id="0525" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">讨论的第二个架构是服务器排序。在这个版本中，multiply模型有一个客户端，作为其计算的一部分，它使用该客户端与add模型进行通信。为了说明服务器排序，我们需要制作这个新版本的multiply模型，它调用add。下面是一个新服务器的代码:</p><figure class="lf lg lh li fq hw"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="a838" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">注意，添加模型的位置是硬编码的。我们还需要一个只向第一个模型发出请求的新客户端:</p><figure class="lf lg lh li fq hw"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="5f47" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在，我们可以运行新的multiply服务器来代替旧的服务器，并对其发出请求:</p><pre class="lf lg lh li fq ll le lm ln aw lo dt"><span id="1a09" class="kb kc if le b fv lp lq l lr ls">docker stop $ID1<br/>SRV=server-sequencing-server<br/>CLIENT=server-sequencing-client<br/>ID1=`docker run -d --net=host --rm vishvananda/pipeline:$SRV 3000`<br/>docker run --net=host --rm vishvananda/pipeline:$CLIENT<br/># Output:<br/># [[1 3] [5 7]]</span></pre><h2 id="6fb7" class="kb kc if bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">混合测序</h2><figure class="lf lg lh li fq hw fe ff paragraph-image"><div class="fe ff lw"><img src="../Images/6bf51b39c1afe1f3aac1f1b715971e2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*ZgFj13J9l07VXzM6Sc7elA.png"/></div></figure><p id="d6c5" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们讨论的最后一种测序类型是混合测序。对于混合排序，我们的multiply服务器必须接受config，告诉它下一个模型在哪里。<a class="ae ka" href="https://oracle.github.io/graphpipe/#/guide/user-guide/spec" rel="noopener ugc nofollow" target="_blank"> GraphPipe规范</a>允许我们将任意配置数据传递给模型。对于内部客户端，我们使用对<code class="eh lb lc ld le b">gp.Remote()</code>的嵌入式调用，就像服务器排序一样:</p><figure class="lf lg lh li fq hw"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="b027" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这个版本和服务器排序版本之间唯一的区别是我们将配置值作为uri传递给<code class="eh lb lc ld le b">gp.Remote()</code>,而不是使用硬编码的值。我们修改后的客户端必须传递第二个模型的位置:</p><figure class="lf lg lh li fq hw"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="e9e8" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们的新版本和以前的一样:</p><pre class="lf lg lh li fq ll le lm ln aw lo dt"><span id="fa5c" class="kb kc if le b fv lp lq l lr ls">docker stop $ID1<br/>SRV=hybrid-sequencing-server<br/>CLIENT=hybrid-sequencing-client<br/>ID1=`docker run -d --net=host --rm vishvananda/pipeline:$SRV 3000`<br/>docker run --net=host --rm vishvananda/pipeline:$CLIENT<br/># Output:<br/># [[1 3] [5 7]]</span></pre><h2 id="916e" class="kb kc if bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">实施优等</h2><figure class="lf lg lh li fq hw fe ff paragraph-image"><div class="fe ff lx"><img src="../Images/2bc6b83f68717c300a359919dd15d625.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*IeilMW-Dmuq-L-iu5taU0A.png"/></div></figure><p id="26a5" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在您已经有了用玩具模型实现这些架构的感觉，我们可以开始玩Classy的完整实现了。这个例子的代码在<a class="ae ka" href="http://github.com/vishvananda/pipeline" rel="noopener ugc nofollow" target="_blank">的同一个库</a>中。这个模型使用了三个服务器组件:<a class="ae ka" href="https://github.com/vishvananda/pipeline/tree/master/downloader" rel="noopener ugc nofollow" target="_blank">下载器</a>、<a class="ae ka" href="https://github.com/vishvananda/pipeline/tree/master/preprocessor" rel="noopener ugc nofollow" target="_blank">预处理器</a>和<a class="ae ka" href="https://github.com/vishvananda/pipeline/tree/master/vgg" rel="noopener ugc nofollow" target="_blank"> vgg </a>。vgg容器使用graphpipe-tf模型服务器来服务用tensorflow构建的vgg的实现，但是下载器和预处理器看起来与上面的玩具服务器非常相似。</p><p id="83b2" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">尽管这个实现展示了一个具有日志记录、超时、错误处理和并发性的更复杂的系统，但该架构仍然使用我们已经看到的排序变体:下载器使用混合排序，预处理器使用服务器排序。</p><p id="834d" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">要运行这三个组件，您可以使用DockerHub中的预构建容器:</p><pre class="lf lg lh li fq ll le lm ln aw lo dt"><span id="7210" class="kb kc if le b fv lp lq l lr ls">docker stop $ID1<br/>docker stop $ID2</span><span id="b243" class="kb kc if le b fv lt lq l lr ls"># NOTE: vgg requires 10G+ of RAM. For small machines or VMs,<br/>#       try the squeezenet implementation further down<br/>SRV=vgg<br/>ID1=`docker run -d --net=host --rm vishvananda/pipeline:$SRV`</span><span id="ccf4" class="kb kc if le b fv lt lq l lr ls">SRV=preprocessor<br/>ID2=`docker run -d --net=host --rm vishvananda/pipeline:$SRV`</span><span id="79a4" class="kb kc if le b fv lt lq l lr ls">SRV=downloader<br/>ID3=`docker run -d --net=host --rm vishvananda/pipeline:$SRV`</span></pre><p id="91ea" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了使我们的实现简单，我们的Classy客户端也是一个go应用程序。正如<a class="ae ka" href="https://github.com/vishvananda/pipeline/blob/master/classy/main.go" rel="noopener ugc nofollow" target="_blank">你可以看到的</a>，代码非常短，所以很容易包含在本地移动应用程序中。Classy客户端允许您传入本地图像或URL，并为每个传入的图像返回一个类。要使用它，只需将文件名或URL作为参数传递。注意，客户端不支持混合URL和文件名。以下是发送url的示例:</p><pre class="lf lg lh li fq ll le lm ln aw lo dt"><span id="eb2e" class="kb kc if le b fv lp lq l lr ls">DOMAIN=https://farm8.staticflickr.com<br/>IMAGE=7457/16344626067_1e89d648a6_o_d.jpg<br/>ARGS=$DOMAIN/$IMAGE<br/>docker run --net=host --rm vishvananda/pipeline:classy $ARGS<br/># Output:<br/># image 0 is class 235: German shepherd, German shepherd dog, German<br/># police dog, alsatian</span></pre><p id="80ae" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里是一个本地文件的例子:</p><pre class="lf lg lh li fq ll le lm ln aw lo dt"><span id="a705" class="kb kc if le b fv lp lq l lr ls">wget -nc $DOMAIN/$IMAGE -o shepherd.jpg<br/>ARGS=images/shepherd.jpg<br/>MNT="-v $PWD:/images/"<br/>docker run $MNT --net=host --rm vishvananda/pipeline:classy $ARGS<br/># Output:<br/># image 0 is class 235: German shepherd, German shepherd dog, German<br/># police dog, alsatian</span></pre><p id="a6ba" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了展示部署的灵活性，我们可以通过更改模型服务器和预处理器，用squeezenet实现替换我们的vgg实现:</p><pre class="lf lg lh li fq ll le lm ln aw lo dt"><span id="516a" class="kb kc if le b fv lp lq l lr ls">docker stop $ID1<br/>docker stop $ID2</span><span id="f0ca" class="kb kc if le b fv lt lq l lr ls">SRV=squeeze<br/>ID1=`docker run -d --net=host --rm vishvananda/pipeline:$SRV`</span><span id="9b94" class="kb kc if le b fv lt lq l lr ls">SRV=preprocessor-squeeze<br/>ID2=`docker run -d --net=host --rm vishvananda/pipeline:$SRV`</span><span id="6979" class="kb kc if le b fv lt lq l lr ls">docker run $MNT --net=host --rm vishvananda/pipeline:$CLIENT<br/># Output:<br/># image 0 is class 235: German shepherd, German shepherd dog, German<br/># police dog, alsatian</span></pre><p id="71a6" class="pw-post-body-paragraph jc jd if je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如您所见，无论是下载器还是客户端实现都不需要改变来支持我们的新模型。我们的预处理器抽象使得客户端不必关心squeezenet使用通道优先数据排序期望不同的输入大小；它只是发送原始的jpg或URL。</p><h2 id="9e25" class="kb kc if bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv dt translated">结论</h2><p id="72dc" class="pw-post-body-paragraph jc jd if je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hn dt translated">本文通过用<a class="ae ka" href="https://oracle.github.io/graphpipe" rel="noopener ugc nofollow" target="_blank"> GraphPipe </a>实现机器模型管道，使其理论具体化。当构建人工智能系统时，可重用的计算块是一个有价值的工具。随着越来越多的模型使用一致的接口进行部署，我们希望看到更复杂的管道出现。开始构建您自己的管道！</p></div></div>    
</body>
</html>