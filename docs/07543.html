<html>
<head>
<title>ML for Diabetes from Bangladesh</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">孟加拉国的糖尿病ML</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/ml-for-diabetes-from-bangladesh-d99d1d058d82?source=collection_archive---------25-----------------------#2018-09-04">https://medium.com/hackernoon/ml-for-diabetes-from-bangladesh-d99d1d058d82?source=collection_archive---------25-----------------------#2018-09-04</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="a72e" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">“你可以带着糖尿病生活。这不是最糟糕的事情，但你必须管理自己，并有一些自我控制。”</h2></div><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff jj"><img src="../Images/dee7fb2232dae0cdc5ef386c2c6cf730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1MdREZ18EtOHK4QD3w5Umg.gif"/></div></div></figure><h1 id="c789" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐代码库、样式和链接</h1><p id="b646" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated">有用的<code class="eh lj lk ll lm b">LINKS</code>:</p><p id="75d7" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">📡<a class="ae ls" href="https://www.youtube.com/watch?v=pN4HqWRybwk" rel="noopener ugc nofollow" target="_blank">皮马印第安人和糖尿病视频</a></p><p id="3359" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">📡<a class="ae ls" href="https://pdfs.semanticscholar.org/ef31/2e378325707b371c4727f6b1f9225fc03a9f.pdf" rel="noopener ugc nofollow" target="_blank">皮马印第安人和糖尿病研究论文</a></p><p id="25da" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">📡<a class="ae ls" href="https://pandas.pydata.org/pandas-docs/stable/visualization.html" rel="noopener ugc nofollow" target="_blank">熊猫可视化</a></p><p id="b6da" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">📡<a class="ae ls" href="https://pandas.pydata.org/pandas-docs/stable/style.html" rel="noopener ugc nofollow" target="_blank">熊猫造型</a></p><h1 id="c2c0" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">📘皮马印第安人与糖尿病简介</h1><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff lt"><img src="../Images/a55ea218dc75be079dbe2c65745fea78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXCcU_ZwxHv0b0W99hoiwQ.png"/></div></div></figure><p id="45e6" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated"><a class="ae ls" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank"> <strong class="kp hv"> <em class="lu">糖尿病</em> </strong> </a> <em class="lu">，是一组长期存在高血糖水平的代谢紊乱。高血糖的症状包括尿频、口渴和饥饿。如果不治疗，糖尿病会导致许多并发症。急性并发症包括糖尿病酮症酸中毒、高渗性高血糖状态或死亡。严重的长期并发症包括心血管疾病、中风、慢性肾病、足部溃疡和眼睛损伤。</em></p><p id="cff8" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated"><em class="lu">该数据集最初来自国家糖尿病、消化和肾脏疾病研究所。数据集的目的是基于数据集中包含的某些诊断测量结果，诊断性地预测患者是否患有糖尿病。从一个较大的数据库中选择这些实例有几个限制。特别是，这里的所有患者都是至少21岁的皮马印第安血统的女性。</em></p><h1 id="bf5b" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">📘目标</h1><p id="37d7" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated">我们将尝试建立一个机器学习模型，以准确预测数据集中的患者是否患有糖尿病？</p><h1 id="a971" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐数据</h1><p id="8b3b" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated">数据集包括几个医学预测变量和一个目标变量，结果。预测变量包括患者的怀孕次数、身体质量指数、胰岛素水平、年龄等。</p><ul class=""><li id="8cd8" class="lv lw hu kp b kq ln kt lo kw lx la ly le lz li ma mb mc md dt translated">怀孕次数:怀孕次数</li><li id="9943" class="lv lw hu kp b kq me kt mf kw mg la mh le mi li ma mb mc md dt translated">葡萄糖:口服葡萄糖耐量试验中2小时的血浆葡萄糖浓度</li><li id="449a" class="lv lw hu kp b kq me kt mf kw mg la mh le mi li ma mb mc md dt translated">血压:舒张压(毫米汞柱)</li><li id="76be" class="lv lw hu kp b kq me kt mf kw mg la mh le mi li ma mb mc md dt translated">皮肤厚度:三头肌皮褶厚度(毫米)</li><li id="d5ad" class="lv lw hu kp b kq me kt mf kw mg la mh le mi li ma mb mc md dt translated">胰岛素:2小时血清胰岛素(μU/ml)</li><li id="9fde" class="lv lw hu kp b kq me kt mf kw mg la mh le mi li ma mb mc md dt translated">身体质量指数:体重指数(体重公斤/(身高米) )</li><li id="dbe4" class="lv lw hu kp b kq me kt mf kw mg la mh le mi li ma mb mc md dt translated">糖尿病谱系功能:糖尿病谱系功能</li><li id="f5f3" class="lv lw hu kp b kq me kt mf kw mg la mh le mi li ma mb mc md dt translated">年龄:年龄(岁)</li><li id="bdcd" class="lv lw hu kp b kq me kt mf kw mg la mh le mi li ma mb mc md dt translated">结果:类变量(0或1)</li></ul><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="7a34" class="mn jw hu lm b fv mo mp l mq mr">%%html<br/>&lt;style&gt; <br/>@import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');<br/>body {background-color: gainsboro;} <br/>a {color: #37c9e1; font-family: 'Roboto';} <br/>h1 {color: #37c9e1; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;} <br/>h2, h3 {color: slategray; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;}<br/>h4 {color: #818286; font-family: 'Roboto';}<br/>span {text-shadow: 4px 4px 4px #aaa;}<br/>div.output_prompt, div.output_area pre {color: slategray;}<br/>div.input_prompt, div.output_subarea {color: #37c9e1;}      <br/>div.output_stderr pre {background-color: gainsboro;}  <br/>div.output_stderr {background-color: slategrey;}       <br/>&lt;/style&gt;</span></pre><h1 id="f49d" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐加载库</h1><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="40a0" class="mn jw hu lm b fv mo mp l mq mr">import numpy as np <br/>import pandas as pd <br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import itertools<br/>plt.style.use('fivethirtyeight')<br/><br/>import warnings<br/>warnings.filterwarnings("ignore", category=<strong class="lm hv">UserWarning</strong>)</span><span id="0c2e" class="mn jw hu lm b fv ms mp l mq mr">style_dict = {'background-color':'slategray',<br/>              'color':'#37c9e1',<br/>              'border-color': 'white',<br/>              'font-family':'Roboto'}</span></pre><h1 id="28b2" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐加载数据</h1><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="c83c" class="mn jw hu lm b fv mo mp l mq mr">diabetes = pd.read_csv('../input/diabetes.csv')<br/>print(diabetes.columns)</span><span id="0326" class="mn jw hu lm b fv ms mp l mq mr">diabetes.head().style.set_properties(**style_dict)</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff mt"><img src="../Images/469f3bcfc0e38a9e568c6f1852b4f993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ginv6g1ELN_JXMai8IpwDQ.png"/></div></div><figcaption class="mu mv fg fe ff mw mx bd b be z ek">Dataset Head output</figcaption></figure><h2 id="e28d" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐糖尿病数据集由768个数据点组成，每个数据点有9个特征</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="79b4" class="mn jw hu lm b fv mo mp l mq mr">print("dimension of diabetes data: <strong class="lm hv">{}</strong>".format(diabetes.shape))</span><span id="d1d9" class="mn jw hu lm b fv ms mp l mq mr">output:<br/>dimension of diabetes data: (768, 9)</span></pre><h2 id="8800" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐结果0表示没有糖尿病，结果1表示有糖尿病</h2><p id="6f7e" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated">在这768个数据点中，500个被标记为0，268个被标记为1:</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="087a" class="mn jw hu lm b fv mo mp l mq mr">print(diabetes.groupby('Outcome').size())</span><span id="70ea" class="mn jw hu lm b fv ms mp l mq mr">Outcome<br/>0    500<br/>1    268<br/>dtype: int64</span></pre><h1 id="ff78" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐基础EDA</h1><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="26cb" class="mn jw hu lm b fv mo mp l mq mr">import seaborn as sns<br/>sns.countplot(diabetes['Outcome'],label="Count")</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nl"><img src="../Images/4d1b496b61fbb2f14231755b3f999533.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*wSSy6lN3gj7ZSfwjM_lFxA.png"/></div></figure><h1 id="5a93" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐数据的简要分析</h1><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="fa89" class="mn jw hu lm b fv mo mp l mq mr">diabetes.info()</span><span id="6d21" class="mn jw hu lm b fv ms mp l mq mr">#Output:<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 768 entries, 0 to 767<br/>Data columns (total 9 columns):<br/>Pregnancies                 768 non-null int64<br/>Glucose                     768 non-null int64<br/>BloodPressure               768 non-null int64<br/>SkinThickness               768 non-null int64<br/>Insulin                     768 non-null int64<br/>BMI                         768 non-null float64<br/>DiabetesPedigreeFunction    768 non-null float64<br/>Age                         768 non-null int64<br/>Outcome                     768 non-null int64<br/>dtypes: float64(2), int64(7)<br/>memory usage: 54.1 KB</span></pre><p id="071b" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">现在显示情节:</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="29b8" class="mn jw hu lm b fv mo mp l mq mr">columns=diabetes.columns[:8]<br/>plt.subplots(figsize=(18,15))<br/>length=len(columns)<br/>for i,j <strong class="lm hv">in</strong> itertools.zip_longest(columns,range(length)):<br/>    plt.subplot((length/2),3,j+1)<br/>    plt.subplots_adjust(wspace=0.2,hspace=0.5)<br/>    diabetes[i].hist(bins=20,edgecolor='black')<br/>    plt.title(i)<br/>plt.show()</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nm"><img src="../Images/3d3dcaab87734a1675fecee7326e1368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DB-jWzrr7XXl-MA7jUF1Ow.png"/></div></div></figure><h1 id="ce2c" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐配对图:</h1><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="7b6f" class="mn jw hu lm b fv mo mp l mq mr">sns.pairplot(data=diabetes,hue='Outcome',diag_kind='kde')<br/>plt.show()</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nn"><img src="../Images/ee69b9694efad3810d5db4dc5a9e64e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GXWO1UioONBLBUoiNZ6Lkw.png"/></div></div></figure><h2 id="2477" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐观察结果:</h2><p id="5197" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated">1)对角线显示了具有核密度图的数据集的分布。</p><p id="cf00" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">2)散点图显示了成对获取的每个属性或特征之间的关系。查看散点图，我们可以说没有两个属性能够清楚地区分两种结果</p><h1 id="9ade" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐可视化的预测ML建模</h1><h1 id="8b19" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐1.k-最近邻</h1><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff no"><img src="../Images/881f5f5f6688d2c407da41aa49cc7960.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5XoNUF49WVftSlsuG4xjZg.png"/></div></div></figure><p id="68f3" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">k-NN算法可以说是最简单的机器学习算法。构建模型只包括存储训练数据集。为了对新的数据点进行预测，该算法会在训练数据集中查找最近的数据点，即它的“最近邻居”</p><h2 id="c9b3" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐让我们调查一下我们是否能确认模型复杂性和准确性之间的联系</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="303a" class="mn jw hu lm b fv mo mp l mq mr">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(diabetes.loc[:, diabetes.columns != 'Outcome'], <br/>                                                    diabetes['Outcome'], stratify=diabetes['Outcome'], random_state=66)</span></pre><h2 id="89f3" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐模型的可视化和准确性</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="715f" class="mn jw hu lm b fv mo mp l mq mr">from sklearn.neighbors import KNeighborsClassifier<br/>training_accuracy = []<br/>test_accuracy = []<br/><em class="lu"># try n_neighbors from 1 to 10</em><br/>neighbors_settings = range(1, 11)<br/>for n_neighbors <strong class="lm hv">in</strong> neighbors_settings:<br/>    <em class="lu"># build the model</em><br/>    knn = KNeighborsClassifier(n_neighbors=n_neighbors)<br/>    knn.fit(X_train, y_train)<br/>    <em class="lu"># record training set accuracy</em><br/>    training_accuracy.append(knn.score(X_train, y_train))<br/>    <em class="lu"># record test set accuracy</em><br/>    test_accuracy.append(knn.score(X_test, y_test))<br/><br/>plt.plot(neighbors_settings, training_accuracy, label="training accuracy")<br/>plt.plot(neighbors_settings, test_accuracy, label="test accuracy")<br/>plt.ylabel("Accuracy")<br/>plt.xlabel("n_neighbors")<br/>plt.legend()</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff np"><img src="../Images/2f553a5d487d2a50e30c3e530d4e039b.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*3M2zWaVBqGCNFoIOK6gk6w.png"/></div></figure><p id="e43e" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">该图显示了y轴上的训练和测试集精度与x轴上的n_neighbors设置的对比。</p><p id="4680" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">上面的情节建议我们选择n_neighbors=9。我们到了:</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="b7cd" class="mn jw hu lm b fv mo mp l mq mr">knn = KNeighborsClassifier(n_neighbors=9)<br/>knn.fit(X_train, y_train)<br/><br/>print('Accuracy of K-NN classifier on training set: <strong class="lm hv">{:.2f}</strong>'.format(knn.score(X_train, y_train)))<br/>print('Accuracy of K-NN classifier on test set: <strong class="lm hv">{:.2f}</strong>'.format(knn.score(X_test, y_test)))</span></pre><h2 id="b7cc" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="a0c7" class="mn jw hu lm b fv mo mp l mq mr">Accuracy of K-NN classifier on training set: 0.79<br/>Accuracy of K-NN classifier on test set: 0.78</span></pre><h1 id="2e97" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐2.逻辑回归</h1><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff lt"><img src="../Images/02a9a4408f3f086c825c3828bffc668d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m41jXPwqFn4GKxarE0rmfg.jpeg"/></div></div></figure><p id="409a" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">🌐<strong class="kp hv">逻辑回归是最常见的分类算法之一。</strong></p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="1f15" class="mn jw hu lm b fv mo mp l mq mr">from sklearn.linear_model import LogisticRegression<br/><br/>logreg = LogisticRegression().fit(X_train, y_train)<br/>print("Training set accuracy: <strong class="lm hv">{:.3f}</strong>".format(logreg.score(X_train, y_train)))<br/>print("Test set accuracy: <strong class="lm hv">{:.3f}</strong>".format(logreg.score(X_test, y_test)))</span></pre><h2 id="e2e7" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="233b" class="mn jw hu lm b fv mo mp l mq mr">Training set accuracy: 0.781<br/>Test set accuracy: 0.771</span></pre><p id="7398" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">🌐<strong class="kp hv">C = 1的默认值在训练集上提供了78%的准确率，在测试集上提供了77%的准确率。</strong></p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="a95b" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">logreg1 </strong>=<strong class="lm hv"> LogisticRegression(C</strong>=<strong class="lm hv">0.01)</strong>.<strong class="lm hv">fit(X_train, y_train)<br/>print("Training set accuracy: {:.3f}"</strong>.<strong class="lm hv">format(logreg1</strong>.<strong class="lm hv">score(X_train, y_train)))<br/>print("Test set accuracy: {:.3f}"</strong>.<strong class="lm hv">format(logreg1</strong>.<strong class="lm hv">score(X_test, y_test)))</strong></span></pre><h2 id="d6af" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="f818" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Training set accuracy: 0.700<br/>Test set accuracy: 0.703</strong></span></pre><p id="0cb3" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated"><strong class="kp hv">🌐使用C=0.01会导致训练集和测试集的精确度都较低。</strong></p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="677f" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">logreg100 </strong>=<strong class="lm hv"> LogisticRegression(C</strong>=<strong class="lm hv">100)</strong>.<strong class="lm hv">fit(X_train, y_train)<br/>print("Training set accuracy: {:.3f}"</strong>.<strong class="lm hv">format(logreg100</strong>.<strong class="lm hv">score(X_train, y_train)))<br/>print("Test set accuracy: {:.3f}"</strong>.<strong class="lm hv">format(logreg100</strong>.<strong class="lm hv">score(X_test, y_test)))</strong></span></pre><h2 id="a42c" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="2cc9" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Training set accuracy: 0.785<br/>Test set accuracy: 0.766</strong></span></pre><h1 id="a937" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐逻辑回归的可视化</h1><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="02a6" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">diabetes_features </strong>=<strong class="lm hv"> [x </strong>for<strong class="lm hv"> i,x in enumerate(diabetes</strong>.<strong class="lm hv">columns) </strong>if<strong class="lm hv"> i</strong>!=<strong class="lm hv">8]<br/>plt</strong>.<strong class="lm hv">figure(figsize</strong>=<strong class="lm hv">(8,6))<br/>plt</strong>.<strong class="lm hv">plot(logreg</strong>.<strong class="lm hv">coef_</strong>.<strong class="lm hv">T, 'o', label</strong>=<strong class="lm hv">"C=1")<br/>plt</strong>.<strong class="lm hv">plot(logreg100</strong>.<strong class="lm hv">coef_</strong>.<strong class="lm hv">T, '^', label</strong>=<strong class="lm hv">"C=100")<br/>plt</strong>.<strong class="lm hv">plot(logreg1</strong>.<strong class="lm hv">coef_</strong>.<strong class="lm hv">T, 'v', label</strong>=<strong class="lm hv">"C=0.001")<br/>plt</strong>.<strong class="lm hv">xticks(range(diabetes</strong>.<strong class="lm hv">shape[1]), diabetes_features, rotation</strong>=<strong class="lm hv">90)<br/>plt</strong>.<strong class="lm hv">hlines(0, 0, diabetes</strong>.<strong class="lm hv">shape[1])<br/>plt</strong>.<strong class="lm hv">ylim(</strong>-<strong class="lm hv">5, 5)<br/>plt</strong>.<strong class="lm hv">xlabel("Feature")<br/>plt</strong>.<strong class="lm hv">ylabel("Coefficient magnitude")<br/>plt</strong>.<strong class="lm hv">legend()</strong></span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nq"><img src="../Images/744d2f7bda89a844e40853016439cbfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*mmrr-UncOd196POXb8eIow.png"/></div></figure><h1 id="8de7" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐3.决策图表</h1><p id="f3ce" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated"><strong class="kp hv">决策树</strong>是一种决策支持工具，它使用决策及其可能结果的树状图形或模型，包括偶然事件结果、资源成本和效用。这是显示只包含条件控制语句的算法的一种方式。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nr"><img src="../Images/1dfc32017940021f82255f9435079cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QO_IQoBjY4AZNXvGz80UlA.png"/></div></div></figure><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="5445" class="mn jw hu lm b fv mo mp l mq mr">from<strong class="lm hv"> </strong>sklearn.tree<strong class="lm hv"> </strong>import<strong class="lm hv"> DecisionTreeClassifier<br/><br/>tree </strong>=<strong class="lm hv"> DecisionTreeClassifier(random_state</strong>=<strong class="lm hv">0)<br/>tree</strong>.<strong class="lm hv">fit(X_train, y_train)<br/>print("Accuracy on training set: {:.3f}"</strong>.<strong class="lm hv">format(tree</strong>.<strong class="lm hv">score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}"</strong>.<strong class="lm hv">format(tree</strong>.<strong class="lm hv">score(X_test, y_test)))</strong></span></pre><h2 id="6875" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="1ab7" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Accuracy on training set: 1.000<br/>Accuracy on test set: 0.714</strong></span></pre><h2 id="3908" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐观察结果:</h2><p id="8705" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated">训练集的准确率为100%，而测试集的准确率要差得多。这表明该树过拟合，不能很好地推广到新数据。因此，我们需要对树进行预修剪。</p><p id="240d" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">我们设置max_depth=3，限制树的深度可以减少过度拟合。这导致训练集的精确度较低，但测试集的精确度有所提高。</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="4424" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">tree </strong>=<strong class="lm hv"> DecisionTreeClassifier(max_depth</strong>=<strong class="lm hv">3, random_state</strong>=<strong class="lm hv">0)<br/>tree</strong>.<strong class="lm hv">fit(X_train, y_train)<br/>print("Accuracy on training set: {:.3f}"</strong>.<strong class="lm hv">format(tree</strong>.<strong class="lm hv">score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}"</strong>.<strong class="lm hv">format(tree</strong>.<strong class="lm hv">score(X_test, y_test)))</strong></span></pre><h2 id="7117" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="6112" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Accuracy on training set: 0.773<br/>Accuracy on test set: 0.740</strong></span></pre><h2 id="9365" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐决策树中的特征重要性</h2><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff ns"><img src="../Images/2debe26fdf3de58778b42e3ccb9d7fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*VsgyaZELUjodv0SRhXjgGg.jpeg"/></div></figure><p id="bc70" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">特征重要性评定每个特征对于树所做决策的重要性。对于每个特性，它是一个介于0和1之间的数字，其中0表示“根本没有使用”，1表示“完美地预测了目标”特征重要性的总和总是1:</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="b7ff" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">print("Feature importances:\n{}"</strong>.<strong class="lm hv">format(tree</strong>.<strong class="lm hv">feature_importances_))</strong></span></pre><h2 id="6bb5" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="ccd4" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Feature importances:<br/>[0.04554275 0.6830362  0.         0.         0.         0.27142106<br/> 0.         0.        ]</strong></span></pre><h2 id="46e8" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐特征重要性的可视化</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="78e7" class="mn jw hu lm b fv mo mp l mq mr">def<strong class="lm hv"> plot_feature_importances_diabetes(model):<br/>    plt</strong>.<strong class="lm hv">figure(figsize</strong>=<strong class="lm hv">(8,6))<br/>    n_features </strong>=<strong class="lm hv"> 8<br/>    plt</strong>.<strong class="lm hv">barh(range(n_features), model</strong>.<strong class="lm hv">feature_importances_, align</strong>=<strong class="lm hv">'center')<br/>    plt</strong>.<strong class="lm hv">yticks(np</strong>.<strong class="lm hv">arange(n_features), diabetes_features)<br/>    plt</strong>.<strong class="lm hv">xlabel("Feature importance")<br/>    plt</strong>.<strong class="lm hv">ylabel("Feature")<br/>    plt</strong>.<strong class="lm hv">ylim(</strong>-<strong class="lm hv">1, n_features)<br/><br/>plot_feature_importances_diabetes(tree)</strong></span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nt"><img src="../Images/4db9b5619ba21a644157180fb404cf4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fIYX11jeCvh21Y3mZ0EibQ.png"/></div></div></figure><p id="55e0" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">🌐观察:特性“葡萄糖”是迄今为止最重要的特性。</p><h1 id="61a9" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐4.随机森林</h1><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nu"><img src="../Images/c6c91e7e8f718dd93d6472cad98fb80e.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*eCl-AfClzKoCDrsww73bng.png"/></div></figure><p id="49c0" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">随机森林是一种灵活、易于使用的机器学习算法，即使没有超参数调整，在大多数情况下也能产生很好的结果。它也是最常用的算法之一，因为它简单，而且可以用于分类和回归任务。</p><p id="3ccd" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated"><strong class="kp hv">让我们在糖尿病数据集上应用一个由100棵树组成的随机森林:</strong></p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="f888" class="mn jw hu lm b fv mo mp l mq mr">from<strong class="lm hv"> </strong>sklearn.ensemble<strong class="lm hv"> </strong>import<strong class="lm hv"> RandomForestClassifier<br/>rf </strong>=<strong class="lm hv"> RandomForestClassifier(n_estimators</strong>=<strong class="lm hv">100, random_state</strong>=<strong class="lm hv">0)<br/>rf</strong>.<strong class="lm hv">fit(X_train, y_train)<br/>print("Accuracy on training set: {:.3f}"</strong>.<strong class="lm hv">format(rf</strong>.<strong class="lm hv">score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}"</strong>.<strong class="lm hv">format(rf</strong>.<strong class="lm hv">score(X_test, y_test)))</strong></span></pre><h2 id="3d0d" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="55b6" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Accuracy on training set: 1.000<br/>Accuracy on test set: 0.786</strong></span></pre><p id="fdc8" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">观察:随机森林给了我们78.6%的准确率，比逻辑回归模型或单一决策树要好。</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="2e6b" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">rf1 </strong>=<strong class="lm hv"> RandomForestClassifier(max_depth</strong>=<strong class="lm hv">3, n_estimators</strong>=<strong class="lm hv">100, random_state</strong>=<strong class="lm hv">0)<br/>rf1</strong>.<strong class="lm hv">fit(X_train, y_train)<br/>print("Accuracy on training set: {:.3f}"</strong>.<strong class="lm hv">format(rf1</strong>.<strong class="lm hv">score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}"</strong>.<strong class="lm hv">format(rf1</strong>.<strong class="lm hv">score(X_test, y_test)))</strong></span></pre><h2 id="a8fa" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="162b" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Accuracy on training set: 0.800<br/>Accuracy on test set: 0.755</strong></span></pre><h2 id="04ba" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐随机森林中特征重要性的可视化</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="9d97" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">plot_feature_importances_diabetes(rf)</strong></span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nv"><img src="../Images/95ea81d4c3ee483db0cd239e80d6a3ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yvPrPosC6xg5WNSRMXGrXw.png"/></div></div></figure><p id="39be" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated"><strong class="kp hv">🌐观察:</strong>与单一决策树类似，随机森林也非常重视“葡萄糖”特征，但它也选择“身体质量指数”作为第二大信息特征。构建随机森林的随机性迫使算法考虑许多可能的解释，结果是随机森林比单棵树捕捉到更广泛的数据图像。</p><h1 id="6cd7" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐5.梯度推进</h1><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nw"><img src="../Images/de8683028b374c13a17604ce7254666f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9njQKrRdwxa4gPNWGPqgMA.png"/></div></div></figure><p id="932c" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated"><strong class="kp hv">梯度推进</strong>是一种用于回归和分类问题的机器学习技术，它以弱预测模型集合的形式产生预测模型，通常是决策树。它像其他boosting方法一样以分阶段的方式构建模型，并通过允许优化任意可微分损失函数来概括它们。</p><p id="c101" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">让我们应用梯度增强:</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="84a8" class="mn jw hu lm b fv mo mp l mq mr">from<strong class="lm hv"> </strong>sklearn.ensemble<strong class="lm hv"> </strong>import<strong class="lm hv"> GradientBoostingClassifier<br/>gb </strong>=<strong class="lm hv"> GradientBoostingClassifier(random_state</strong>=<strong class="lm hv">0)<br/>gb</strong>.<strong class="lm hv">fit(X_train, y_train)<br/>print("Accuracy on training set: {:.3f}"</strong>.<strong class="lm hv">format(gb</strong>.<strong class="lm hv">score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}"</strong>.<strong class="lm hv">format(gb</strong>.<strong class="lm hv">score(X_test, y_test)))</strong></span></pre><h2 id="9d45" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="5592" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Accuracy on training set: 0.917<br/>Accuracy on test set: 0.792</strong></span></pre><p id="578f" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">再次使用<strong class="kp hv">最大深度</strong> = <strong class="kp hv"> 1: </strong>应用梯度推进</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="4bf6" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">gb1 </strong>=<strong class="lm hv"> GradientBoostingClassifier(random_state</strong>=<strong class="lm hv">0, max_depth</strong>=<strong class="lm hv">1)<br/>gb1</strong>.<strong class="lm hv">fit(X_train, y_train)<br/>print("Accuracy on training set: {:.3f}"</strong>.<strong class="lm hv">format(gb1</strong>.<strong class="lm hv">score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}"</strong>.<strong class="lm hv">format(gb1</strong>.<strong class="lm hv">score(X_test, y_test)))</strong></span></pre><h2 id="17a0" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="e897" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Accuracy on training set: 0.804<br/>Accuracy on test set: 0.781</strong></span></pre><p id="c538" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">再次使用<strong class="kp hv">learning _ rate</strong>=<strong class="kp hv">0.01:</strong>应用梯度增强</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="a2cf" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">gb2 </strong>=<strong class="lm hv"> GradientBoostingClassifier(random_state</strong>=<strong class="lm hv">0, learning_rate</strong>=<strong class="lm hv">0.01)<br/>gb2</strong>.<strong class="lm hv">fit(X_train, y_train)<br/>print("Accuracy on training set: {:.3f}"</strong>.<strong class="lm hv">format(gb2</strong>.<strong class="lm hv">score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}"</strong>.<strong class="lm hv">format(gb2</strong>.<strong class="lm hv">score(X_test, y_test)))</strong></span></pre><h2 id="8ee4" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="88dc" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Accuracy on training set: 0.802<br/>Accuracy on test set: 0.776</strong></span></pre><p id="9501" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">🌐观察:降低模型复杂性的两种方法都降低了训练集的准确性，正如预期的那样。在这种情况下，这些方法都没有提高测试集的泛化性能。</p><h2 id="8e72" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐梯度增强中特征重要性的可视化</h2><p id="6531" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated">简短说明:即使我们对模型并不满意，我们也可以可视化特性的重要性，以便更深入地了解我们的模型。</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="584b" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">plot_feature_importances_diabetes(gb1)</strong></span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nx"><img src="../Images/630ead2ead7ab63d69f37b056c021f6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m0wH6KJkg0k09fiu-aJskg.png"/></div></div></figure><h1 id="0a82" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐可视化预测深度学习建模</h1><h1 id="a527" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐神经网络</h1><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff ny"><img src="../Images/a46b7b571daf48b0c808959b4948235a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3yzi7qXglRRGcq1XKKvEVw.jpeg"/></div></div></figure><p id="e277" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">神经网络使用大脑的处理作为基础来开发可用于模拟复杂模式和预测问题的算法。</p><p id="5085" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">让我们开始申请:</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="645d" class="mn jw hu lm b fv mo mp l mq mr">from<strong class="lm hv"> </strong>sklearn.neural_network<strong class="lm hv"> </strong>import<strong class="lm hv"> MLPClassifier<br/>mlp </strong>=<strong class="lm hv"> MLPClassifier(random_state</strong>=<strong class="lm hv">42)<br/>mlp</strong>.<strong class="lm hv">fit(X_train, y_train)<br/>print("Accuracy on training set: {:.2f}"</strong>.<strong class="lm hv">format(mlp</strong>.<strong class="lm hv">score(X_train, y_train)))<br/>print("Accuracy on test set: {:.2f}"</strong>.<strong class="lm hv">format(mlp</strong>.<strong class="lm hv">score(X_test, y_test)))</strong></span></pre><h2 id="d5db" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="c920" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Accuracy on training set: 0.71<br/>Accuracy on test set: 0.67</strong></span></pre><p id="0ebc" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">🌐观察:MLP的准确性不如其他模型，这可能是由于数据的缩放。神经网络也期望所有输入特征以相似的方式变化，并且理想地具有0的平均值和1的方差。</p><p id="7f3b" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">现在应用来自神经网络的'<strong class="kp hv">标准定标器'</strong>:</p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="41db" class="mn jw hu lm b fv mo mp l mq mr">from<strong class="lm hv"> </strong>sklearn.preprocessing<strong class="lm hv"> </strong>import<strong class="lm hv"> StandardScaler<br/>scaler </strong>=<strong class="lm hv"> StandardScaler()<br/>X_train_scaled </strong>=<strong class="lm hv"> scaler</strong>.<strong class="lm hv">fit_transform(X_train)<br/>X_test_scaled </strong>=<strong class="lm hv"> scaler</strong>.<strong class="lm hv">fit_transform(X_test)<br/>mlp </strong>=<strong class="lm hv"> MLPClassifier(random_state</strong>=<strong class="lm hv">0)<br/>mlp</strong>.<strong class="lm hv">fit(X_train_scaled, y_train)<br/>print("Accuracy on training set: {:.3f}"</strong>.<strong class="lm hv">format(<br/>    mlp</strong>.<strong class="lm hv">score(X_train_scaled, y_train)))<br/>print("Accuracy on test set: {:.3f}"</strong>.<strong class="lm hv">format(mlp</strong>.<strong class="lm hv">score(X_test_scaled, y_test)))</strong></span></pre><h2 id="64db" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="4c47" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Accuracy on training set: 0.823<br/>Accuracy on test set: 0.802</strong></span></pre><p id="a69d" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">现在再次将<strong class="kp hv">MLP classifier</strong>与<strong class="kp hv">“max _ ITER</strong>=<strong class="kp hv">1000”:</strong></p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="c5d1" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">mlp </strong>=<strong class="lm hv"> MLPClassifier(max_iter</strong>=<strong class="lm hv">1000, random_state</strong>=<strong class="lm hv">0)<br/>mlp</strong>.<strong class="lm hv">fit(X_train_scaled, y_train)<br/>print("Accuracy on training set: {:.3f}"</strong>.<strong class="lm hv">format(<br/>    mlp</strong>.<strong class="lm hv">score(X_train_scaled, y_train)))<br/>print("Accuracy on test set: {:.3f}"</strong>.<strong class="lm hv">format(mlp</strong>.<strong class="lm hv">score(X_test_scaled, y_test)))</strong></span></pre><h2 id="c74d" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="3616" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Accuracy on training set: 0.877<br/>Accuracy on test set: 0.755</strong></span></pre><p id="146a" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">再次应用“<strong class="kp hv">MLP classifier”</strong>与“<strong class="kp hv">alpha</strong>=<strong class="kp hv">1”:</strong></p><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="6258" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">mlp </strong>=<strong class="lm hv"> MLPClassifier(max_iter</strong>=<strong class="lm hv">1000, alpha</strong>=<strong class="lm hv">1, random_state</strong>=<strong class="lm hv">0)<br/>mlp</strong>.<strong class="lm hv">fit(X_train_scaled, y_train)<br/>print("Accuracy on training set: {:.3f}"</strong>.<strong class="lm hv">format(<br/>    mlp</strong>.<strong class="lm hv">score(X_train_scaled, y_train)))<br/>print("Accuracy on test set: {:.3f}"</strong>.<strong class="lm hv">format(mlp</strong>.<strong class="lm hv">score(X_test_scaled, y_test)))</strong></span></pre><h2 id="4899" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐输出:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="ba2c" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">Accuracy on training set: 0.795<br/>Accuracy on test set: 0.792</strong></span></pre><h1 id="8dd0" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">🌐神经网络的虚拟化</h1><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff jj"><img src="../Images/66ddff6ba3a46d0b795c04b6b7acd0c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uxa4td8GJGUwWdhnl_gy1A.jpeg"/></div></div></figure><h2 id="809a" class="mn jw hu bd jx my mz na kb nb nc nd kf kw ne nf kh la ng nh kj le ni nj kl nk dt translated">🌐代码:</h2><pre class="jk jl jm jn fq mj lm mk ml aw mm dt"><span id="f4e4" class="mn jw hu lm b fv mo mp l mq mr"><strong class="lm hv">plt</strong>.<strong class="lm hv">figure(figsize</strong>=<strong class="lm hv">(20, 5))<br/>plt</strong>.<strong class="lm hv">imshow(mlp</strong>.<strong class="lm hv">coefs_[0], interpolation</strong>=<strong class="lm hv">'none', cmap</strong>=<strong class="lm hv">'viridis')<br/>plt</strong>.<strong class="lm hv">yticks(range(8), diabetes_features)<br/>plt</strong>.<strong class="lm hv">xlabel("Columns in weight matrix")<br/>plt</strong>.<strong class="lm hv">ylabel("Input feature")<br/>plt</strong>.<strong class="lm hv">colorbar()</strong></span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nz"><img src="../Images/f087c598513d8ed5a53b7e888993b705.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kf0Ic44-BHDAtfrTBr9N9A.png"/></div></div></figure><p id="63de" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">观察结果:</p><p id="4d15" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">从热点图中，很难快速指出哪些要素与其他要素相比权重相对较低。</p><h1 id="ef22" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">📘摘要</h1><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff oa"><img src="../Images/f083b3379acaf64387453d3bbd342f36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*TgQvtIo9zf5yMJuL7x1d0g.jpeg"/></div></figure><p id="459e" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">就这些。感谢阅读。:)</p><p id="d3c9" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">完整代码请访问<a class="ae ls" href="https://www.kaggle.com/harunshimanto/ml-for-diabetes-from-bangladesh/notebook" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>。</p><p id="ac29" class="pw-post-body-paragraph kn ko hu kp b kq ln iv ks kt lo iy kv kw lp ky kz la lq lc ld le lr lg lh li hn dt translated">如果你喜欢这篇文章，然后给👏鼓掌。开心分析！</p></div></div>    
</body>
</html>