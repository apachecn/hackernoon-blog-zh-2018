<html>
<head>
<title>Instance Segmentation in Google Colab with Custom Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于自定义数据集的Google Colab实例分割</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/instance-segmentation-in-google-colab-with-custom-dataset-b3099ac23f35?source=collection_archive---------7-----------------------#2018-09-18">https://medium.com/hackernoon/instance-segmentation-in-google-colab-with-custom-dataset-b3099ac23f35?source=collection_archive---------7-----------------------#2018-09-18</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="f386" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">本文提出了一个简单而免费的解决方案，使用自定义数据集在Google Colab笔记本中训练Tensorflow模型进行实例分割。</p><p id="0d88" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><a class="ae jp" href="https://hackernoon.com/object-detection-in-google-colab-with-custom-dataset-5a7bb2b0e97e" rel="noopener ugc nofollow" target="_blank">上一篇文章</a>是关于使用自定义数据集在Google Colab中进行对象检测的，在那里我训练了一个模型来推断照片中我的狗的边界框。我的文章的主角又是我的狗:在这种情况下，我们向前迈进了一步，我们不仅识别边界框，我们甚至进行像素分类。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff jq"><img src="../Images/e2ae48315dad5335578a3b7b07af75c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hXM7HTFNazzsUXI6MREbnA.png"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">Instance segmentation with my dog</figcaption></figure><p id="d809" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><em class="kg">与上一篇文章相比，我们持有相同的特征:</em></p><ul class=""><li id="50a5" class="kh ki hu it b iu iv iy iz jc kj jg kk jk kl jo km kn ko kp dt translated"><em class="kg">唯一的要求是数据集，用标注工具</em>创建</li><li id="ea9d" class="kh ki hu it b iu kq iy kr jc ks jg kt jk ku jo km kn ko kp dt translated"><em class="kg">一个单独的Google Colab笔记本包含所有的步骤:它从数据集开始，执行模型的训练并显示推理</em></li><li id="2675" class="kh ki hu it b iu kq iy kr jc ks jg kt jk ku jo km kn ko kp dt translated"><em class="kg">它运行在Google Colab(支持GPU的环境)和Google Drive存储中，因此</em> <strong class="it hv"> <em class="kg">它完全基于免费的云资源</em> </strong></li></ul><p id="70cd" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这些特性允许任何遵循本教程的人创建一个实例分割模型，并在Google Colab中测试它，或者导出模型以在本地机器上运行。</p><p id="27c9" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">本文的源代码，包括样本数据集，可以在我的<a class="ae jp" href="https://github.com/RomRoc/maskrcnn_train_tensorflow_colab" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中找到。</p><h1 id="6853" class="kv kw hu bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls dt translated">选择框架</h1><p id="f0eb" class="pw-post-body-paragraph ir is hu it b iu lt iw ix iy lu ja jb jc lv je jf jg lw ji jj jk lx jm jn jo hn dt translated">有各种开源框架来实现实例分割，您可以在斯坦福大学的<a class="ae jp" href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds06.pdf" rel="noopener ugc nofollow" target="_blank">本演示文稿</a>中找到概述。</p><p id="b3d4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们放弃了不是基于Tensorflow的解决方案，例如基于Caffe2的脸书Detectron，因为我们决定在已经与Tensorflow集成的Google Colab中训练模型。</p><p id="e4a0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">最受欢迎的框架之一，易于使用且记录良好，是Matterport Mask R-CNN。根据我的测试，这是可用的最简单和最健壮的实现之一。</p><p id="2301" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">此外，我在其他实现中遇到的一个大问题是将注释输出文件转换成框架输入格式。需要明确的是，一旦使用图形工具创建了数据集的像素标注，就应该将其转换为训练框架定义的输入格式。</p><p id="1d04" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">Matterport在一篇清晰的<a class="ae jp" href="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46" rel="noopener ugc nofollow" target="_blank">文章</a>中开发了这项任务，演示了如何将注释文件转换为Matterport Mask R-CNN格式。</p><h1 id="4799" class="kv kw hu bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls dt translated">制作数据集</h1><p id="1e3e" class="pw-post-body-paragraph ir is hu it b iu lt iw ix iy lu ja jb jc lv je jf jg lw ji jj jk lx jm jn jo hn dt translated">在上一篇文章中，我们创建了边界框注释来获得对象检测模型，现在我们将训练实例分割模型，因此我们创建像素级掩模注释来定义数据集中对象的边界。在各种可用的工具中，我选择了一个直观且做得很好的工具:牛津大学的<strong class="it hv"> VGG图像注释器(VIA) </strong>，你可以在项目的<a class="ae jp" href="http://www.robots.ox.ac.uk/~vgg/software/via/" rel="noopener ugc nofollow" target="_blank">官方页面</a>中看到文档。此外，很容易将VIA与Matterport框架集成。</p><p id="10ed" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这个工具不需要任何安装，你只需要下载软件包，用现代浏览器打开<code class="eh ly lz ma mb b">via.html</code>文件。</p><p id="3a9f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">创建一个好的数据集来实现一个性能良好的训练模型是很重要的。在不同的光照条件下，从不同的角度和在不同的背景下拍摄物体的照片，是获得良好的通用模型和避免过度拟合的好原则。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff mc"><img src="../Images/a65de79448f3a3a8d8503144d4c7b5ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XKP96dxuk-gW1Iqz7oKt0w.png"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">Dataset of dog</figcaption></figure><p id="d665" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在注释过程的最后，我创建了具有以下结构的“images.zip”文件:</p><pre class="jr js jt ju fq md mb me mf aw mg dt"><span id="019f" class="mh kw hu mb b fv mi mj l mk ml">images.zip<br/>|- "train" directory<br/>  |- jpg image files of training data<br/>  |- "via_region_data.json" annotations file of training data<br/>|- "val" directory<br/>  |- jpg image files of validation data<br/>  |- "via_region_data.json" annotations file of validation data</span></pre><p id="6a84" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">最后，我将zip文件上传到Google Drive，以便在培训和测试过程中使用。我将数据集文件包含在我的Gitub repo中，其中包含狗图像的像素注释。</p><h1 id="e567" class="kv kw hu bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls dt translated">培训模式</h1><p id="f5c0" class="pw-post-body-paragraph ir is hu it b iu lt iw ix iy lu ja jb jc lv je jf jg lw ji jj jk lx jm jn jo hn dt translated">所有的步骤都在谷歌Colab笔记本包括在我的回购。在我的例子中，训练过程持续大约半小时，有5个时期，为了得到更准确的模型，你可以增加时期的数量和数据集的大小。</p><p id="5792" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我选择了支持Python3 GPU的环境，使用Google Colab中提供的Tesla K80 GPU长达12小时。笔记本的后续步骤是:</p><p id="f987" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">安装需要的包</strong>:用Tensorflow安装Matterport实例分割的包、库和环境变量。</p><p id="2899" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">下载并提取数据集</strong>:下载Google Colab文件系统中的images.zip数据集，之前上传到Google Drive中。用image.zip数据集的Google Drive id更新<code class="eh ly lz ma mb b">fileId</code>变量。</p><p id="0cf8" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">编辑设置文件</strong>:我的repo中的代码受Matterport Splash of Color sample的启发，要使用不同的数据集运行，您应该用对象的名称替换出现的“balloon”和“Balloon”。</p><p id="afda" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">训练模型</strong>:在训练过程中使用预先训练的权重应用迁移学习。选项有COCO和ImageNet。</p><p id="cf19" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">训练过程输出神经网络的结构和各种参数，如网络结构(Resnet50或Resnet101)。</p><pre class="jr js jt ju fq md mb me mf aw mg dt"><span id="a915" class="mh kw hu mb b fv mi mj l mk ml">Using TensorFlow backend. <br/>Weights:  coco <br/>Dataset:  dataset/ <br/>Logs:  /logs</span><span id="2a8c" class="mh kw hu mb b fv mm mj l mk ml">Configurations: <br/>BACKBONE                       resnet101 <br/>BACKBONE_STRIDES               [4, 8, 16, 32, 64] <br/>BATCH_SIZE                     2<br/>...<br/>GPU_COUNT                      1 <br/>GRADIENT_CLIP_NORM             5.0 <br/>IMAGES_PER_GPU                 2<br/>...<br/>Selecting layers to train <br/>fpn_c5p5               (Conv2D) <br/>fpn_c4p4               (Conv2D) <br/>fpn_c3p3               (Conv2D) <br/>fpn_c2p2               (Conv2D) <br/>...</span></pre><p id="58ef" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在培训流程的Tensorboard图表下方:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff mn"><img src="../Images/36759a6ecb2398dbd0cc0524d5cede19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*46TdUZSSTKd599B-10RIIw.png"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">Tensorboard charts</figcaption></figure><h1 id="792d" class="kv kw hu bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls dt translated">推理</h1><p id="0c3b" class="pw-post-body-paragraph ir is hu it b iu lt iw ix iy lu ja jb jc lv je jf jg lw ji jj jk lx jm jn jo hn dt translated">最后，我们可以用训练好的模型运行测试数据集推理。</p><p id="5e71" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">输出包括推断数据(图像分辨率、锚点形状等)，以及带有边界框、分割遮罩和置信度得分的测试图像。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff jq"><img src="../Images/bddfeceb62ec97073c2fde903f165d42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fe8fjQ3WJWn7BbpTCvTa_Q.png"/></div></div></figure><h1 id="de95" class="kv kw hu bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls dt translated">结论</h1><p id="c743" class="pw-post-body-paragraph ir is hu it b iu lt iw ix iy lu ja jb jc lv je jf jg lw ji jj jk lx jm jn jo hn dt translated">如果您想在单个对象类上运行实例分段，您可以对我的Github代码做一些小的修改，并使其适应您的数据集。</p><p id="dc71" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我希望你喜欢这篇文章，万一留下一些掌声，它将鼓励我写关于计算机视觉的机器学习的其他实用文章:)</p></div></div>    
</body>
</html>