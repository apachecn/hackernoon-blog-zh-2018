<html>
<head>
<title>Speaker Diarization — The Squad Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">扬声器二进制化——团队方式</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/speaker-diarization-the-squad-way-2205e0accbda?source=collection_archive---------3-----------------------#2018-10-06">https://medium.com/hackernoon/speaker-diarization-the-squad-way-2205e0accbda?source=collection_archive---------3-----------------------#2018-10-06</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="fbc4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><a class="ae jp" href="https://en.wikipedia.org/wiki/Speaker_diarisation" rel="noopener ugc nofollow" target="_blank">说话人二进制化</a>旨在解决多方录音中<strong class="it hv"><em class="jq"/></strong>时“谁说话”的问题。</p><p id="ad18" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在<a class="ae jp" href="https://www.squadplatform.com/" rel="noopener ugc nofollow" target="_blank">小队</a>，ML团队正在为<a class="ae jp" href="https://www.squadvoice.co/" rel="noopener ugc nofollow" target="_blank">小队语音</a>构建一个自动化的质量保证引擎。在质量检查阶段，根据各种质量参数对呼叫代表的表现进行评分，例如:</p><ol class=""><li id="d70c" class="jr js hu it b iu iv iy iz jc jt jg ju jk jv jo jw jx jy jz dt translated">无论该代表是否在电话中催促，</li><li id="cb25" class="jr js hu it b iu ka iy kb jc kc jg kd jk ke jo jw jx jy jz dt translated">无论他是否在任何时候对领导无礼，</li><li id="f25a" class="jr js hu it b iu ka iy kb jc kc jg kd jk ke jo jw jx jy jz dt translated">他是否使用了恰当的语言，等等。</li></ol><p id="30dd" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">对呼叫记录的这种质量检查由主叫代表所说的“<strong class="it hv"><em class="jq"/></strong>是什么”和相同内容所说的“<strong class="it hv"><em class="jq"/></strong>如何”来指导。因此，我们需要确保座席发言的电话录音部分与客户或主管发言的部分分开。这是一个具有挑战性的问题，因为Squad在印度的云电话合作伙伴只能以单声道格式记录通话。单声道格式将双方的音频存储在单个声道上，与立体声格式相反，在立体声格式中，呼叫者的音频将存储在一个声道上，而被呼叫者的音频将写在不同的声道上。因此，作为质量检查的先决条件，需要一个扬声器二进制化系统。然而，我们有一个更集中的问题，因为我们用例的发言者数量固定为两个。</p><figure class="kg kh ki kj fq kk fe ff paragraph-image"><div class="fe ff kf"><img src="../Images/d101d803fa71f02f872c44ea3a02b9e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*xaM6d2ywWid9saSlh_zm6w.jpeg"/></div></figure><h2 id="18ee" class="kn ko hu bd kp kq kr ks kt ku kv kw kx jc ky kz la jg lb lc ld jk le lf lg lh dt translated"><strong class="ak">如何解决扬声器二进制化？</strong></h2><p id="528d" class="pw-post-body-paragraph ir is hu it b iu li iw ix iy lj ja jb jc lk je jf jg ll ji jj jk lm jm jn jo hn dt translated">说话人二进制化的问题相当复杂。老实说，这是我迄今为止解决的最困难的<a class="ae jp" href="https://hackernoon.com/tagged/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>问题。这个解决方案利用了监督和非监督机器学习技术。此外，它依赖于最近的<a class="ae jp" href="https://hackernoon.com/tagged/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>和传统的<a class="ae jp" href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html" rel="noopener ugc nofollow" target="_blank">凝聚聚类</a>模型的组合。所以让我们开始吧。</p><p id="4073" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们的说话人二进制化解决方案来自于<a class="ae jp" href="https://www.youtube.com/watch?v=96b_weTZb2w&amp;index=33&amp;list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF" rel="noopener ugc nofollow" target="_blank">一次学习</a>，作品使用的说话人识别架构:<a class="ae jp" href="https://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf" rel="noopener ugc nofollow" target="_blank">“vox celeb:大规模说话人识别数据集</a>”，以及本作品中解释的说话人变化检测算法:<a class="ae jp" href="http://herve.niderb.fr/download/pdfs/Bredin2017.pdf" rel="noopener ugc nofollow" target="_blank">“TristouNet:说话人话轮嵌入的三重损失</a>。</p><h2 id="b8ec" class="kn ko hu bd kp kq kr ks kt ku kv kw kx jc ky kz la jg lb lc ld jk le lf lg lh dt translated"><strong class="ak">扬声器差异化渠道</strong></h2><p id="50c8" class="pw-post-body-paragraph ir is hu it b iu li iw ix iy lj ja jb jc lk je jf jg ll ji jj jk lm jm jn jo hn dt translated">我们利用三个基于VGG-M的CNN架构[具有共享的权重]来使用三元组损失优化范例进行训练，以便学习相同/(不同)说话者的音频记录之间的(不)相似性。我们称之为说话者差异化管道。</p><p id="24d7" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了训练说话者区分管道，在模型训练的每次迭代中，生成三个一组的音频记录，由<q_sound p_sound="" n_sound="">表示。这里q_sound表示锚音频片段，p_sound表示同一说话者的某个其他音频片段，n_sound表示不同说话者的音频片段。然后，计算三元组中每个音频的幅度谱图，并将其推送到上述基于VGG-M的模型。在整个优化过程中，所有三个CNN共享相同的权重。最后，来自由<q_vec p_vec="" n_vec="">表示的网络的最后一层的嵌入被推到铰链损耗层，定义为:</q_vec></q_sound></p><figure class="kg kh ki kj fq kk fe ff paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="fe ff ln"><img src="../Images/e061d35264b6991dd372549bd58f4de2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XQ5GqDrKej8RibhhTSa75A.png"/></div></div><figcaption class="ls lt fg fe ff lu lv bd b be z ek">Loss function optimized for speaker differentiation</figcaption></figure><p id="1a0f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在上式中，D(x，y)表示两个嵌入向量之间的欧几里德距离，铰链损失函数中的“g”余量常数保持等于1。</p><figure class="kg kh ki kj fq kk fe ff paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="fe ff lw"><img src="../Images/af3ae88234d71afe6a84119138bad160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M3L4_xrGjd2EVLGzuymq5A.jpeg"/></div></div><figcaption class="ls lt fg fe ff lu lv bd b be z ek">Speaker differentiation pipeline visualization</figcaption></figure><p id="7741" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv"> <em class="jq">这一步要小心的几个陷阱</em> </strong>:</p><ol class=""><li id="c7e5" class="jr js hu it b iu iv iy iz jc jt jg ju jk jv jo jw jx jy jz dt translated">确保输入星等谱图是沿时间轴归一化的平均值和方差。直觉上，每个人的声音中会有一些更突出的频率，这有助于他们声带产生的声音的独特性。</li><li id="1903" class="jr js hu it b iu ka iy kb jc kc jg kd jk ke jo jw jx jy jz dt translated">这将确保对应于在短时间帧内发音的不同音素的频率仓的能量被平滑。</li><li id="1933" class="jr js hu it b iu ka iy kb jc kc jg kd jk ke jo jw jx jy jz dt translated">在最后一个全连接层中使用<a class="ae jp" href="https://keras.io/activations/#softplus" rel="noopener ugc nofollow" target="_blank"><strong class="it hv"><em class="jq">soft plus</em></strong></a>激活函数，而不是Relu，如果使用欧氏距离作为距离度量，这是因为平方根(x)在0处不可微。或者优化平方欧几里得距离，如果您希望使用Relu。</li></ol><figure class="kg kh ki kj fq kk fe ff paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="fe ff lx"><img src="../Images/4d9a7c2915c3014383782a893bf39310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TjazOQwbMHFtpU5-zBI5og.png"/></div></div><figcaption class="ls lt fg fe ff lu lv bd b be z ek">Base CNN model trained for speaker differentiation</figcaption></figure><p id="c9e3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">使用该模型的一个主要好处是，它将为模型的任何长度的音频输入提供相同的嵌入向量维数。然而，由于模型是使用批量梯度下降进行训练的，因此在训练阶段，我们将音频记录长度固定为3s。</p><h2 id="f143" class="kn ko hu bd kp kq kr ks kt ku kv kw kx jc ky kz la jg lb lc ld jk le lf lg lh dt translated"><strong class="ak">用于训练说话人区分管道的数据集:</strong></h2><p id="357a" class="pw-post-body-paragraph ir is hu it b iu li iw ix iy lj ja jb jc lk je jf jg ll ji jj jk lm jm jn jo hn dt translated">为了训练管道，我们利用了通过SquadVoice发出的IVR任务呼叫的音频记录。我们采集了100个发言者的通话记录，每个发言者平均12.06分钟的音频，而每个IVR记录平均为2.41分钟。</p><p id="e174" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">此外，我们对这些IVR记录进行了预处理，如使用<a class="ae jp" href="http://sox.sourceforge.net/" rel="noopener ugc nofollow" target="_blank"> sox </a>去除IVR语音并从通话记录结束时修剪静音。</p><p id="8f4b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">接下来，我们从每个音频记录中计算出长度在(4.5，s 12.5s)之间的随机非重叠音频片段。因此，生成总共8586个音频片段的数据集，</p><p id="74a3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">最后，我们在每次迭代中将随机的3s音频记录作为输入传递给神经网络，并确保训练集和验证集音频片段都具有非重叠的说话者分布。也就是说，不存在其音频片段同时属于训练和验证分割的说话者。</p><h2 id="a730" class="kn ko hu bd kp kq kr ks kt ku kv kw kx jc ky kz la jg lb lc ld jk le lf lg lh dt translated"><strong class="ak">说话人变化检测</strong></h2><p id="f462" class="pw-post-body-paragraph ir is hu it b iu li iw ix iy lj ja jb jc lk je jf jg ll ji jj jk lm jm jn jo hn dt translated">我们应用从TristouNet工作中学到的知识，使用上面训练的模型找到说话者改变点。我们维护了持续时间为3s的左右滑动窗口的两个滑动窗口对，并以0.1s的非常小的步幅移动它们。然后，对于每个左右窗口对，获得它们相应的幅度谱图，并通过在步骤(1)中训练的CNN运行它们，以获得每个左右窗口对的嵌入。之后，我们计算左右嵌入之间的欧几里德距离，如果距离超过特定阈值，那么这将代表说话人改变点。例如，对于30秒的音频记录，我们将有以下工作流程:</p><figure class="kg kh ki kj fq kk fe ff paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="fe ff ly"><img src="../Images/8332d558e9afff7b778811b30ce52754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wHCwkxNOv1DiabMVXGVHrw.jpeg"/></div></div><figcaption class="ls lt fg fe ff lu lv bd b be z ek">Workflow for speaker change detection</figcaption></figure><h2 id="e37c" class="kn ko hu bd kp kq kr ks kt ku kv kw kx jc ky kz la jg lb lc ld jk le lf lg lh dt translated"><strong class="ak">用于对音频片段进行分组的凝聚聚类</strong></h2><p id="485f" class="pw-post-body-paragraph ir is hu it b iu li iw ix iy lj ja jb jc lk je jf jg ll ji jj jk lm jm jn jo hn dt translated">我们根据扬声器更换点对音频进行了分解。即，如果检测到5个说话者改变点，则我们将音频分成6个音频记录，计算获得的片段的幅度谱图，然后使用之前训练的CNN计算说话者嵌入。随后在这些嵌入的基础上运行<strong class="it hv"> <em class="jq">凝聚聚类</em> </strong>来按照说话者将音频片段分组在一起。</p><h2 id="0212" class="kn ko hu bd kp kq kr ks kt ku kv kw kx jc ky kz la jg lb lc ld jk le lf lg lh dt translated"><strong class="ak">实验结果及评价</strong></h2><p id="1945" class="pw-post-body-paragraph ir is hu it b iu li iw ix iy lj ja jb jc lk je jf jg ll ji jj jk lm jm jn jo hn dt translated">这样就建成了完整的二化管道。当在100个呼叫记录的数据集上测试时。其平均<a class="ae jp" href="http://pyannote.github.io/pyannote-metrics/reference.html#diarization" rel="noopener ugc nofollow" target="_blank">二进制化错误率【混淆部分】</a>为12.23%，根据行业标准，这是相当高的。此外，流水线在存在背景噪声的情况下也表现良好。作为例子，增加了一些原始版本的音频调用示例。所附录音是根据实际客户流程模拟的电话。</p><pre class="kg kh ki kj fq lz ma mb mc aw md dt"><span id="43db" class="kn ko hu ma b fv me mf l mg mh">+---------------+-------------+-------------+---------------+<br/>| original call | Calling Rep |    Lead     | Confusion DER |<br/>+---------------+-------------+-------------+---------------+<br/>| <a class="ae jp" href="https://drive.google.com/file/d/1kMW9sCcIt1exM_5s7AqA_RFRJ5CVNh_F/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">call_1</a>        | <a class="ae jp" href="https://drive.google.com/file/d/1BjaVDHHp7HkKNrvSgsptjmQUNOB3nMpt/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">call_1_rep</a>  | <a class="ae jp" href="https://drive.google.com/file/d/18gsvjKs-lgMfIYN-b-3jSFvel63LdFN5/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">call_1_lead</a> |       5.5%    |<br/>| <a class="ae jp" href="https://drive.google.com/open?id=1OdyLZVw2-x5MbXEeyw7HT50XJp5iJOcW" rel="noopener ugc nofollow" target="_blank">call_2</a>        | <a class="ae jp" href="https://drive.google.com/file/d/1kiRxkPZ22H0TtEJAz1-oX8DbLZIGR2Kg/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">call_2_rep</a>  | <a class="ae jp" href="https://drive.google.com/file/d/1Fu_dKunYk-GloTteK-Il-lBbnPhs9uDO/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">call_2_lead</a> |       6.44%   |<br/>| <a class="ae jp" href="https://drive.google.com/file/d/1uFEi7TAx4Su3nub_YAVI6RrWLzXQ8wCD/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">call_3</a>        | <a class="ae jp" href="https://drive.google.com/file/d/1-awWVk62k347GtkjamOQ9CzvmvtKfTrh/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">call_3_rep</a>  | <a class="ae jp" href="https://drive.google.com/file/d/1bpniRacLzblKqT6PLGpCfJq3FCrA4DCL/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">call_3_lead</a> |       6.26%   |<br/>+---------------+-------------+-------------+---------------+</span></pre><h2 id="fff1" class="kn ko hu bd kp kq kr ks kt ku kv kw kx jc ky kz la jg lb lc ld jk le lf lg lh dt translated">承认</h2><p id="4a30" class="pw-post-body-paragraph ir is hu it b iu li iw ix iy lj ja jb jc lk je jf jg ll ji jj jk lm jm jn jo hn dt translated">我要感谢<a class="ae jp" href="https://www.linkedin.com/in/sanchit-aggarwal/" rel="noopener ugc nofollow" target="_blank">桑奇特·阿格瓦尔</a>对管道设计和扬声器二化结果的讨论和审查，以及<a class="ae jp" href="https://www.linkedin.com/in/pragyajswl/" rel="noopener ugc nofollow" target="_blank">普拉加·贾伊斯瓦尔</a>和<a class="ae jp" href="https://www.linkedin.com/in/ved-vasu-sharma-34588ba2/" rel="noopener ugc nofollow" target="_blank">维德·瓦苏·夏尔马</a>在将管道转移到SquadAI代码库时进行的设计讨论和公关审查。我还要感谢<a class="ae jp" href="https://www.squadvoice.co/" rel="noopener ugc nofollow" target="_blank">团队之声</a>运营团队的重要投入。</p><figure class="kg kh ki kj fq kk"><div class="bz el l di"><div class="mi mj l"/></div></figure></div></div>    
</body>
</html>