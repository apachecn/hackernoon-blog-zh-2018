<html>
<head>
<title>How to use Keras sparse_categorical_crossentropy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用Keras稀疏分类交叉熵</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/how-to-use-keras-sparse-categorical-crossentropy-47da9734bd98?source=collection_archive---------9-----------------------#2018-10-08">https://medium.com/hackernoon/how-to-use-keras-sparse-categorical-crossentropy-47da9734bd98?source=collection_archive---------9-----------------------#2018-10-08</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/11b152d1163d1a76d93a54a1495d376a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tt0yd7zNiTSDZ1eg.png"/></div></div></figure><p id="4f4c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这个快速教程中，我将向您展示两个简单的例子，在编译Keras模型时使用<code class="eh ka kb kc kd b">sparse_categorical_crossentropy</code>损失函数和<code class="eh ka kb kc kd b">sparse_categorical_accuracy</code>度量。</p><h1 id="f298" class="ke kf hu bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb dt translated">示例一— MNIST分类</h1><p id="3521" class="pw-post-body-paragraph jc jd hu je b jf lc jh ji jj ld jl jm jn le jp jq jr lf jt ju jv lg jx jy jz hn dt translated">作为多类、单标签分类数据集之一，任务是将手写数字的灰度图像(28像素乘28像素)分类到它们的十个类别(0到9)。让我们构建一个Keras CNN模型来处理它，最后一层应用了“softmax”激活，它输出一个十个概率分数的数组(总和为1)。每个分数将是当前数字图像属于我们的10个数字类之一的概率。</p><figure class="lh li lj lk fq iv"><div class="bz el l di"><div class="ll lm l"/></div></figure><p id="8640" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">对于这种输出形状为(None，10)的模型，传统的方法是将目标输出转换为独热编码数组，以匹配输出形状，然而，在<code class="eh ka kb kc kd b">sparse_categorical_crossentropy</code>损失函数的帮助下，我们可以跳过这一步，将整数作为目标。</p><p id="c713" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">像这样编译模型时，你需要做的就是用<code class="eh ka kb kc kd b">sparse_categorical_crossentropy</code>替换<code class="eh ka kb kc kd b">categorical_crossentropy</code>。</p><figure class="lh li lj lk fq iv"><div class="bz el l di"><div class="ll lm l"/></div></figure><p id="5013" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">之后，您可以使用整数目标来训练模型，即一维数组，如</p><pre class="lh li lj lk fq ln kd lo lp aw lq dt"><span id="3d6a" class="lr kf hu kd b fv ls lt l lu lv"><strong class="kd hv">array([5, 0, 4, 1, 9 ...], dtype=uint8)</strong></span></pre><p id="97f1" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">请注意，这不会影响模型输出形状，它仍会为每个输入样本输出十个概率得分。</p><h1 id="27d8" class="ke kf hu bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb dt translated">示例二—字符级序列到序列预测</h1><p id="e004" class="pw-post-body-paragraph jc jd hu je b jf lc jh ji jj ld jl jm jn le jp jq jr lf jt ju jv lg jx jy jz hn dt translated">我们将根据威廉·莎士比亚的综合作品训练一个模型，然后用它来创作一部类似风格的戏剧。</p><p id="a065" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">文本blob中的每个字符首先通过调用Python的内置函数<code class="eh ka kb kc kd b">ord()</code>转换成一个整数，该函数返回一个表示字符的整数作为其ASCII值。例如，<code class="eh ka kb kc kd b">ord('a')</code>返回整数<code class="eh ka kb kc kd b">97</code>。因此，我们有一个整数列表来表示整个文本。</p><p id="0fa1" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">给定序列长度为100的移动窗口，模型学习预测未来一个时间步长的序列。换句话说，给定序列中时间步长T0~T99的特征，模型预测时间步长T1~T100的特征。</p><p id="3dd4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">让我们在Keras中构建一个简单的序列对序列模型。</p><figure class="lh li lj lk fq iv"><div class="bz el l di"><div class="ll lm l"/></div></figure><p id="1508" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们可以进一步可视化模型的结构，以分别理解其输入和输出形状。</p><figure class="lh li lj lk fq iv fe ff paragraph-image"><div class="fe ff lw"><img src="../Images/80a7e0ada3429a5666b3469e40ddcd26.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/0*sMSrTX7f8tcghuUV.png"/></div><figcaption class="lx ly fg fe ff lz ma bd b be z ek">The training model</figcaption></figure><p id="b877" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">即使模型具有三维输出，当用损失函数<code class="eh ka kb kc kd b">sparse_categorical_crossentropy</code>编译时，我们可以将训练目标作为整数序列。与前面的例子类似，在没有<code class="eh ka kb kc kd b">sparse_categorical_crossentropy</code>的帮助下，首先需要将输出的整数转换成一键编码的形式以适应模型。</p><p id="4530" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">训练模式是，</p><ul class=""><li id="2de1" class="mb mc hu je b jf jg jj jk jn md jr me jv mf jz mg mh mi mj dt translated">无状态</li><li id="7511" class="mb mc hu je b jf mk jj ml jn mm jr mn jv mo jz mg mh mi mj dt translated">序列长度=100</li><li id="9f32" class="mb mc hu je b jf mk jj ml jn mm jr mn jv mo jz mg mh mi mj dt translated">batch_size = 128</li><li id="37de" class="mb mc hu je b jf mk jj ml jn mm jr mn jv mo jz mg mh mi mj dt translated">模型输入形状:(批处理大小，序列长度)</li><li id="aaba" class="mb mc hu je b jf mk jj ml jn mm jr mn jv mo jz mg mh mi mj dt translated">模型输出形状:(批处理大小，序列长度，最大令牌数)</li></ul><p id="1a1b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">一旦训练好模型，我们就可以让它“有状态”，一次预测五个字符。通过使其有状态，LSTMs在一个批次中的每个样本的最后状态将被用作下一个批次中的样本的初始状态，或者简单地说，一次预测的那五个字符和下一个预测批次是一个序列中的字符。</p><p id="3108" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">预测模型加载训练好的模型权重并一次预测五个字符，</p><ul class=""><li id="d822" class="mb mc hu je b jf jg jj jk jn md jr me jv mf jz mg mh mi mj dt translated">宏伟威严的</li><li id="c82a" class="mb mc hu je b jf mk jj ml jn mm jr mn jv mo jz mg mh mi mj dt translated">seq_len =1，一个字符/批次</li><li id="2c09" class="mb mc hu je b jf mk jj ml jn mm jr mn jv mo jz mg mh mi mj dt translated">batch_size = 5</li><li id="e7f1" class="mb mc hu je b jf mk jj ml jn mm jr mn jv mo jz mg mh mi mj dt translated">模型输入形状:(批处理大小，序列长度)</li><li id="e971" class="mb mc hu je b jf mk jj ml jn mm jr mn jv mo jz mg mh mi mj dt translated">模型输出形状:(批处理大小，序列长度，最大令牌数)</li><li id="fcb7" class="mb mc hu je b jf mk jj ml jn mm jr mn jv mo jz mg mh mi mj dt translated">需要在预测前调用<code class="eh ka kb kc kd b">reset_states()</code>来重置LSTMs的初始状态。</li></ul><p id="a694" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">关于该模型的更多实现细节，请参考我的<a class="ae mp" href="https://github.com/Tony607/keras_sparse_categorical_crossentropy" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>。</p><h1 id="c17f" class="ke kf hu bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb dt translated">结论和进一步阅读</h1><p id="97ae" class="pw-post-body-paragraph jc jd hu je b jf lc jh ji jj ld jl jm jn le jp jq jr lf jt ju jv lg jx jy jz hn dt translated">本教程探索了两个例子，使用<code class="eh ka kb kc kd b">sparse_categorical_crossentropy</code>来保持整数作为字符/多类分类标签，而不转换成一个热标签。因此，当标签是整数时，模型的输出将是类似softmax one-hot的形状。</p><p id="8539" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">要了解<a class="ae mp" href="https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/keras/backend.py#L3582" rel="noopener ugc nofollow" target="_blank">keras . back end . sparse _ category _ cross entropy</a>和<a class="ae mp" href="https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/keras/metrics.py#L587" rel="noopener ugc nofollow" target="_blank">sparse _ category _ accuracy</a>的实际实现，可以在TensorFlow repository上找到。别忘了在我的GitHub上下载本教程的源代码。</p><div class="mq mr fm fo ms mt"><a href="https://github.com/Tony607/keras_sparse_categorical_crossentropy" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab ej"><div class="mv ab mw cl cj mx"><h2 class="bd hv fv z el my eo ep mz er et ht dt translated">Tony 607/keras _ sparse _ category _ cross entropy</h2><div class="na l"><h3 class="bd b fv z el my eo ep mz er et ek translated">使用Keras sparse _ categorial _ cross entropy-Tony 607/Keras _ sparse _ categorial _ cross entropy的示例</h3></div><div class="nb l"><p class="bd b gc z el my eo ep mz er et ek translated">github.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh ja mt"/></div></div></a></div><p id="0c7a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><a class="ae mp" href="https://twitter.com/intent/tweet?url=https%3A//www.dlology.com/blog/how-to-use-keras-sparse_categorical_crossentropy/&amp;text=How%20to%20use%20Keras%20sparse_categorical_crossentropy" rel="noopener ugc nofollow" target="_blank">在Twitter上分享</a> <a class="ae mp" href="https://www.facebook.com/sharer/sharer.php?u=https://www.dlology.com/blog/how-to-use-keras-sparse_categorical_crossentropy/" rel="noopener ugc nofollow" target="_blank">在脸书分享</a></p></div><div class="ab cl ni nj hc nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="hn ho hp hq hr"><p id="3431" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><em class="np">原载于</em><a class="ae mp" href="https://www.dlology.com/blog/how-to-use-keras-sparse_categorical_crossentropy/" rel="noopener ugc nofollow" target="_blank"><em class="np">www.dlology.com</em></a><em class="np">。</em></p></div></div>    
</body>
</html>