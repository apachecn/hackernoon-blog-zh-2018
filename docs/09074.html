<html>
<head>
<title>Training and Deploying A Deep Learning Model in Keras MobileNet V2 and Heroku: A Step-by-Step Tutorial Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Keras MobileNet V2和Heroku培训和部署深度学习模型:分步教程第1部分</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/tf-serving-keras-mobilenetv2-632b8d92983c?source=collection_archive---------0-----------------------#2018-11-03">https://medium.com/hackernoon/tf-serving-keras-mobilenetv2-632b8d92983c?source=collection_archive---------0-----------------------#2018-11-03</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/></div><div class="ab cl ir is hc it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hn ho hp hq hr"><figure class="iz ja jb jc fq jd fe ff paragraph-image"><div class="fe ff iy"><img src="../Images/11bd78497b9ec40d9b82b044153da260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*LBjjmGofNJ_6vetddYAvIQ.jpeg"/></div></figure><h2 id="646e" class="jg jh hu bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd dt translated">为什么要在Keras + Heroku上训练和部署深度学习模型？</h2><p id="724e" class="pw-post-body-paragraph ke kf hu kg b kh ki kj kk kl km kn ko jr kp kq kr jv ks kt ku jz kv kw kx ky hn dt translated">本教程将逐步指导您如何训练和部署深度学习模型。在广泛搜索互联网后，我发现很难找到教程来从头到尾地构建和部署深度学习模型。虽然有一些关于这个过程的各个阶段的优秀文章(我在自己的深度学习之旅中使用过)，但我写这篇教程是为了填补我认为是一个重要的空白。</p><p id="b2ab" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">本教程将带您了解在TensorFlow中训练模型并将其部署到Heroku的整个过程——代码可从GitHub repo <a class="ae le" href="https://github.com/malnakli/ML/tree/master/tf_serving_keras_mobilenetv2" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="5214" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated"><strong class="kg hv">我们将要使用的技术的完整列表:</strong></p><ul class=""><li id="880d" class="lf lg hu kg b kh kz kl la jr lh jv li jz lj ky lk ll lm ln dt translated"><a class="ae le" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras 2.2 </a>是一个高级神经网络API，用Python编写，能够运行在TensorFlow之上。</li><li id="69da" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated"><a class="ae le" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow 1.11 </a> (TF)是一个面向研究和生产的开源机器学习库。TensorFlow是谷歌将深度学习的力量放到全球开发者手中的尝试。[1]</li><li id="fcbe" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated"><a class="ae le" href="https://scikit-image.org/" rel="noopener ugc nofollow" target="_blank"> Scikit-image 0.14 </a>是图像处理的算法集合。</li><li id="dfc2" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated"><a class="ae le" href="http://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank">Scikit-learn 0.19</a>Python中的机器学习。</li><li id="ae92" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated"><a class="ae le" href="https://pillow.readthedocs.io/en/4.1.x/" rel="noopener ugc nofollow" target="_blank"> Pillow 4.1 </a>是Python图像库</li><li id="d908" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated"><a class="ae le" href="https://www.python.org/downloads/release/python-367/" rel="noopener ugc nofollow" target="_blank"> Python 3.6 </a></li></ul></div><div class="ab cl ir is hc it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hn ho hp hq hr"><p id="fadf" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">TensorFlow、Keras(当然还有python)已经被越来越多的行业和研究团体所采用:</p><figure class="iz ja jb jc fq jd fe ff paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="fe ff lt"><img src="../Images/25ab8035026f24fd2525bb71c5d34a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zm4jefLTzTuS9edi9jbg2A.png"/></div></div><figcaption class="ly lz fg fe ff ma mb bd b be z ek"><em class="mc">Deep learning frameworks ranking computed by Jeff Hale, based on 11 data sources across 7 categories</em></figcaption></figure><p id="603d" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">由于易于使用，TensorFlow逐渐增加了它的功能得分——它为初学者和专家提供API，以便快速进入桌面、移动、web或云开发。关于张量流的简单介绍，请参见:(<a class="ae le" href="https://github.com/easy-tensorflow/easy-tensorflow#why-use-tensorflow" rel="noopener ugc nofollow" target="_blank"> easy-tensorflow </a>)</p><p id="309e" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">Keras网站解释了为什么它的用户采用率在2018年飙升:</p><ul class=""><li id="9c59" class="lf lg hu kg b kh kz kl la jr lh jv li jz lj ky lk ll lm ln dt translated">Keras是为人类设计的API，不是为机器设计的。这使得Keras易学易用；然而，这种易用性并不以降低灵活性为代价。</li><li id="0e36" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated">Keras模型可以很容易地部署在更多的平台上。</li><li id="a31d" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated">Keras支持TensorFlow、CNTK、Theano等多个后端引擎。</li><li id="8e5e" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated">Keras有<a class="ae le" href="https://keras.io/utils/#multi_gpu_model" rel="noopener ugc nofollow" target="_blank">内置支持多GPU数据并行</a>。</li></ul><p id="0f7b" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">由于培训和部署很复杂，我们希望保持简单，我将本教程分为两个部分:</p><h2 id="5257" class="jg jh hu bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd dt translated"><strong class="ak">第一部分:</strong></h2><ul class=""><li id="4b23" class="lf lg hu kg b kh ki kl km jr md jv me jz mf ky lk ll lm ln dt translated"><strong class="kg hv">准备好你的训练数据。</strong></li><li id="f0ce" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated"><strong class="kg hv">训练深度学习模型。</strong></li></ul><h2 id="e856" class="jg jh hu bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd dt translated"><a class="ae le" rel="noopener" href="/@malnakli/tf-serving-keras-mobilenetv2-c167b4b2bb25">第二部分:</a></h2><ul class=""><li id="4407" class="lf lg hu kg b kh ki kl km jr md jv me jz mf ky lk ll lm ln dt translated"><strong class="kg hv">用</strong> <a class="ae le" href="https://www.tensorflow.org/serving/" rel="noopener ugc nofollow" target="_blank"> <strong class="kg hv"> TensorFlow服侍</strong> </a> <strong class="kg hv">服侍你的模型。</strong></li><li id="9293" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated"><strong class="kg hv">部署到</strong> <a class="ae le" href="https://www.heroku.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kg hv">英雄</strong> </a> <strong class="kg hv">。</strong></li></ul></div><div class="ab cl ir is hc it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hn ho hp hq hr"><p id="dbe8" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated"><strong class="kg hv">为了从本博客中获益:</strong></p><ul class=""><li id="1090" class="lf lg hu kg b kh kz kl la jr lh jv li jz lj ky lk ll lm ln dt translated">你应该熟悉python。</li><li id="49a3" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated">你应该已经对什么是深度学习和神经网络有所了解了。</li></ul><h1 id="121a" class="mg jh hu bd ji mh mi mj jm mk ml mm jq mn mo mp ju mq mr ms jy mt mu mv kc mw dt translated">为培训准备数据</h1><p id="ee9c" class="pw-post-body-paragraph ke kf hu kg b kh ki kj kk kl km kn ko jr kp kq kr jv ks kt ku jz kv kw kx ky hn dt translated">解决深度学习问题最困难的部分之一是拥有一个准备充分的数据集。准备数据有三个一般步骤:</p><ol class=""><li id="5493" class="lf lg hu kg b kh kz kl la jr lh jv li jz lj ky mx ll lm ln dt translated"><strong class="kg hv">识别偏差</strong> —由于模型是在预定义的数据集上训练的，因此确保不引入偏差非常重要。</li></ol><blockquote class="my mz na"><p id="5f3a" class="ke kf nb kg b kh kz kj kk kl la kn ko nc lb kq kr nd lc kt ku ne ld kw kx ky hn dt translated">在<a class="ae le" href="https://en.wikipedia.org/wiki/Statistics" rel="noopener ugc nofollow" target="_blank">统计</a>中，<strong class="kg hv">抽样偏差</strong>是一种<a class="ae le" href="https://en.wikipedia.org/wiki/Bias" rel="noopener ugc nofollow" target="_blank">偏差</a>，其中样本的收集方式使得预期<a class="ae le" href="https://en.wikipedia.org/wiki/Statistical_population" rel="noopener ugc nofollow" target="_blank">人群</a>中的某些成员比其他人更不可能被包括在内。它产生一个<strong class="kg hv">有偏样本</strong>，一个群体(或非人为因素)的非随机样本，其中所有个体或实例被选择的可能性并不相等。如果不考虑这一点，结果可能会被错误地归因于所研究的现象，而不是归因于<a class="ae le" href="https://en.wikipedia.org/wiki/Sampling_(statistics)" rel="noopener ugc nofollow" target="_blank">取样</a>的方法。</p></blockquote><p id="c682" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">(汗学院为如何<a class="ae le" href="https://www.khanacademy.org/math/ap-statistics/gathering-data-ap/sampling-observational-studies/a/identifying-bias-in-samples-and-surveys" rel="noopener ugc nofollow" target="_blank">识别样本和调查中的偏差提供了很好的例子</a>。)</p><p id="e0f6" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">2.从数据中移除异常值。</p><blockquote class="my mz na"><p id="0ab7" class="ke kf nb kg b kh kz kj kk kl la kn ko nc lb kq kr nd lc kt ku ne ld kw kx ky hn dt translated">异常值是偏离其他数据观察值的极端值，它们可能表明测量值的可变性、实验误差或新奇性。换句话说，异常值是偏离样本总体模式的观察值。</p></blockquote><p id="a6ae" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated"><a class="ae le" href="http://A Brief Overview of Outlier Detection Techniques" rel="noopener ugc nofollow" target="_blank">更多关于离群点检测</a>。</p><p id="3069" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">3.将我们的数据集转换成机器可以理解的语言——数字。在本教程中，我将只关注转换数据集，因为其他两点需要一个博客来涵盖它们的全部细节。</p><p id="829b" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">为了准备一个数据集，你首先必须有一个数据集。我们将使用时尚-MNIST数据集，因为它已经针对分类问题进行了优化和标记。</p><p id="2816" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">(点击阅读更多关于时尚MNIST数据集<a class="ae le" href="https://arxiv.org/abs/1708.07747" rel="noopener ugc nofollow" target="_blank">。)</a></p><p id="7b35" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">时尚MNIST数据集包含70，000幅灰度(28x28px像素)图像，分为以下几类:</p><pre class="iz ja jb jc fq nf ng nh ni aw nj dt"><span id="86f6" class="jg jh hu ng b fv nk nl l nm nn">                +-----------+---------------------+<br/>                |   Label   |     Description     |     <br/>                +-----------+---------------------+<br/>                |     0     |     T-shirt/top     | <br/>                |     1     |     Trouser         | <br/>                |     2     |     Pullover        | <br/>                |     3     |     Dress           |<br/>                |     4     |     Coat            |<br/>                |     5     |     Sandal          |<br/>                |     6     |     Shirt           |<br/>                |     7     |     Sneaker         |<br/>                |     8     |     Bag             | <br/>                |     9     |     Ankle boot      |<br/>                +-----------+---------------------+</span></pre><p id="8dc2" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">幸运的是，大多数深度学习(DL)框架都支持现成的时尚MNIST数据集，包括Keras。要自己下载数据集并查看其他示例<a class="ae le" href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener ugc nofollow" target="_blank">，您可以链接到github repo —这里是</a>。</p><pre class="iz ja jb jc fq nf ng nh ni aw nj dt"><span id="3b84" class="jg jh hu ng b fv nk nl l nm nn">from keras.datasets.fashion_mnist import load_data</span><span id="e072" class="jg jh hu ng b fv no nl l nm nn"># Load the fashion-mnist train data and test data<br/>(x_train, y_train), (x_test, y_test) = load_data()</span><span id="c631" class="jg jh hu ng b fv no nl l nm nn"># output<br/>x_train shape: (60000, 28, 28) y_train shape: (60000,)<br/>x_test shape: (10000, 28, 28) y_test shape: (10000,)</span></pre><p id="5d8d" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">默认情况下，<code class="eh np nq nr ng b">load_data()function</code>返回训练和测试数据集。</p><p id="90fe" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">将数据分成训练集和测试集是非常重要的。</p><p id="7742" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated"><strong class="kg hv">训练数据</strong>:用于训练神经网络(NN)</p><p id="4dd2" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated"><strong class="kg hv">测试数据:</strong>用于在训练阶段通过调整和重新调整超参数来验证和优化神经网络的结果。</p><p id="17c4" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">超参数是在学习过程开始之前设置其值的参数。</p><p id="5187" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">在训练一个神经网络之后，我们针对我们的验证数据集运行训练好的模型，以确保该模型是一般化的并且不会过度拟合。</p><p id="d066" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated"><strong class="kg hv">什么是过度拟合？:</strong></p><p id="11e0" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">过度拟合意味着模型在根据训练数据进行测试时预测正确的结果，但在其他方面预测不准确。但是，如果模型预测训练数据的结果不正确，这就称为拟合不足。关于<a class="ae le" rel="noopener" href="/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76">过配合和欠配合</a>的进一步解释。</p><p id="3659" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">因此，我们使用验证数据集来检测过拟合或欠拟合。但是，大多数情况下，我们会多次训练模型，以便在训练和验证数据集中获得更高的分数。因为我们基于验证数据集结果重新训练模型，所以我们不仅会在训练数据集中，而且会在验证集中过度拟合。为了避免这种情况，我们使用了第三个在训练中从未使用过的数据集，即测试数据集。</p></div><div class="ab cl ir is hc it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hn ho hp hq hr"><p id="dfd3" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">这里是一些数据样本</p><figure class="iz ja jb jc fq jd fe ff paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="fe ff ns"><img src="../Images/a3cf88e3029d21ef0d249d1d45438421.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*HvpeUxR7tOJIoFrvA12UEA.png"/></div></div></figure><figure class="iz ja jb jc fq jd fe ff paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="fe ff ns"><img src="../Images/f8236f103798002f9a1dfaca7e78ce1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*Bfeby6lY8NR7pVPTBOb21A.png"/></div></div></figure></div><div class="ab cl ir is hc it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hn ho hp hq hr"><h2 id="d382" class="jg jh hu bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd dt translated">数据标准化</h2><pre class="iz ja jb jc fq nf ng nh ni aw nj dt"><span id="6f5d" class="jg jh hu ng b fv nk nl l nm nn">norm_x_train = x_train.astype('float32') / 255<br/>norm_x_test = x_test.astype('float32') / 255</span></pre><p id="6fe9" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">标准化数据维度，使它们具有大致相同的比例。一般来说，规范化使非常深的神经网络更容易训练，特别是在卷积和递归神经网络中。这里有一个很好的解释<a class="ae le" href="https://www.coursera.org/lecture/deep-neural-network/normalizing-activations-in-a-network-4ptp2" rel="noopener ugc nofollow" target="_blank">视频</a>和一篇<a class="ae le" rel="noopener" href="/@darrenyaoyao.huang/why-we-need-normalization-in-deep-learning-from-batch-normalization-to-group-normalization-d06ea0e59c17">文章</a></p><h2 id="09df" class="jg jh hu bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd dt translated">将标签(y_train和y_test)转换为一个热编码</h2><pre class="iz ja jb jc fq nf ng nh ni aw nj dt"><span id="ca5b" class="jg jh hu ng b fv nk nl l nm nn">from keras.utils import to_categorical</span><span id="fbfa" class="jg jh hu ng b fv no nl l nm nn">encoded_y_train = to_categorical(y_train, num_classes=10, dtype='float32')</span><span id="c0c5" class="jg jh hu ng b fv no nl l nm nn">encoded_y_test = to_categorical(y_test, num_classes=10, dtype='float32')</span></pre><p id="6fd1" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">一种流行的编码是将分类变量表示为二进制向量。<a class="ae le" href="https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/" rel="noopener ugc nofollow" target="_blank">这里有完整的解释</a>如果你想深入了解，有问题尽管问</p><h2 id="18de" class="jg jh hu bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd dt translated">调整图像大小并转换为3通道(RGB)</h2><p id="151f" class="pw-post-body-paragraph ke kf hu kg b kh ki kj kk kl km kn ko jr kp kq kr jv ks kt ku jz kv kw kx ky hn dt translated"><a class="ae le" href="https://keras.io/applications/#mobilenetv2" rel="noopener ugc nofollow" target="_blank"> MobileNet V2 </a>型号接受以下格式之一:<em class="nb"> (96，96)，(128，128)，(160，160)，(192，192)或(224，224)。</em>此外，图像必须是3通道(RGB)格式。因此，我们需要调整尺寸&amp;转换我们的图像。从(28 X 28)到(96 X 96 X 3)。</p><p id="0249" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">在我们所有的数据中运行前面的代码，可能会耗尽大量的内存资源；因此，我们将使用发电机。<a class="ae le" href="https://www.programiz.com/python-programming/generator" rel="noopener ugc nofollow" target="_blank"> Python生成器</a>是一个返回对象(迭代器)的函数，我们可以迭代这个对象(一次迭代一个值)。</p><figure class="iz ja jb jc fq jd"><div class="bz el l di"><div class="nt nu l"/></div></figure></div><div class="ab cl ir is hc it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hn ho hp hq hr"><h1 id="76e9" class="mg jh hu bd ji mh nv mj jm mk nw mm jq mn nx mp ju mq ny ms jy mt nz mv kc mw dt translated">训练深度学习模型</h1><p id="1cec" class="pw-post-body-paragraph ke kf hu kg b kh ki kj kk kl km kn ko jr kp kq kr jv ks kt ku jz kv kw kx ky hn dt translated">在我们对数据集进行分割、归一化和转换后，现在我们将训练一个模型。</p><p id="32d5" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">有许多训练模型的技术，我们将只涵盖其中一种，尽管我认为它是最重要的方法或策略之一，即迁移学习。</p><h2 id="c0ed" class="jg jh hu bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd dt translated">迁移学习</h2><p id="658b" class="pw-post-body-paragraph ke kf hu kg b kh ki kj kk kl km kn ko jr kp kq kr jv ks kt ku jz kv kw kx ky hn dt translated">深度学习中的迁移学习是指将知识从一个领域转移到一个相似的领域。在我们的例子中，我选择了MobileNet V2模型，因为它训练速度更快，体积更小。最重要的是，MobileNet使用<a class="ae le" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet数据集</a>进行预训练。</p><blockquote class="my mz na"><p id="4d68" class="ke kf nb kg b kh kz kj kk kl la kn ko nc lb kq kr nd lc kt ku ne ld kw kx ky hn dt translated">ImageNet是根据WordNet层次结构组织的图像数据集。WordNet中的每个有意义的概念，可能由多个单词或单词短语描述，称为“同义词集”或“同义词集”。WordNet中有超过100，000个同义词集，其中大多数是名词(80，000+)。在ImageNet中，我们的目标是平均提供1000个图像来说明每个synset。每个概念的图像都经过质量控制和人工注释。[2]</p></blockquote><p id="f3fa" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">因为我们的数据集是ImageNet数据集的一个子集，所以我们将把这个模型的知识转移到我们的数据集上。一篇很好的文章更详细地解释了这一点:<a class="ae le" href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">深度学习迁移学习的温和介绍</a></p><figure class="iz ja jb jc fq jd"><div class="bz el l di"><div class="nt nu l"/></div></figure><p id="d135" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">当使用迁移学习作为一种技巧时，理解下面的代码很重要:</p><pre class="iz ja jb jc fq nf ng nh ni aw nj dt"><span id="c8d1" class="jg jh hu ng b fv nk nl l nm nn">for layer in <!-- -->base_model<!-- -->.layers:</span><span id="8b8f" class="jg jh hu ng b fv no nl l nm nn"># trainable has to be false in order to freeze the layers<br/>  <!-- -->layer.trainable = False # or True</span></pre><p id="d70c" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">当使用一个预先训练好的模型(例如我们的例子中的MobileNetV2)时，你需要密切关注一个叫做<strong class="kg hv">的概念微调。</strong></p><blockquote class="my mz na"><p id="3b47" class="ke kf nb kg b kh kz kj kk kl la kn ko nc lb kq kr nd lc kt ku ne ld kw kx ky hn dt translated">一般来说，“微调”是指我们冻结除倒数第二层之外的预训练神经网络(在数据集A[例如ImageNet]上)的所有层的权重，并在数据集B[例如时尚-MNIST]上训练神经网络，只是为了学习倒数第二层上的表示。我们通常用我们选择的另一层来替换最后一层(softmax )(取决于新问题所需的输出数量)。[3]</p></blockquote><p id="f3f9" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">在我们的例子中，我们有10个类，所以我们有如下</p><pre class="iz ja jb jc fq nf ng nh ni aw nj dt"><span id="6ada" class="jg jh hu ng b fv nk nl l nm nn">output_tensor<!-- --> = Dense(10, activation='softmax')(op)</span></pre><p id="3cb6" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated"><strong class="kg hv">我们什么时候使用需要微调？</strong></p><ul class=""><li id="7850" class="lf lg hu kg b kh kz kl la jr lh jv li jz lj ky lk ll lm ln dt translated">当您有小数据集时(例如，几千个)</li><li id="4c46" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated">当用于训练预训练模型的数据集与新数据集非常相似或相同时。</li></ul><h2 id="c130" class="jg jh hu bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd dt translated">构建和培训</h2><pre class="iz ja jb jc fq nf ng nh ni aw nj dt"><span id="60c4" class="jg jh hu ng b fv nk nl l nm nn">from keras.optimizers import Adam</span><span id="9403" class="jg jh hu ng b fv no nl l nm nn">model = build_model()<br/>model.compile(optimizer=Adam(),<br/>              loss='categorical_crossentropy',<br/>              metrics=['categorical_accuracy'])</span></pre><p id="4587" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">现在我们编译这个模型。一些编译定义:</p><blockquote class="my mz na"><p id="e9ff" class="ke kf nb kg b kh kz kj kk kl la kn ko nc lb kq kr nd lc kt ku ne ld kw kx ky hn dt translated"><strong class="kg hv"> Adam </strong>优化算法是随机梯度下降的扩展，最近在计算机视觉和自然语言处理的深度学习应用中得到了更广泛的采用。[4]</p><p id="57e6" class="ke kf nb kg b kh kz kj kk kl la kn ko nc lb kq kr nd lc kt ku ne ld kw kx ky hn dt translated">损失函数(<strong class="kg hv">categorial _ cross entropy</strong>)是预测模型在预测预期结果方面表现如何的度量。[5]</p><p id="08fc" class="ke kf nb kg b kh kz kj kk kl la kn ko nc lb kq kr nd lc kt ku ne ld kw kx ky hn dt translated"><strong class="kg hv">category _ accuracy</strong>是一个度量函数，用于判断模型的性能。[6]</p></blockquote><pre class="iz ja jb jc fq nf ng nh ni aw nj dt"><span id="687e" class="jg jh hu ng b fv nk nl l nm nn">train_generator = load_data_generator(norm_x_train, encoded_y_train, batch_size=64)</span><span id="4883" class="jg jh hu ng b fv no nl l nm nn">model.fit_generator(<br/>    generator=train_generator,<br/>    steps_per_epoch=900,<br/>    verbose=1,<br/>    epochs=5)</span></pre><p id="1a1e" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">在训练任何深度学习模型时，理解以下内容是至关重要的。</p><blockquote class="my mz na"><p id="a7c5" class="ke kf nb kg b kh kz kj kk kl la kn ko nc lb kq kr nd lc kt ku ne ld kw kx ky hn dt translated"><strong class="kg hv">时期</strong>是指整个数据集仅通过神经网络前后传递一次。[7]</p><p id="fa2d" class="ke kf nb kg b kh kz kj kk kl la kn ko nc lb kq kr nd lc kt ku ne ld kw kx ky hn dt translated"><strong class="kg hv">批次大小</strong>是单个批次中训练样本的总数[7]。这与前面提到的python生成有很大关系</p><p id="30bc" class="ke kf nb kg b kh kz kj kk kl la kn ko nc lb kq kr nd lc kt ku ne ld kw kx ky hn dt translated">迭代次数(<strong class="kg hv"> steps_per_epoch </strong>)是完成一个epoch [7]所需的批数。</p></blockquote><p id="b788" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">要更详细地了解<a class="ae le" href="https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9" rel="noopener" target="_blank">，请阅读—时期与批量大小与迭代</a>。</p></div><div class="ab cl ir is hc it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hn ho hp hq hr"><p id="4540" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">经过5个时期的训练，准确率达到94%,但是在测试数据集中我们会做得怎么样:</p><pre class="iz ja jb jc fq nf ng nh ni aw nj dt"><span id="8162" class="jg jh hu ng b fv nk nl l nm nn">test_generator = load_data_generator(norm_x_test, encoded_y_test, batch_size=64)<br/>model.evaluate_generator(generator=test_generator,<br/>                         steps=900,<br/>                         verbose=1)</span></pre><p id="c674" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">对于花费在训练上的时间来说，86%似乎是合理的——1小时的CPU时间。</p><p id="1b37" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated"><strong class="kg hv">你可以尝试提高准确度的事情</strong></p><ul class=""><li id="120a" class="lf lg hu kg b kh kz kl la jr lh jv li jz lj ky lk ll lm ln dt translated">多选择<strong class="kg hv">历元</strong>，10为例。</li><li id="82e6" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated">尝试冻结图层。<strong class="kg hv">可训练</strong> =假。</li><li id="8c71" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated">选择更大的<strong class="kg hv"> batch_size </strong>，以128为例。</li><li id="dc2a" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky lk ll lm ln dt translated">选择不同的<strong class="kg hv">优化器</strong>，例如Nadam。然而，在这种情况下，更改优化器有时确实会带来很大的改进。</li></ul><h2 id="0bee" class="jg jh hu bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd dt translated">保存模型</h2><p id="a8ee" class="pw-post-body-paragraph ke kf hu kg b kh ki kj kk kl km kn ko jr kp kq kr jv ks kt ku jz kv kw kx ky hn dt translated">确保您保存了模型，因为我们将在下一部分中使用它</p><pre class="iz ja jb jc fq nf ng nh ni aw nj dt"><span id="ac02" class="jg jh hu ng b fv nk nl l nm nn">model_name = "tf_serving_keras_mobilenetv2"<br/>model.save(f"models/{model_name}.h5")</span></pre><h1 id="ec2f" class="mg jh hu bd ji mh mi mj jm mk ml mm jq mn mo mp ju mq mr ms jy mt mu mv kc mw dt translated">结论</h1><p id="86d6" class="pw-post-body-paragraph ke kf hu kg b kh ki kj kk kl km kn ko jr kp kq kr jv ks kt ku jz kv kw kx ky hn dt translated">我们已经为MobileNetV2模型准备了时尚数据集。此外，我们使用MobileNet模型作为迁移学习的基础模型。</p><p id="9f20" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">在<a class="ae le" rel="noopener" href="/@malnakli/tf-serving-keras-mobilenetv2-c167b4b2bb25">第2部分</a>中，我们将通过TensorFlow服务来准备要成为服务器的模型。然后我们将模型部署到Heroku。</p><p id="fd2c" class="pw-post-body-paragraph ke kf hu kg b kh kz kj kk kl la kn ko jr lb kq kr jv lc kt ku jz ld kw kx ky hn dt translated">问题/建议？—把它们留在下面的评论里。</p><h1 id="d6a3" class="mg jh hu bd ji mh mi mj jm mk ml mm jq mn mo mp ju mq mr ms jy mt mu mv kc mw dt translated">参考</h1><ol class=""><li id="ec9a" class="lf lg hu kg b kh ki kl km jr md jv me jz mf ky mx ll lm ln dt translated"><a class="ae le" href="https://flyyufelix.github.io/2016/10/03/fine-tuning-in-keras-part1.html" rel="noopener ugc nofollow" target="_blank">Keras中微调深度学习模型的综合指南</a></li><li id="ba07" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky mx ll lm ln dt translated">什么是ImageNet？</li><li id="0856" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky mx ll lm ln dt translated"><a class="ae le" href="https://www.linkedin.com/pulse/transfer-learning-fine-tuning-lets-discuss-arun-das/" rel="noopener ugc nofollow" target="_blank">迁移学习和微调:我们来讨论一下</a></li><li id="1986" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky mx ll lm ln dt translated"><a class="ae le" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">深度学习的Adam优化算法简介</a></li><li id="fb4c" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky mx ll lm ln dt translated"><a class="ae le" href="https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0" rel="noopener ugc nofollow" target="_blank">回归损失函数所有机器学习者都应该知道</a></li><li id="c472" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky mx ll lm ln dt translated"><a class="ae le" href="https://faroit.github.io/keras-docs/2.1.0/metrics/" rel="noopener ugc nofollow" target="_blank">指标的使用</a></li><li id="f5e3" class="lf lg hu kg b kh lo kl lp jr lq jv lr jz ls ky mx ll lm ln dt translated"><a class="ae le" href="https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9" rel="noopener" target="_blank">历元对批量对迭代</a></li></ol></div></div>    
</body>
</html>