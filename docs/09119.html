<html>
<head>
<title>HOW FAR HAVE WE‘VE COME IN TIME SERIES PREDICTION, FROM RNN TO LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从RNN到LSTM，我们在时间序列预测方面走了多远</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/how-far-have-we-gotten-in-time-series-prediction-from-rnn-to-lstm-5c9db64eb805?source=collection_archive---------14-----------------------#2018-11-05">https://medium.com/hackernoon/how-far-have-we-gotten-in-time-series-prediction-from-rnn-to-lstm-5c9db64eb805?source=collection_archive---------14-----------------------#2018-11-05</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div class="fe ff ir"><img src="../Images/b4fb8b52b7fb16ecb3bd00b8a7874fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*WNxN2ArLaGt0-Rm3tzWw1g.jpeg"/></div></figure><p id="d1ba" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">自从有时间以来，人类已经使用了许多方法来解决时间序列预测的问题。</p><p id="84cf" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">人类已经使用了包括统计在内的多种方式，因为我们认为利用这种力量可以打开无限的可能性。今天，许多算法被用来解决大量类似股票市场、预测股票价格、语音识别等问题。</p><p id="8571" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">随着深度学习热潮的开始，人们想到为什么不使用这些ML算法和神经网络来解决这个问题。</p><h1 id="8348" class="jw jx hu bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dt translated"><strong class="ak">进入递归神经网络！</strong></h1><figure class="kv kw kx ky fq iv fe ff paragraph-image"><div class="fe ff ku"><img src="../Images/ca2d043283b53148a46d6957371e98b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*eqHgDPrbURxagEShmKzpIg.jpeg"/></div></figure><p id="1cce" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">递归神经网络是在20世纪80年代开发的。一个<strong class="ja hv">递归神经网络</strong> ( <strong class="ja hv"> RNN </strong>)是一类<a class="ae kz" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">一个</a>人工神经网络，其中节点之间的连接形成一个<a class="ae kz" href="https://en.wikipedia.org/wiki/Directed_graph" rel="noopener ugc nofollow" target="_blank"> d </a>方向的序列。这允许它展示时间序列的时间动态行为。与前馈神经网络不同，rnn可以使用其内部状态(记忆)来处理输入序列。这使得它们非常适合作为理想的算法或神经网络类型。这种算法出现在许多架构中，例如，完全递归</p><h1 id="5fdc" class="jw jx hu bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dt translated"><strong class="ak">全循环架构</strong></h1><figure class="kv kw kx ky fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="fe ff la"><img src="../Images/b85eff139529e21e9fae112b56e21e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JluFN2aG9VvHP0I7kFVLqw.png"/></div></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">basic recurrent neural network</figcaption></figure><p id="2bce" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">基本rnn是由连续“层”组成的节点组成的网络，给定层中的每个节点通过单向连接与下一个连续层中的所有其他节点相连(这意味着它是完全连接的)。每个节点(神经元)都有一个时变的实值激活。每个连接(synapse)都有一个可修改的权重(在训练模型时找到)。节点可以是输入节点(从网络外部接收数据)、输出节点(产生结果)或隐藏节点(修改数据并将数据从输入路由到输出)。</p><p id="0fe6" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">来自前一层和前一层的隐藏层的输出被馈送到该层，因此它将前一隐藏状态的数据作为输入之一，因此它知道在前一层或状态中发生了什么</p><figure class="kv kw kx ky fq iv fe ff paragraph-image"><div class="fe ff lj"><img src="../Images/296afa45115bf815122b09377edfe33f.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*thDPvjFkYT7yhTgQSi9yQQ.png"/></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">a simpler representation of this architecture</figcaption></figure><h1 id="a956" class="jw jx hu bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dt translated"><strong class="ak">问题</strong></h1><p id="ed14" class="pw-post-body-paragraph iy iz hu ja b jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr lo jt ju jv hn dt translated">一切都有问题，这个神经网络也是如此。由于上面提到的RNN的特征，相同的特征具有一些缺点，并且最主要的一个是，当隐藏层和前一层的输出被馈送时，在离散的步骤(层)之后，先前步骤或层的记忆开始消失！这就是所谓的消失梯度。</p><p id="6842" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">在RNN的领导下，还有许多其他的架构，但是对这类问题(时间序列预测)最有影响力的是…..LSTM</p><h1 id="c70d" class="jw jx hu bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dt translated"><strong class="ak"> LSTM </strong></h1><p id="242f" class="pw-post-body-paragraph iy iz hu ja b jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr lo jt ju jv hn dt translated">LSTM(长短期记忆)是RNN的一个进化的、甚至更好的架构/子类别/部分，通过简单地添加LSTM细胞来实现。它有时也被称为LSTM网络(在本文中，我们简称它为LSTM)。</p><p id="dd59" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">它避免了<a class="ae kz" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失渐变问题</a>(上面也提到了，一个更专业的术语)。LSTM通常由称为“忘记”门的循环门扩充。LSTM防止反向传播的错误消失或爆炸。相反，错误可以通过空间中展开的无限数量的虚拟层反向流动。也就是说，LSTM可以学习需要记忆几千甚至几百万个离散时间点之前发生的事件的任务。</p><h1 id="159d" class="jw jx hu bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dt translated"><strong class="ak">结论</strong></h1><p id="481d" class="pw-post-body-paragraph iy iz hu ja b jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr lo jt ju jv hn dt translated">结论很简单，我们仍然有很长的路要走，在未来发展这类算法，以适应更多的任务，帮助世界变得更好</p></div></div>    
</body>
</html>