<html>
<head>
<title>Text Generation for Char LSTM models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Char LSTM模型文本生成</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/text-generation-for-char-lstm-models-685dc186e319?source=collection_archive---------8-----------------------#2018-11-08">https://medium.com/hackernoon/text-generation-for-char-lstm-models-685dc186e319?source=collection_archive---------8-----------------------#2018-11-08</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="46bf" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在笑话语料库上训练人物级语言模型。</p><p id="72f0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我决定尝试解决这个问题的方法，这是我在OpenAI的<a class="ae jq" href="https://openai.com/requests-for-research/" rel="noopener ugc nofollow" target="_blank">研究请求</a>博客上找到的。你可以在这里 看一下代码<a class="ae jq" href="https://github.com/coderbee/jokes" rel="noopener ugc nofollow" target="_blank"> <strong class="it hv">。这是在<a class="ae jq" href="https://hackernoon.com/tagged/pytorch" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>中写的，并受到了<a class="ae jq" href="https://www.fast.ai" rel="noopener ugc nofollow" target="_blank"> Fast.ai </a>关于实现RNN的“从零开始”的奇妙课程的极大启发。</strong></a></p><p id="a74d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">数据准备</strong>我开始使用<a class="ae jq" href="https://hackernoon.com/tagged/openai" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>提供的<a class="ae jq" href="https://github.com/taivop/joke-dataset" rel="noopener ugc nofollow" target="_blank">数据集</a>。数据被转换成小写字母，在第一次运行时，我选择了排名最高的笑话，单词长度不到200个。以下是遇到的所有令牌的示例:</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff jr"><img src="../Images/5ee710a7051e00e1c1e1005204f8476f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oLNPzpvy8H_vaQNpmR4Njg.png"/></div></div></figure><p id="22bc" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">*前面露骨的话！* </strong>这个特定的数据集具有明确的单词/内容，因此这些会出现在模型的输出预测中。另一个有趣的问题是从模型的输出中过滤掉不合适的单词——或者通过检查生成的输出中是否出现一组已知的显式单词，或者更好的是——通过构建一个干净/不干净的情感分类器！我还使用我在github上找到的更干净的数据集运行了结果。</p><p id="d895" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我尝试了LSTM的几个模型，主要是改变批量大小，隐藏层。随着反向传播char令牌(<em class="jp"> bptt </em>)数量的增加，训练char级别模型变得越来越困难。现在让我们主要关注文本生成部分。该模型一次输出一个字符，我们的目标是生成一串(看似易懂，可能有趣的)文本。有许多方法可以采用:</p><h2 id="4f18" class="kd ke hu bd kf kg kh ki kj kk kl km kn jc ko kp kq jg kr ks kt jk ku kv kw kx dt translated">贪婪的方法:</h2><p id="7a6d" class="pw-post-body-paragraph ir is hu it b iu ky iw ix iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo hn dt translated">这是最简单的方法，我们递归地选择最可能的下一个序列字符。为了猜测接下来的n个字符，我们需要在模型上运行n次推理，这是相当快的。然而，这种情况下的输出差异较小，容易陷入循环。似乎有一组概率很高的char序列，并且预测通常收敛于这些序列之一。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff ld"><img src="../Images/2a3b0e4ac4213e08c9a128292ebb0a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2oBBgXHwGWGq-pZYn0-e_A.png"/></div></div></figure><h2 id="fecd" class="kd ke hu bd kf kg kh ki kj kk kl km kn jc ko kp kq jg kr ks kt jk ku kv kw kx dt translated"><strong class="ak">顶K或多项式逼近:</strong></h2><p id="5536" class="pw-post-body-paragraph ir is hu it b iu ky iw ix iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo hn dt translated">这种方法从n个类别中模拟一个结果的发生，每个类别都有确定的发生概率。为了在输出中注入更多的多样性，我们将神经网络建模为概率事件，并观察在单次游戏中输出哪个标记，而不是选择最可能的字符。然而，观察结果，这种方法并不擅长产生完整的单词。单词内部固有的结构更容易被打破。一种替代方法是结合贪婪的最佳匹配方法，有节制地使用跨国分布</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff ld"><img src="../Images/64210b0df8dea181cac18e05d7bb2c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6cHEIrbDFy1L22zq52OD4Q.png"/></div></div></figure><h2 id="0c45" class="kd ke hu bd kf kg kh ki kj kk kl km kn jc ko kp kq jg kr ks kt jk ku kv kw kx dt translated">结合</h2><p id="0fd9" class="pw-post-body-paragraph ir is hu it b iu ky iw ix iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo hn dt translated">我们看到，使用贪婪方法总是会导致循环或重复的模式，而top-k方法的多项式会在char级别产生更多难以理解的单词。我试图通过使用贪婪的方法来生成字符，直到遇到一个空格(单词中的间隙)，从而获得两个世界的平衡。在这一点上，我使用了多项式生成器，这样在单词选择上就有了更多的多样性。这导致文本更加多样化，但整体上更容易理解。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff le"><img src="../Images/da68449a247b7d372249a2da8db2e8e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D2Sa9GuAaoYi6iNyhI0f5A.png"/></div></div></figure><h2 id="877f" class="kd ke hu bd kf kg kh ki kj kk kl km kn jc ko kp kq jg kr ks kt jk ku kv kw kx dt translated">波束搜索</h2><p id="f416" class="pw-post-body-paragraph ir is hu it b iu ky iw ix iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo hn dt translated">波束搜索是一种试图获得更多“最优结果”的方法，我们着眼于预测迭代中长于一个字符的序列。给定一个输入序列，我们不是预测下一个字符，而是预测下k个字符。这有助于我们在一组输出序列上找到一个更全局的解。在某种意义上，我们需要考虑所有可能的长度为k的输出字符束。并且需要一些比较的度量标准。</p><p id="7994" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在计算概率时，有几件事可以帮助我们:首先，使用贝叶斯规则，我们可以将获得特定输出序列的概率建模为各个条件概率序列的乘积。例如:给定输入序列‘太阳的预测’和‘得分’将是<em class="jp"> Score = P('a'| input = '太阳')*P('n'| input = '他太阳a')* P('d' | input = 'e孙安')。</em>其次，模型soft-max的输出通常是对数格式，这使得实现更容易，我们可以将对数值相加，而不是相乘。</p><p id="e732" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">下面显示了实现波束搜索过滤器所需的大部分代码。我们定义了一个数据结构<em class="jp">字母</em>来存储给定长度(比如说3)的所有可能的序列和它的分数的占位符。<em class="jp"> charscore </em>函数计算给定输入序列预测给定输出的概率。<em class="jp"> beam_search </em>函数遍历可能的序列，并计算每个序列的分数。<em class="jp"> beam_text </em>是迭代应用beam_search函数生成给定长度序列的函数。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff lf"><img src="../Images/248d99fd79d57a44ae995316c6292838.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6T0jnNoisDEr2Nx2C_fy3A.png"/></div></div></figure><p id="5155" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">波束搜索的这种简单实现需要相当长的时间(例如，产生15个样本需要8分钟)。下面是另一个例子，我们正在生成15束，或45个字符令牌。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff lg"><img src="../Images/1d0367f7d045ed9a960cb8c4f96609ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DyCIlQEVdSBQzEuhlWOnLQ.png"/></div></div></figure><p id="7557" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我相信通过在CPU上完全运行推理，并更好地优化<em class="jp"> beam_search </em>，可以大大降低这个速度。</p><h2 id="9bd9" class="kd ke hu bd kf kg kh ki kj kk kl km kn jc ko kp kq jg kr ks kt jk ku kv kw kx dt translated">未来的实验</h2><p id="28ae" class="pw-post-body-paragraph ir is hu it b iu ky iw ix iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo hn dt translated">下面是我尝试过的几个变种，还没有很大的成功。这些主要是由波束搜索的缓慢运行时间所驱动的。</p><p id="61f7" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">贪婪波束搜索:</strong>这类似于波束搜索，但是我们不是搜索所有序列，而是搜索“最可能”的序列。例如，对于3的波束宽度，我们可以在3个阶段的每一个阶段中查看前10个最可能的选择。所以我们每次迭代都要搜索10*10*10的波束。这当然大大减少了延迟问题，但结果很快陷入了一个循环，重复模式。例如，对于两个波束图案'.. '和“…”无休止地重复。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff lh"><img src="../Images/da944ca44c45786f2e8e38e3f755b1f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cs5bNHSif-WqLKKv0Seguw.png"/></div></div></figure><p id="0e8f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了增加更多的多样性，在计算波束得分时，我将各个概率相乘，而不是相加。结果更加多样化，但不一定更好。</p><pre class="js jt ju jv fq li lj lk ll aw lm dt"><span id="89e4" class="kd ke hu lj b fv ln lo l lp lq">[i, j, k], i_score * j_score * k_score]</span></pre><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff lr"><img src="../Images/d36c3acd6066fb25e3aa29467c6db41b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FjrUC6HVzYbmSoVgkMxzlQ.png"/></div></div></figure><h2 id="b615" class="kd ke hu bd kf kg kh ki kj kk kl km kn jc ko kp kq jg kr ks kt jk ku kv kw kx dt translated">参考资料:</h2><div class="ls lt fm fo lu lv"><a href="https://www.fast.ai" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab ej"><div class="lx ab ly cl cj lz"><h2 class="bd hv fv z el ma eo ep mb er et ht dt translated">fast.ai让神经网络再次变得不酷</h2><div class="mc l"><h3 class="bd b fv z el ma eo ep mb er et ek translated">fastai是第一个为所有最常用的深度学习库提供单一一致接口的深度学习库</h3></div><div class="md l"><p class="bd b gc z el ma eo ep mb er et ek translated">www.fast.ai</p></div></div><div class="me l"><div class="mf l mg mh mi me mj kb lv"/></div></div></a></div><div class="ls lt fm fo lu lv"><a href="https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab ej"><div class="lx ab ly cl cj lz"><h2 class="bd hv fv z el ma eo ep mb er et ht dt translated">如何实现用于自然语言处理的波束搜索解码器</h2><div class="mc l"><h3 class="bd b fv z el ma eo ep mb er et ek translated">自然语言处理任务，如字幕生成和机器翻译，涉及生成序列的…</h3></div><div class="md l"><p class="bd b gc z el ma eo ep mb er et ek translated">machinelearningmastery.com</p></div></div><div class="me l"><div class="mk l mg mh mi me mj kb lv"/></div></div></a></div><p id="1420" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">【https://github.com/coderbee/jokes/ T4】</p><p id="b337" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><a class="ae jq" href="https://github.com/amoudgl/short-jokes-dataset" rel="noopener ugc nofollow" target="_blank">短笑话数据集</a></p><figure class="js jt ju jv fq jw"><div class="bz el l di"><div class="ml mm l"/></div></figure></div></div>    
</body>
</html>