<html>
<head>
<title>Optimizing neural networks for production with Intel’s OpenVINO</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用英特尔的OpenVINO为生产优化神经网络</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/optimizing-neural-networks-for-production-with-intels-openvino-a7ee3a6883d?source=collection_archive---------8-----------------------#2018-11-29">https://medium.com/hackernoon/optimizing-neural-networks-for-production-with-intels-openvino-a7ee3a6883d?source=collection_archive---------8-----------------------#2018-11-29</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="9db2" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">在各种神经网络设计框架之间的圣战中，通常缺少一个重要步骤——制作生产就绪和优化的可交付产品。我已经测试了英特尔的OpenVINO优化系统，它看起来真的很有前途。</h2></div><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff jj"><img src="../Images/81510128c7064dccb2fd03ff035b8b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Tud0TqDvQwHWbxjRt3K5g.png"/></div></div></figure><h1 id="ed80" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">介绍</h1><p id="6862" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated">我在<a class="ae lj" href="https://skylum.com" rel="noopener ugc nofollow" target="_blank"> Skylum </a>工作——这家公司制作领先的人工智能照片编辑软件<a class="ae lj" href="https://skylum.com/luminar" rel="noopener ugc nofollow" target="_blank"> Luminar </a>、<a class="ae lj" href="https://skylum.com/aurorahdr" rel="noopener ugc nofollow" target="_blank">极光HDR </a>和<a class="ae lj" href="https://photolemur.com" rel="noopener ugc nofollow" target="_blank"> Photolemur </a>。目前，我们的系统使用Tensorflow作为神经计算引擎。为我们的客户提供优化的小型神经网络并不是一个简单的过程。有几件事你必须记住——张量流构建本身的大小、神经模型的大小以及它们的计算速度。TF在这方面并不完美。在所有优化之后，本机TensorFlow推理引擎的大小至少为60兆字节，并且优化边缘CPU计算的模型也不是完美的。TFLite是一种廉价的解决方案，而TF Mobile擅长于它所做的事情—针对移动CPU进行优化。任何主要的图书馆都没有涉及纯桌面优化领域，所以这是我感兴趣的一部分。</p><p id="2ca8" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">作为与<a class="ae lj" href="https://intel.com" rel="noopener ugc nofollow" target="_blank">英特尔</a>正在进行的合作努力的一部分，我正在积极地寻求将我们的推断转换为CPU的原生内容，并随时用于英特尔的build-it GPU，这是大多数英特尔芯片组目前拥有的。在这里，我报告我的测试结果，他们的<a class="ae lj" href="https://software.intel.com/en-us/openvino-toolkit" rel="noopener ugc nofollow" target="_blank"> OpenVINO优化包</a>。</p><h1 id="b1bf" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">技术细节</h1><p id="84c8" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated">我选择语义分割任务作为我们软件的一个非常有代表性的问题。例如，它为我们的<a class="ae lj" href="https://www.youtube.com/watch?v=ScbyevtYiiE" rel="noopener ugc nofollow" target="_blank">人工智能天空增强器</a>滤镜以及一系列即将到来的效果提供动力。显然，我不能报告我们实际的神经网络架构的结果，所以我选择了一个看起来足够接近的架构——deep lab V3+和启用了ASPP层的MobileNetV2 head。为了扩大范围，我选择了这种网络的两个版本——输出步幅为8和16，提供不同的输出分辨率。</p><p id="4fc8" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">为了生成这些图，我使用了官方的Google知识库，训练了模型，并使用官方提供的导出脚本导出了冻结的计算图，将输入和输出层分别命名为“input”和“segmap”。使用一个加载和评估冻结的简单python脚本测试了结果文件的TF性能。pb图。分辨率为513x513px，使用的CPU是工作频率为3.70GHz的英特尔酷睿i7–8700k CPU</p><p id="b842" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">使用OpenVINO测试同一型号的性能并不简单，我非常感谢来自英特尔的帮助。首先，我在我的测试系统上安装了OpenVino，按照官方的说明运行Ubuntu 16.04。首次尝试运行优化脚本失败。这是我得到的。</p><pre class="jk jl jm jn fq lp lq lr ls aw lt dt"><span id="1bd8" class="lu jw hu lq b fv lv lw l lx ly">python mo.py — input_model /data/1.pb — input_shape “(1,513,513,3)” — log_level=DEBUG — data_type FP32 — output segmap — input input — scale 1 — model_name test — framework tf — output_dir ./</span><span id="c3b4" class="lu jw hu lq b fv lz lw l lx ly">[ ERROR ]  Stopped shape/value propagation at "GreaterEqual" node.tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node GreaterEqual was passed int64 from add_1_port_0_ie_placeholder:0 incompatible with expected int32.</span></pre><p id="f07c" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">这里的问题是OpenVINO支持的一系列层。当DeepLab导出模型时，它实际上包括一系列预处理和后处理操作(调整大小、标准化等)，以尽可能容易地利用模型。它是使用内置的张量流运算来完成的，这种运算有时并不理想，而且写得很差。例如，<a class="ae lj" href="https://hackernoon.com/how-tensorflows-tf-image-resize-stole-60-days-of-my-life-aba5eb093f35" rel="noopener ugc nofollow" target="_blank">他们的调整大小功能和bug最近让我很头疼</a>。因此，我在<a class="ae lj" href="https://software.intel.com/en-us/comment/1930073#comment-1930073" rel="noopener ugc nofollow" target="_blank">英特尔的论坛</a>寻求帮助，他们的一名员工做了一个<a class="ae lj" href="https://github.com/FionaZZ92/OpenVINO" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a> +解释。</p><p id="9cbe" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">长话短说——您必须从TensorFlow图中删除预处理和后处理操作，只优化实际的神经网络操作。这实际上很有意义，因为用C++实现这些操作要快得多。</p><h1 id="5b4f" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">推理速度比较</h1><p id="cf84" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated">因此，在此之后，我用TF引擎和OpenVINO引擎对几个版本的TF图进行了比较。Fiona在GitHub repo中实际提出的是将TF图实际切割成3块——预处理、推理和后处理。用TF完成前后处理，推理外包给OpenVINO。</p><p id="611e" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">当我使用提供的脚本直接进行比较时，我发现执行时间没有差别。直觉告诉我这不可能是真的，所以我分别测量了这三个阶段。想法是启动两次TF引擎是一个耗时的操作，并且考虑到各种TF运算符的非理想优化的事实，可能会有开销。那碰巧是真的。</p><pre class="jk jl jm jn fq lp lq lr ls aw lt dt"><span id="4b01" class="lu jw hu lq b fv lv lw l lx ly">time cost preprocess: 0.0021 sec<br/>time cost to inference : 0.1979 sec<br/>time cost postprocess: 0.2642sec</span></pre><p id="1c08" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">使用Tensorflow进行后处理实际上比推理本身花费了更多的时间！</p><p id="353e" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">为了处理这种情况，我做了一个公平使用比较。我已经删除了在OpenCV和Numpy中实现的预处理和后处理阶段，并用TF和OpenVINO对相同的操作堆栈进行了比较。</p><p id="b863" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">重要的注意事项是优化的级别。我在这台机器上构建的Tensorflow是谷歌回购的官方TF 1.11，构建时没有AVX2和FMA优化。OpenVINO目前在其发行版中提供了两个级别的优化——AVX级别和SSE级别。对于我们的产品，就向后兼容性而言，我们最感兴趣的是SSE，但是我在这里报告的是两个版本。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff ma"><img src="../Images/8577bd4ac85d5829745ca71b311e87d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6KqQreO85y-d7mivIaK-cA.png"/></div></div><figcaption class="mb mc fg fe ff md me bd b be z ek">Inference speed comparison between TensorFlow and OpenVINO on a DeepLabV3+ / MobileNetV2 / ASPP head network.</figcaption></figure><p id="0454" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">正如你所看到的<strong class="kp hv"> OpenVINO在这场公平的面对面比较中提供了非常显著的速度提升</strong>，幅度为30–50%，大概是利用了他们对英特尔架构的了解。</p><h1 id="6801" class="jv jw hu bd jx jy jz ka kb kc kd ke kf ja kg jb kh jd ki je kj jg kk jh kl km dt translated">进一步研究</h1><p id="1ed5" class="pw-post-body-paragraph kn ko hu kp b kq kr iv ks kt ku iy kv kw kx ky kz la lb lc ld le lf lg lh li hn dt translated">我仍然有兴趣看看在其他用例中的性能，并可能尝试更“纯粹”的东西，如没有预处理和后处理的PB模型，以及更传统的架构，如ResNet/DenseNet甚至VGG。希望这能让我们对OpenVINO的能力有所了解。</p><p id="2586" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">我对使用英特尔的内置GPU测试推理非常感兴趣，但OpenCL在不同平台上的可用性是一个问题，所以我将推迟这个问题，直到有更好的一致的多平台生产就绪解决方案。此外，FT16和INT8推理这一非常重要的话题也不在这里讨论。我无法以半精度运行相同的实验，也没有发现OpenVINO中提到INT8量化优化。</p><p id="59a5" class="pw-post-body-paragraph kn ko hu kp b kq lk iv ks kt ll iy kv kw lm ky kz la ln lc ld le lo lg lh li hn dt translated">而且，把OpenVINO和TensorRT做个比较会很酷。你的掌声和关注将激励我做进一步的研究，敬请期待！</p></div></div>    
</body>
</html>