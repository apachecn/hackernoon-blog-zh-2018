<html>
<head>
<title>Self-Play</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自娱自乐</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/self-play-1f69ceb06a4d?source=collection_archive---------15-----------------------#2018-12-11">https://medium.com/hackernoon/self-play-1f69ceb06a4d?source=collection_archive---------15-----------------------#2018-12-11</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="d6ff" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果你对人工智能感兴趣，你可能已经读过关于AlphaZero、T2、西尔弗等人的文章。艾尔。】。这个算法学会了在超人类水平“白板”下围棋、国际象棋和日本象棋。换句话说，AlphaZero的游戏代理没有查看玩得好的游戏或好棋的例子。它也不知道可以帮助它评估头寸的试探法(例如，“车比卒值钱”)。它只是和自己玩了几百万次游戏。</p><p id="289e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这对我来说很神秘。如果只是和自己玩，新的信息从哪里来？它怎么知道自己做得好不好？如果我和自己下一盘棋，如果我赢了自己，我应该说我做得很好吗？但我战胜自己的同时，也输给了自己。我怎么知道和别人比赛我会不会做得很好？</p><p id="f9ef" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">据我所知，自我表演并没有被完全理解。例如，没有关于性能的保证，也没有公认的方法。但是人们已经多次成功了。</p><p id="ead5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">下面我将讨论一些著名的自我游戏的例子，研究人员在构建以这种方式学习的代理时遇到的问题，以及使其工作所需的条件。</p></div><div class="ab cl jq jr hc js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hn ho hp hq hr"><p id="2ddf" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">自弹自唱的例子</strong></p><p id="b648" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">自玩在游戏代理的培训中有着悠久的历史。代理人已经被构造来玩跳棋，西洋双陆棋，国际象棋，围棋。</p><p id="79cd" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">1959年，亚瑟·塞缪尔编写了一个学习玩跳棋的程序。该系统从“棋盘配置、游戏结果”等样本中学习。样本是从人类游戏和自我游戏中收集的。为了决定下一步行动，该系统使用了多种方法来评估棋盘:(I)有限深度的修剪(alpha-beta [ <a class="ae jp" href="https://en.wikipedia.org/wiki/Alpha–beta_pruning" rel="noopener ugc nofollow" target="_blank"> Wikipedia </a> ])搜索树，(ii)查找表(由原始样本组成)，以及(iii)由人工设计的特征组成的评估函数。从样本中学习(iii)的参数。</p><p id="6ebd" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">1992年，Gerald Tesauro [ <a class="ae jp" href="http://papers.nips.cc/paper/465-practical-issues-in-temporal-difference-learning.pdf" rel="noopener ugc nofollow" target="_blank"> Tesauro </a> ]创造了TD-Gammon，这是一个训练有素的神经网络，可以玩世界冠军级别的双陆棋。这个NN学会了完全通过自己玩来评估一个双陆棋棋盘配置<em class="jx">。它的搜索树更像是一个搜索树桩——根据版本的不同，只由一层或两层组成(一层<em class="jx">层</em>是单个玩家的移动)。西洋双陆棋和西洋跳棋(或国际象棋或围棋，就此而言)之间的一个重要区别是，西洋双陆棋使用骰子，这在游戏中引入了随机性。</em></p><p id="985c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">Deepmind的AlphaZero [ <a class="ae jp" href="http://science.sciencemag.org/content/362/6419/1140" rel="noopener ugc nofollow" target="_blank"> Silver等。艾尔。通过学习比跳棋复杂得多的国际象棋和比国际象棋复杂得多的围棋，将这一研究路线推向了极致。更准确地说，跳棋有10个⁴棋盘状态，象棋有10⁵ [ </a><a class="ae jp" href="https://www.quora.com/How-many-possible-configurations-of-pieces-on-a-chessboard-are-there-How-much-memory-would-be-required-to-store-the-entire-catalogue-of-games-played-from-all-possible-moves" rel="noopener ugc nofollow" target="_blank">卡斯帕</a>]，围棋有10个⁷⁰ [ <a class="ae jp" href="https://tromp.github.io/go/legal.html" rel="noopener ugc nofollow" target="_blank">特罗姆普</a>]。(实际上，checkers是如此的“小”，以至于它被精确地解决了[ <a class="ae jp" href="http://sciencenetlinks.com/science-news/science-updates/checkers-solved/" rel="noopener ugc nofollow" target="_blank"> Hirshon </a> ]！)</p><p id="4109" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">AlphaZero的博弈代理包括(I)蒙特卡罗树搜索[ <a class="ae jp" href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search" rel="noopener ugc nofollow" target="_blank">维基百科</a> ](比alpha-beta剪枝更高效)，以及(ii)由深度神经网络组成的评估函数。AlphaZero完全靠自己玩来学习。没有人类玩耍的样本。没有手动设计的功能。AlphaZero (AlphaGo)的前身，在2016年击败了排名第一的职业围棋选手Lee Sedol[<a class="ae jp" href="https://deepmind.com/research/alphago/" rel="noopener ugc nofollow" target="_blank">deep mind</a>]。</p><p id="9153" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">OpenAI目前正在开发一个深度神经网络代理——实际上是一个五代理团队——它玩Dota 2 [ <a class="ae jp" href="https://openai.com/five/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>。最终目标是打败世界顶级职业球队。Dota 2的状态空间是巨大的、连续的，而且只是部分可观测的。Dota 2的游戏状态由20000个维度组成。相比之下，国际象棋的8×8正方形棋盘有12个不同的棋子(768位)，围棋的19×19正方形棋盘有2个棋子(722位)。在国际象棋和围棋中，双方都可以看到整个棋盘。在Dota 2中，整个游戏状态只有一小部分是可见的。将所有这些与大约1000个动作结合起来——五个玩家中的每一个<em class="jx">——你就有了一个极其复杂的游戏。OpenAI正在通过自我游戏[ <a class="ae jp" href="https://towardsdatascience.com/the-science-behind-openai-five-that-just-produced-one-of-the-greatest-breakthrough-in-the-history-b045bcdc2b69" rel="noopener" target="_blank">罗德里格兹</a> ]解决这个游戏。</em></p></div><div class="ab cl jq jr hc js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hn ho hp hq hr"><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div class="fe ff jy"><img src="../Images/ad8ab57f8a9ad9ac7b7067e74bce80a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*zDSn2eMEyIm7H0Ni2Pwe1Q.png"/></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek">Optimal Tic-tac-toe for player X</figcaption></figure><p id="6327" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">通过构建学习</strong></p><p id="1502" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了试图真正理解自我游戏，我提出了以下问题:训练一个神经网络通过自我游戏完美地玩井字游戏<em class="jx"/>，并用进化策略做到这一点。井字游戏是一个简单的小游戏(9 x 9棋盘，2个棋子)，可以使用minimax*算法快速准确地解决。这种简单性使我能够在便宜的硬件上快速运行实验。</p><p id="2569" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">所谓“完美”，我的意思是我希望它连续100次击败minimax代理(因此，也许“真的，<em class="jx">真的</em>好”会比“完美”更好)。此外，我希望随着训练的进行，代理人能够继续赢得100场比赛——让我相信，无论它从自我游戏中学到了什么，都会可靠地引导它走向完美的游戏。为了评估训练过程，我周期性地暂停训练，玩了100次NN代理对minimax(来自Dwayne Crooks的<a class="ae jp" href="https://hackernoon.com/tagged/python" rel="noopener ugc nofollow" target="_blank"> Python </a>包<strong class="it hv"> xo </strong>)游戏，记录了NN的损失次数。</p><p id="17b2" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我选择用进化策略(特别是CMA-ES [ <a class="ae jp" href="https://arxiv.org/abs/1604.00772" rel="noopener ugc nofollow" target="_blank"> Hansen </a>)而不是反向传播来训练神经网络(NN)，部分原因是我最近阅读了OpenAI的进化策略，作为强化学习的可扩展替代方案[ <a class="ae jp" href="https://arxiv.org/abs/1703.03864" rel="noopener ugc nofollow" target="_blank"> Salimans等人。艾尔。</a> ]和优步·艾关于深度神经进化的论文[ <a class="ae jp" href="https://eng.uber.com/deep-neuroevolution/" rel="noopener ugc nofollow" target="_blank">斯坦利&amp;克鲁恩</a> ]，部分原因是我有一台多CPU的电脑，没有GPU，部分原因是它会迫使算法与上面讨论的算法有足够的不同，以至于我真的必须钻研自我游戏的想法才能去任何地方。</p><p id="4a37" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">*<em class="jx">Minimax[</em><a class="ae jp" href="https://en.wikipedia.org/wiki/Minimax" rel="noopener ugc nofollow" target="_blank"><em class="jx">Wikipedia</em></a><em class="jx">]从给定的棋盘配置开始计算每一种可能的游戏实现，然后确定哪一步棋具有最好的预期结果。上面讨论的其他方法(阿尔法-贝塔剪枝，蒙特卡罗树搜索)是对极大极小的近似。它们被用在跳棋、国际象棋和围棋中，因为这些游戏有太多的实现，无法在实际的时间内计算出来。</em></p></div><div class="ab cl jq jr hc js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hn ho hp hq hr"><p id="a48d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">自玩健身</strong></p><p id="731e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">要用CMA-ES优化神经网络的参数，需要指定一个适应度函数。它是你赋予一组神经网络参数的值或质量。作为一个例子，如果你不关心自己的发挥，你可以只对一个极小(完美)的球员发挥NN，比如说，100次，并计算失败的次数作为适应度(嗯，<em class="jx">负</em>适应度)。然后要求CMA-ES找出使损失数最小的NN参数。我试过了。它工作了。它产生了一个NN玩家，总是跟minimax打平。然而，要通过“自我游戏”来学习，在不知道极大极小解或任何其他策略提示的情况下，神经网络应该与什么游戏来确定其适应性呢？</p><p id="4fda" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">经过大量实验，我发现这很有效:为NN玩家构建一个“全知代理”来与之竞争。全知代理(OA)之所以全知，是因为它嵌入了上一代“x favorite”NN的副本，并且知道它会对任何棋盘配置做出什么举动。在下一步棋之前，OA模拟游戏对抗xfavorite，看看哪一步棋最适合对抗它。</p><p id="9ac7" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">通过了解xfavorite的移动，OA可以在实际上不知道如何玩井字游戏或不做任何树搜索的情况下玩得比xfavorite更好。这很简单。</p><p id="76d0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了测试OA的能力，我把它放在一边，嵌入了minimax播放器(而不是xfavorite NN)，并让它与minimax播放器进行了100场比赛。OA每次都能抽到minimax玩家。知道了这些，我们可以说，“如果NN收敛到minimax play，OA就会画出来”。换句话说，优化不会因为OA不够擅长井字游戏而卡壳。不过，它可能会因为其他原因而停滞不前。</p><p id="0676" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">请注意，开放存取本身对于实际的竞技游戏并没有什么用处，因为你永远无法在开放存取中嵌入一个对手，并称之为公平或有意义的游戏。但这并不意味着你不能训练一个对抗OA的特工。这就是我尝试过的——而且奏效了。为了跟踪它的进度，我每10分钟暂停一次训练，把当前的xfavorite对着<strong class="it hv"> xo </strong>的minimax播放器玩100次，记录下输的次数:</p><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div class="fe ff kk"><img src="../Images/6b2ec81c164fdcd3addd8b32b1bc6e76.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*7-2mrzDaZ_dBQYY6L1zS5g.png"/></div></figure><p id="f499" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">(是的，第四轮是最好的。不过，你可能会看到所有的<a class="ae jp" href="https://dspub99.github.io/betazero/betazero.html" rel="noopener ugc nofollow" target="_blank"/>。)请注意，随着优化的继续，它是如何达到零损耗并且<em class="jx">保持在那里</em>。对抗OA的训练迫使代理人走向完美的、极小的游戏。</p><p id="bb6b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然而，更有启发性的是，所有的事情都没有成功。</p></div><div class="ab cl jq jr hc js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hn ho hp hq hr"><p id="a60d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">问题</strong></p><p id="b780" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在这种情况下，训练一个神经网络对抗它自己——字面上的自我游戏——是没有意义的。适应度函数是什么？这在AlphaZero中是有意义的，因为该算法记录游戏，并通过监督学习来学习移动的质量。但是如果NN自己玩，它同时赢和输。你怎么得分？</p><p id="4678" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">通过对抗上一代的best can结果来训练一个NN，就是罗尚博问题:NN学习一个策略B来击败策略A，成为“冠军”。然后一个新的NN学习了一个策略C打败了B，然后另一个学习了A打败了C，把A，B，C想成石头，布，剪刀，就有道理了。这种策略循环被证明存在于西洋双陆棋中。艾尔。 ]并由OpenAI在术语“策略崩溃”<a class="ae jp" href="https://blog.openai.com/openai-five/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>下讨论(afaict)。</p><p id="dde5" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">使用几个前冠军国家队的“名人堂”可能会缓解罗沙莫问题。OpenAI在OpenAI Five中使用了它，但我在这个问题上尝试的几个版本都没有任何运气。</p><p id="4bcb" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">用大量numSims卡对抗OA的训练不起作用。OA非常好，以至于适应度函数很少有梯度。它只是失去了，失去了，失去了。没有那么多的信息。最好的num sim实际上是num sim = 1——稍微偏向“更好”,有很多探索的噪音。</p></div><div class="ab cl jq jr hc js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hn ho hp hq hr"><p id="96b1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">解决方案</strong></p><p id="5ef6" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">看起来，如果你想通过自我游戏来训练一个代理，你需要创建一个算法来很好地探索状态空间(棋盘配置)，并且可能有一些机制来鼓励单调的改进。</p><p id="4f4f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">AlphaGo的代理人在探索过程中使用了随机性，并雇佣MCTS总是寻找更好的走法。OpenAI Five玩的游戏有天然的随机性，并使用名人堂来防止回溯。TD-Gammon将掷骰子作为探索的来源。目前还不清楚TD-Gammon是如何保持进步的，但是。艾尔。 ]研究了一个假设，即双陆棋的动态自然有助于此。</p></div><div class="ab cl jq jr hc js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hn ho hp hq hr"><p id="39a1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">结论</strong></p><p id="30f3" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">自我发挥是一个令人兴奋的想法，因为它有望让工程师不仅不必指定问题的<em class="jx">解决方案</em>(正如参数优化所做的那样)，甚至还可以指定<em class="jx">目标</em>。这是一个比我们通常在研究强化学习时考虑的更高层次的自主性。</p><p id="b1f2" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">虽然自我游戏似乎没有被完全理解，但它肯定是有效的。这应该是一个可以看很多年的有趣话题。</p><p id="0276" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">感谢您的阅读。如果你想了解更多关于自我游戏的内容，请查看整篇文章中的参考链接。如果你对训练井字游戏代理感兴趣，可以使用<a class="ae jp" href="https://github.com/dspub99/betazero" rel="noopener ugc nofollow" target="_blank"> Python源代码</a>，以及关于其工作方式的更详细的文章<a class="ae jp" href="https://dspub99.github.io/betazero/betazero.html" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="jz ka kb kc fq kd"><div class="bz el l di"><div class="kl km l"/></div></figure></div></div>    
</body>
</html>