<html>
<head>
<title>Text Summarization Using Keras Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Keras模型的文本摘要</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/text-summarization-using-keras-models-366b002408d9?source=collection_archive---------0-----------------------#2018-12-20">https://medium.com/hackernoon/text-summarization-using-keras-models-366b002408d9?source=collection_archive---------0-----------------------#2018-12-20</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/4583707d1155706ea868e78cfb0a6e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ap4ybx0bZouNL8kSa2SvIg.jpeg"/></div></div></figure><p id="bfdf" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">了解如何总结Rajdeep Dua和Manpreet Singh Ghotra撰写的本文，Raj deep Dua目前领导Salesforce India的开发人员关系团队，Manpreet Singh gho tra目前在Salesforce开发机器学习平台/API。</p><p id="5ee6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">文本摘要是自然语言处理(NLP)中的一种方法，用于生成参考文档的简短而精确的摘要。手动生成大型文档的摘要是一项非常困难的任务。使用机器学习技术的文本摘要仍然是一个活跃的研究课题。在讨论文本摘要以及如何进行之前，这里有一个摘要的定义。</p><p id="ab9f" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">摘要是从一个或多个文本生成的文本输出，以更短的形式传达来自原始文本的相关信息。自动文本摘要的目标是使用语义将源文本转换成较短的版本。</p><p id="5317" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">最近，已经开发了各种方法用于使用NLP技术的自动文本摘要，并且它们已经在各种领域中广泛实现。一些例子包括搜索引擎创建用于文档预览的摘要，以及新闻网站产生新闻主题的综合描述，通常作为标题，以帮助用户浏览。</p><p id="ccc2" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了有效地总结文本，深度学习模型需要能够理解文档，并识别和提取重要信息。这些方法极具挑战性且非常复杂，尤其是当文档长度增加时。</p><h1 id="d51c" class="kb kc hu bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky dt translated">评论的文本摘要</h1><p id="402d" class="pw-post-body-paragraph jc jd hu je b jf kz jh ji jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz hn dt translated">本文将向您展示如何解决文本摘要问题，为世界上最大的电子商务平台亚马逊上销售的精美食品的产品评论创建相关摘要。评论包括产品和用户信息、评级和纯文本评论。它还包括所有其他亚马逊类别的评论。通过定义编码器-解码器<strong class="je hv">递归神经网络</strong> ( <strong class="je hv"> RNN </strong>)架构，开发一个基本的字符级<strong class="je hv">序列对序列</strong> ( <strong class="je hv"> seq2seq </strong>)模型。</p><p id="dae7" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">本文中使用的数据集可以在https://www.kaggle.com/snap/amazon-fine-food-reviews/<a class="ae le" href="https://www.kaggle.com/snap/amazon-fine-food-reviews/" rel="noopener ugc nofollow" target="_blank">找到。您的数据集将包括以下内容:</a></p><p id="2d1b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">568，454条评论</p><p id="552a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">256，059个用户</p><p id="1c46" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">74，258件产品</p><h2 id="41e2" class="lf kc hu bd kd lg lh li kh lj lk ll kl jn lm ln kp jr lo lp kt jv lq lr kx ls dt translated">怎么做…</h2><p id="6093" class="pw-post-body-paragraph jc jd hu je b jf kz jh ji jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz hn dt translated">您将开发一个建模管道和编码器-解码器架构，试图为一组给定的评论创建相关的摘要。建模管道使用通过Keras functional API编写的RNN模型。管道还使用各种数据操作库。</p><p id="d253" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">编码器-解码器架构被用作构建用于序列预测的rnn的一种方式。它包括两个主要部分:编码器和解码器。编码器读取完整的输入序列，并将其编码为内部表示，通常是固定长度的向量，称为上下文向量。另一方面，解码器从编码器读取编码的输入序列，并生成输出序列。可以使用各种类型的编码器，更常见的是使用双向rnn，如LSTMs。</p><h2 id="e91c" class="lf kc hu bd kd lg lh li kh lj lk ll kl jn lm ln kp jr lo lp kt jv lq lr kx ls dt translated">数据处理</h2><p id="9da2" class="pw-post-body-paragraph jc jd hu je b jf kz jh ji jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz hn dt translated">将正确的数据作为神经架构的输入，用于训练和验证，这一点至关重要。确保数据具有有用的比例和格式，并且包含有意义的要素。这将导致更好和更一致的结果。</p><p id="e390" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">采用以下工作流程进行数据预处理:</p><p id="1463" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">1.使用pandas加载数据集</p><p id="0003" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">2.将数据集分割成机器学习的输入和输出变量</p><p id="7873" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">3.对输入变量应用预处理转换</p><p id="930f" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">4.汇总数据以显示变化</p><p id="37d7" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在一步一步开始:</p><p id="15d6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">1.从导入重要的包和数据集开始。使用<code class="eh lt lu lv lw b">pandas</code>库加载数据并检查数据集的形状——它包括10个特征和500万个数据点:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="6dec" class="lf kc hu lw b fv mf mg l mh mi">import pandas as pd</span><span id="2c86" class="lf kc hu lw b fv mj mg l mh mi">import re</span><span id="ebbf" class="lf kc hu lw b fv mj mg l mh mi">from nltk.corpus import stopwords</span><span id="bb5b" class="lf kc hu lw b fv mj mg l mh mi">from pickle import dump, load</span><span id="c356" class="lf kc hu lw b fv mj mg l mh mi">reviews = pd.read_csv("/deeplearning-keras/ch09/summarization/Reviews.csv")</span><span id="d34e" class="lf kc hu lw b fv mj mg l mh mi">print(reviews.shape)</span><span id="b199" class="lf kc hu lw b fv mj mg l mh mi">print(reviews.head())</span><span id="53fd" class="lf kc hu lw b fv mj mg l mh mi">print(reviews.isnull().sum())</span></pre><p id="9845" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">输出如下所示:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="8169" class="lf kc hu lw b fv mf mg l mh mi">(568454, 10)Id 0</span><span id="3061" class="lf kc hu lw b fv mj mg l mh mi">ProductId 0</span><span id="2098" class="lf kc hu lw b fv mj mg l mh mi">UserId 0</span><span id="e728" class="lf kc hu lw b fv mj mg l mh mi">ProfileName 16</span><span id="3b45" class="lf kc hu lw b fv mj mg l mh mi">HelpfulnessNumerator 0</span><span id="4d1f" class="lf kc hu lw b fv mj mg l mh mi">HelpfulnessDenominator 0</span><span id="adee" class="lf kc hu lw b fv mj mg l mh mi">Score 0</span><span id="c60e" class="lf kc hu lw b fv mj mg l mh mi">Time 0</span><span id="0250" class="lf kc hu lw b fv mj mg l mh mi">Summary 27</span><span id="289d" class="lf kc hu lw b fv mj mg l mh mi">Text 0</span></pre><p id="a7dc" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">2.删除空值和不需要的功能，如以下代码片段所示:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="9044" class="lf kc hu lw b fv mf mg l mh mi">reviews = reviews.dropna()</span><span id="3d05" class="lf kc hu lw b fv mj mg l mh mi">reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator', 'Score','Time'], 1)</span><span id="7c96" class="lf kc hu lw b fv mj mg l mh mi">reviews = reviews.reset_index(drop=True) print(reviews.head())</span><span id="fbb4" class="lf kc hu lw b fv mj mg l mh mi">for i in range(5):</span><span id="193a" class="lf kc hu lw b fv mj mg l mh mi">print("Review #",i+1)</span><span id="89b4" class="lf kc hu lw b fv mj mg l mh mi">print(reviews.Summary[i])</span><span id="caa1" class="lf kc hu lw b fv mj mg l mh mi">print(reviews.Text[i])</span><span id="9c5a" class="lf kc hu lw b fv mj mg l mh mi">print()</span></pre><p id="acf2" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">输出如下所示:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="8cde" class="lf kc hu lw b fv mf mg l mh mi"><strong class="lw hv">Summary Text</strong></span><span id="bf51" class="lf kc hu lw b fv mj mg l mh mi">0 Good Quality Dog Food I have bought several of the Vitality canned d...</span><span id="ad27" class="lf kc hu lw b fv mj mg l mh mi">1 Not as Advertised Product arrived labeled as Jumbo Salted Peanut...</span><span id="6daf" class="lf kc hu lw b fv mj mg l mh mi">2 "Delight," says it all This is a confection that has been around a fe...</span><span id="c2d9" class="lf kc hu lw b fv mj mg l mh mi">3 Cough Medicine If you are looking for the secret ingredient i...</span><span id="b09e" class="lf kc hu lw b fv mj mg l mh mi"><strong class="lw hv">Review # 1</strong></span><span id="66ca" class="lf kc hu lw b fv mj mg l mh mi">Not as Advertised - Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as "Jumbo".</span><span id="e203" class="lf kc hu lw b fv mj mg l mh mi"><strong class="lw hv">Review # 2</strong></span><span id="0da4" class="lf kc hu lw b fv mj mg l mh mi">"Delight" says it all - This is a confection that has been around a few centuries. It is a light, pillowy citrus gelatin with nuts - in this case, Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar. And it is a tiny mouthful of heaven. Not too chewy, and very flavorful. I highly recommend this yummy treat. If you are familiar with the story of C.S. Lewis' "The Lion, The Witch, and The Wardrobe" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.</span><span id="3f0b" class="lf kc hu lw b fv mj mg l mh mi"><strong class="lw hv">Review # 3</strong></span><span id="73fe" class="lf kc hu lw b fv mj mg l mh mi">Cough Medicine - If you are looking for the secret ingredient in Robitussin I believe I have found it. I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda. The flavor is very medicinal.</span></pre><p id="cef7" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">根据定义，缩写是将两个单词组合成简化形式，省略一些内部字母并使用撇号。可以从<a class="ae le" href="http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python" rel="noopener ugc nofollow" target="_blank">http://stack overflow . com/questions/1979 01 88/expanding-English-language-contractions-in-python</a>获取<code class="eh lt lu lv lw b">contractions</code>的列表。</p><p id="b4dc" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">3.将缩写替换为更长的形式，如下所示:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="a690" class="lf kc hu lw b fv mf mg l mh mi"><strong class="lw hv">contractions</strong> = {</span><span id="ad7e" class="lf kc hu lw b fv mj mg l mh mi">"ain't": "am not",</span><span id="8a25" class="lf kc hu lw b fv mj mg l mh mi">"aren't": "are not",</span><span id="7352" class="lf kc hu lw b fv mj mg l mh mi">"can't": "cannot",</span><span id="bd16" class="lf kc hu lw b fv mj mg l mh mi">"can't've": "cannot have",</span><span id="5566" class="lf kc hu lw b fv mj mg l mh mi">"'cause": "because",</span><span id="ed3f" class="lf kc hu lw b fv mj mg l mh mi">"could've": "could have",</span><span id="f0ff" class="lf kc hu lw b fv mj mg l mh mi">"couldn't": "could not",</span><span id="5106" class="lf kc hu lw b fv mj mg l mh mi">"couldn't've": "could not have",</span><span id="ca26" class="lf kc hu lw b fv mj mg l mh mi">"didn't": "did not",</span><span id="ca45" class="lf kc hu lw b fv mj mg l mh mi">"doesn't": "does not",</span><span id="3523" class="lf kc hu lw b fv mj mg l mh mi">"don't": "do not",</span><span id="2428" class="lf kc hu lw b fv mj mg l mh mi">"hadn't": "had not",</span><span id="2599" class="lf kc hu lw b fv mj mg l mh mi">"hadn't've": "had not have",</span><span id="e0c7" class="lf kc hu lw b fv mj mg l mh mi">"hasn't": "has not",</span><span id="84b8" class="lf kc hu lw b fv mj mg l mh mi">"haven't": "have not",</span><span id="6378" class="lf kc hu lw b fv mj mg l mh mi">"he'd": "he would",</span><span id="8b58" class="lf kc hu lw b fv mj mg l mh mi">"he'd've": "he would have",</span></pre><p id="d28d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">4.通过替换缩写和删除停用词来清理文本文档:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="97e4" class="lf kc hu lw b fv mf mg l mh mi">def <strong class="lw hv">clean_text</strong>(text, remove_stopwords=True):</span><span id="77ea" class="lf kc hu lw b fv mj mg l mh mi"># Convert words to lower case</span><span id="9641" class="lf kc hu lw b fv mj mg l mh mi">text = text.lower()</span><span id="0bcf" class="lf kc hu lw b fv mj mg l mh mi">if True:</span><span id="f98e" class="lf kc hu lw b fv mj mg l mh mi">text = text.split()</span><span id="2f7c" class="lf kc hu lw b fv mj mg l mh mi">new_text = []</span><span id="60eb" class="lf kc hu lw b fv mj mg l mh mi">for word in text:</span><span id="0f1c" class="lf kc hu lw b fv mj mg l mh mi">if word in contractions:</span><span id="6c62" class="lf kc hu lw b fv mj mg l mh mi">new_text.append(contractions[word])</span><span id="4328" class="lf kc hu lw b fv mj mg l mh mi">else:</span><span id="f82c" class="lf kc hu lw b fv mj mg l mh mi">new_text.append(word)</span><span id="a49e" class="lf kc hu lw b fv mj mg l mh mi">text = " ".join(new_text)</span><span id="6ea3" class="lf kc hu lw b fv mj mg l mh mi">text = re.sub(r'https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)</span><span id="661d" class="lf kc hu lw b fv mj mg l mh mi">text = re.sub(r'\&lt;a href', ' ', text)</span><span id="f299" class="lf kc hu lw b fv mj mg l mh mi">text = re.sub(r'<!-- -->&amp;amp;<!-- -->', '', text)</span><span id="2cce" class="lf kc hu lw b fv mj mg l mh mi">text = re.sub(r'[_"\-;%()|+&amp;=*%.,!?:#$@\[\]/]', ' ', text)</span><span id="475d" class="lf kc hu lw b fv mj mg l mh mi">text = re.sub(r'<!-- -->&lt;br /&gt;<!-- -->', ' ', text)</span><span id="3f81" class="lf kc hu lw b fv mj mg l mh mi">text = re.sub(r'\'', ' ', text)</span><span id="38fa" class="lf kc hu lw b fv mj mg l mh mi">if remove_stopwords:</span><span id="fa48" class="lf kc hu lw b fv mj mg l mh mi">text = text.split()</span><span id="8d92" class="lf kc hu lw b fv mj mg l mh mi">stops = set(stopwords.words("english"))</span><span id="963b" class="lf kc hu lw b fv mj mg l mh mi">text = [w for w in text if not w in stops]</span><span id="9400" class="lf kc hu lw b fv mj mg l mh mi">text = " ".join(text)</span><span id="1a65" class="lf kc hu lw b fv mj mg l mh mi">return text</span></pre><p id="15e7" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">5.删除不需要的字符，并可以选择停止单词。此外，确保更换<code class="eh lt lu lv lw b">contractions</code>，如前所示。你可以从<strong class="je hv">自然语言工具包</strong> ( <strong class="je hv"> NLTK </strong>)中获得停用词列表，这有助于从段落中拆分句子、拆分单词以及识别词性。使用以下命令导入工具包:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="c7e7" class="lf kc hu lw b fv mf mg l mh mi">import nltk</span><span id="8067" class="lf kc hu lw b fv mj mg l mh mi">nltk.download('stopwords')</span></pre><p id="af94" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">6.清理摘要，如以下代码片段所示:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="2558" class="lf kc hu lw b fv mf mg l mh mi"># Clean the summaries and texts</span><span id="5342" class="lf kc hu lw b fv mj mg l mh mi">clean_summaries = []</span><span id="0fc7" class="lf kc hu lw b fv mj mg l mh mi">for summary in reviews.Summary:</span><span id="7761" class="lf kc hu lw b fv mj mg l mh mi">clean_summaries.append(clean_text(summary, remove_stopwords=False))</span><span id="4040" class="lf kc hu lw b fv mj mg l mh mi">print("Summaries are complete.")</span><span id="9f03" class="lf kc hu lw b fv mj mg l mh mi">clean_texts = []</span><span id="f6b7" class="lf kc hu lw b fv mj mg l mh mi">for text in reviews.Text:</span><span id="8767" class="lf kc hu lw b fv mj mg l mh mi">clean_texts.append(clean_text(text))</span><span id="83d4" class="lf kc hu lw b fv mj mg l mh mi">print("Texts are complete.")</span></pre><p id="f2c7" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">7.最后，将所有评论保存到一个<code class="eh lt lu lv lw b">pickle</code>文件中。<code class="eh lt lu lv lw b">pickle</code>序列化对象，使其可以保存到文件中，并在以后再次加载到程序中:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="6a88" class="lf kc hu lw b fv mf mg l mh mi">stories = list()</span><span id="2a92" class="lf kc hu lw b fv mj mg l mh mi">for i, text in enumerate(clean_texts):</span><span id="c4e2" class="lf kc hu lw b fv mj mg l mh mi">stories.append({'story': text, 'highlights': clean_summaries[i]})</span><span id="a1c1" class="lf kc hu lw b fv mj mg l mh mi"># save to file</span><span id="af39" class="lf kc hu lw b fv mj mg l mh mi">dump(stories, open('/deeplearning-keras/ch09/summarization/review_dataset.pkl', 'wb'))</span></pre><h2 id="4dbb" class="lf kc hu bd kd lg lh li kh lj lk ll kl jn lm ln kp jr lo lp kt jv lq lr kx ls dt translated">编码器-解码器架构</h2><p id="490d" class="pw-post-body-paragraph jc jd hu je b jf kz jh ji jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz hn dt translated">为文本摘要开发一个基本的字符级seq2seq模型。使用单词级模型，这在文本处理领域很常见。对于本文，使用角色级别模型。如前所述，编码器和解码器架构是为序列预测创建rnn的一种方式。编码器读取整个输入序列，并将其编码成一个内部表示，通常是一个名为上下文向量的固定长度向量。另一方面，解码器从编码器读取编码的输入序列，并产生输出序列。</p><p id="f22d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">编码器-解码器架构包括两个主要模型:一个读取输入序列并将其编码为固定长度的向量，另一个解码固定长度的向量并输出预测序列。这个架构是为seq2seq问题设计的。</p><p id="4313" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">1.首先，定义超参数，例如批量大小、用于训练的时期数以及用于训练的样本数:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="49db" class="lf kc hu lw b fv mf mg l mh mi">batch_size = 64</span><span id="1794" class="lf kc hu lw b fv mj mg l mh mi">epochs = 110</span><span id="42fa" class="lf kc hu lw b fv mj mg l mh mi">latent_dim = 256</span><span id="174e" class="lf kc hu lw b fv mj mg l mh mi">num_samples = 10000</span></pre><p id="714d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">2.接下来，从<code class="eh lt lu lv lw b">pickle</code>文件加载<code class="eh lt lu lv lw b">review</code>数据集:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="f7d7" class="lf kc hu lw b fv mf mg l mh mi">stories = load(open('/deeplearning-keras/ch09/summarization/review_dataset.pkl', 'rb'))</span><span id="58ca" class="lf kc hu lw b fv mj mg l mh mi">print('Loaded Stories %d' % len(stories))</span><span id="8aae" class="lf kc hu lw b fv mj mg l mh mi">print(type(stories))</span></pre><p id="e86e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">输出如下所示:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="21b7" class="lf kc hu lw b fv mf mg l mh mi">Loaded Stories <strong class="lw hv">568411</strong></span></pre><p id="e52f" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">3.然后，对数据进行矢量化:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="98fd" class="lf kc hu lw b fv mf mg l mh mi">input_texts = []</span><span id="fa54" class="lf kc hu lw b fv mj mg l mh mi">target_texts = []</span><span id="7a19" class="lf kc hu lw b fv mj mg l mh mi">input_characters = set()</span><span id="2c35" class="lf kc hu lw b fv mj mg l mh mi">target_characters = set()</span><span id="a034" class="lf kc hu lw b fv mj mg l mh mi">for story in stories:</span><span id="7be6" class="lf kc hu lw b fv mj mg l mh mi">input_text = story['story']</span><span id="656c" class="lf kc hu lw b fv mj mg l mh mi">for highlight in story['highlights']:</span><span id="6b14" class="lf kc hu lw b fv mj mg l mh mi">target_text = highlight</span><span id="70a7" class="lf kc hu lw b fv mj mg l mh mi"># We use "tab" as the "start sequence" character</span><span id="f8ed" class="lf kc hu lw b fv mj mg l mh mi"># for the targets, and "\n" as "end sequence" character.</span><span id="a37e" class="lf kc hu lw b fv mj mg l mh mi">target_text = '\t' + target_text + '\n'</span><span id="50f2" class="lf kc hu lw b fv mj mg l mh mi">input_texts.append(input_text)</span><span id="e024" class="lf kc hu lw b fv mj mg l mh mi">target_texts.append(target_text)</span><span id="50a7" class="lf kc hu lw b fv mj mg l mh mi">for char in input_text:</span><span id="535f" class="lf kc hu lw b fv mj mg l mh mi">if char not in input_characters:</span><span id="3302" class="lf kc hu lw b fv mj mg l mh mi">input_characters.add(char)</span><span id="730c" class="lf kc hu lw b fv mj mg l mh mi">for char in target_text:</span><span id="32ac" class="lf kc hu lw b fv mj mg l mh mi">if char not in target_characters:</span><span id="8c4d" class="lf kc hu lw b fv mj mg l mh mi">target_characters.add(char)</span><span id="4e90" class="lf kc hu lw b fv mj mg l mh mi">input_characters = sorted(list(input_characters))</span><span id="cb41" class="lf kc hu lw b fv mj mg l mh mi">target_characters = sorted(list(target_characters))</span><span id="db0d" class="lf kc hu lw b fv mj mg l mh mi">num_encoder_tokens = len(input_characters)</span><span id="d2d2" class="lf kc hu lw b fv mj mg l mh mi">num_decoder_tokens = len(target_characters)</span><span id="f9dd" class="lf kc hu lw b fv mj mg l mh mi">max_encoder_seq_length = max([len(txt) for txt in input_texts])</span><span id="484e" class="lf kc hu lw b fv mj mg l mh mi">max_decoder_seq_length = max([len(txt) for txt in target_texts])</span><span id="ae35" class="lf kc hu lw b fv mj mg l mh mi">print('Number of samples:', len(input_texts))</span><span id="3172" class="lf kc hu lw b fv mj mg l mh mi">print('Number of unique input tokens:', num_encoder_tokens)</span><span id="f088" class="lf kc hu lw b fv mj mg l mh mi">print('Number of unique output tokens:', num_decoder_tokens)</span><span id="b081" class="lf kc hu lw b fv mj mg l mh mi">print('Max sequence length for inputs:', max_encoder_seq_length)</span><span id="44b4" class="lf kc hu lw b fv mj mg l mh mi">print('Max sequence length for outputs:', max_decoder_seq_length)</span></pre><p id="aa25" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">输出如下所示:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="f650" class="lf kc hu lw b fv mf mg l mh mi">Number of samples: 568411</span><span id="d824" class="lf kc hu lw b fv mj mg l mh mi">Number of unique input tokens: 84</span><span id="af61" class="lf kc hu lw b fv mj mg l mh mi">Number of unique output tokens: 48</span><span id="0de5" class="lf kc hu lw b fv mj mg l mh mi">Max sequence length for inputs: 15074</span><span id="2923" class="lf kc hu lw b fv mj mg l mh mi">Max sequence length for outputs: 5</span></pre><p id="c345" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">4.现在，创建一个通用函数来定义编码器-解码器RNN:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="d87b" class="lf kc hu lw b fv mf mg l mh mi">def <strong class="lw hv">define_models</strong>(n_input, n_output, n_units):</span><span id="d9d6" class="lf kc hu lw b fv mj mg l mh mi"># define training encoder</span><span id="c71d" class="lf kc hu lw b fv mj mg l mh mi">encoder_inputs = Input(shape=(None, n_input))</span><span id="e1dd" class="lf kc hu lw b fv mj mg l mh mi">encoder = LSTM(n_units, return_state=True)</span><span id="7b72" class="lf kc hu lw b fv mj mg l mh mi">encoder_outputs, state_h, state_c = encoder(encoder_inputs)</span><span id="55e9" class="lf kc hu lw b fv mj mg l mh mi">encoder_states = [state_h, state_c]</span><span id="4e53" class="lf kc hu lw b fv mj mg l mh mi"># define training decoder</span><span id="02bb" class="lf kc hu lw b fv mj mg l mh mi">decoder_inputs = Input(shape=(None, n_output))</span><span id="0f6f" class="lf kc hu lw b fv mj mg l mh mi">decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)</span><span id="7a32" class="lf kc hu lw b fv mj mg l mh mi">decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)</span><span id="6a40" class="lf kc hu lw b fv mj mg l mh mi">decoder_dense = Dense(n_output, activation='softmax')</span><span id="6aca" class="lf kc hu lw b fv mj mg l mh mi">decoder_outputs = decoder_dense(decoder_outputs)</span><span id="861b" class="lf kc hu lw b fv mj mg l mh mi">model = Model([encoder_inputs, decoder_inputs], decoder_outputs)</span><span id="3704" class="lf kc hu lw b fv mj mg l mh mi"># define inference encoder</span><span id="80e3" class="lf kc hu lw b fv mj mg l mh mi">encoder_model = Model(encoder_inputs, encoder_states)</span><span id="7c23" class="lf kc hu lw b fv mj mg l mh mi"># define inference decoder</span><span id="b1f0" class="lf kc hu lw b fv mj mg l mh mi">decoder_state_input_h = Input(shape=(n_units,))</span><span id="327f" class="lf kc hu lw b fv mj mg l mh mi">decoder_state_input_c = Input(shape=(n_units,))</span><span id="937c" class="lf kc hu lw b fv mj mg l mh mi">decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]</span><span id="3774" class="lf kc hu lw b fv mj mg l mh mi">decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,  initial_state=decoder_states_inputs)</span><span id="b177" class="lf kc hu lw b fv mj mg l mh mi">decoder_states = [state_h, state_c]</span><span id="9235" class="lf kc hu lw b fv mj mg l mh mi">decoder_outputs = decoder_dense(decoder_outputs)</span><span id="aeb3" class="lf kc hu lw b fv mj mg l mh mi">decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)</span><span id="ea5a" class="lf kc hu lw b fv mj mg l mh mi"># return all models</span><span id="2fbd" class="lf kc hu lw b fv mj mg l mh mi">return model, encoder_model, decoder_model</span></pre><h2 id="2c63" class="lf kc hu bd kd lg lh li kh lj lk ll kl jn lm ln kp jr lo lp kt jv lq lr kx ls dt translated">培养</h2><p id="f16d" class="pw-post-body-paragraph jc jd hu je b jf kz jh ji jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz hn dt translated">1.运行训练时，使用<code class="eh lt lu lv lw b">rmsprop</code>优化器和<code class="eh lt lu lv lw b">categorical_crossentropy</code>作为<code class="eh lt lu lv lw b">loss</code>功能:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="43a6" class="lf kc hu lw b fv mf mg l mh mi"># Run training</span><span id="0bdd" class="lf kc hu lw b fv mj mg l mh mi">model.compile(optimizer='rmsprop', loss='categorical_crossentropy')</span><span id="6876" class="lf kc hu lw b fv mj mg l mh mi">model.fit([encoder_input_data, decoder_input_data], decoder_target_data,</span><span id="c81d" class="lf kc hu lw b fv mj mg l mh mi">batch_size=batch_size,</span><span id="89d4" class="lf kc hu lw b fv mj mg l mh mi">epochs=epochs,</span><span id="4b46" class="lf kc hu lw b fv mj mg l mh mi">validation_split=0.2)</span><span id="1d71" class="lf kc hu lw b fv mj mg l mh mi"># Save model</span><span id="4858" class="lf kc hu lw b fv mj mg l mh mi">model.save('/deeplearning-keras/ch09/summarization/model2.h5')</span></pre><p id="5f43" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">输出如下所示:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="7920" class="lf kc hu lw b fv mf mg l mh mi">64/800 [=&gt;............................] - ETA: 22:05 - loss: 2.1460</span><span id="18e3" class="lf kc hu lw b fv mj mg l mh mi">128/800 [===&gt;..........................] - ETA: 18:51 - loss: 2.1234</span><span id="2f95" class="lf kc hu lw b fv mj mg l mh mi">192/800 [======&gt;.......................] - ETA: 16:36 - loss: 2.0878</span><span id="a362" class="lf kc hu lw b fv mj mg l mh mi">256/800 [========&gt;.....................] - ETA: 14:38 - loss: 2.1215</span><span id="79dd" class="lf kc hu lw b fv mj mg l mh mi">320/800 [===========&gt;..................] - ETA: 12:47 - loss: 1.9832</span><span id="5a86" class="lf kc hu lw b fv mj mg l mh mi">384/800 [=============&gt;................] - ETA: 11:01 - loss: 1.8665</span><span id="d8fa" class="lf kc hu lw b fv mj mg l mh mi">448/800 [===============&gt;..............] - ETA: 9:17 - loss: 1.7547</span><span id="5b74" class="lf kc hu lw b fv mj mg l mh mi">512/800 [==================&gt;...........] - ETA: 7:35 - loss: 1.6619</span><span id="b51a" class="lf kc hu lw b fv mj mg l mh mi">576/800 [====================&gt;.........] - ETA: 5:53 - loss: 1.5820</span><span id="b04f" class="lf kc hu lw b fv mj mg l mh mi">512/800 [==================&gt;...........] - ETA: 7:19 - loss: 0.7519</span><span id="f6ef" class="lf kc hu lw b fv mj mg l mh mi">576/800 [====================&gt;.........] - ETA: 5:42 - loss: 0.7493</span><span id="afca" class="lf kc hu lw b fv mj mg l mh mi">640/800 [=======================&gt;......] - ETA: 4:06 - loss: 0.7528</span><span id="d83f" class="lf kc hu lw b fv mj mg l mh mi">704/800 [=========================&gt;....] - ETA: 2:28 - loss: 0.7553</span><span id="c540" class="lf kc hu lw b fv mj mg l mh mi">768/800 [===========================&gt;..] - ETA: 50s - loss: 0.7554</span></pre><p id="048b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">2.要进行推断，请使用以下方法:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="6cbd" class="lf kc hu lw b fv mf mg l mh mi"># generate target given source sequence</span><span id="9c69" class="lf kc hu lw b fv mj mg l mh mi">def predict_sequence(infenc, infdec, source, n_steps, cardinality):</span><span id="1e11" class="lf kc hu lw b fv mj mg l mh mi"># encode</span><span id="5fd9" class="lf kc hu lw b fv mj mg l mh mi">state = infenc.predict(source)</span><span id="ed11" class="lf kc hu lw b fv mj mg l mh mi"># start of sequence input</span><span id="736a" class="lf kc hu lw b fv mj mg l mh mi">target_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)</span><span id="934a" class="lf kc hu lw b fv mj mg l mh mi"># collect predictions</span><span id="3e8e" class="lf kc hu lw b fv mj mg l mh mi">output = list()</span><span id="6d66" class="lf kc hu lw b fv mj mg l mh mi">for t in range(n_steps):</span><span id="d006" class="lf kc hu lw b fv mj mg l mh mi"># predict next char</span><span id="3725" class="lf kc hu lw b fv mj mg l mh mi">yhat, h, c = infdec.predict([target_seq] + state)</span><span id="2217" class="lf kc hu lw b fv mj mg l mh mi"># store prediction</span><span id="04b6" class="lf kc hu lw b fv mj mg l mh mi">output.append(yhat[0,0,:])</span><span id="90b5" class="lf kc hu lw b fv mj mg l mh mi"># update state</span><span id="64b7" class="lf kc hu lw b fv mj mg l mh mi">state = [h, c]</span><span id="f151" class="lf kc hu lw b fv mj mg l mh mi"># update target sequence</span><span id="09ed" class="lf kc hu lw b fv mj mg l mh mi">target_seq = yhat</span><span id="dd74" class="lf kc hu lw b fv mj mg l mh mi">return array(output)</span></pre><p id="c1f4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">输出如下所示:</p><pre class="lx ly lz ma fq mb lw mc md aw me dt"><span id="eee2" class="lf kc hu lw b fv mf mg l mh mi">Review(1): The coffee tasted great and was at such a good price! I highly recommend this to everyone!</span><span id="aa44" class="lf kc hu lw b fv mj mg l mh mi">Summary(1): great coffee</span><span id="20dc" class="lf kc hu lw b fv mj mg l mh mi">Review(2): This is the worst cheese that I have ever bought! I will never buy it again and I hope you won't either!</span><span id="1ec9" class="lf kc hu lw b fv mj mg l mh mi">Summary(2): omg gross gross</span><span id="e380" class="lf kc hu lw b fv mj mg l mh mi">Review(3): love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon to know quaker flavor packets</span><span id="a3e9" class="lf kc hu lw b fv mj mg l mh mi">Summary(3): love it</span></pre><p id="eb04" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><em class="ka">如果你觉得这篇文章很有趣，你可以探索一下</em> <a class="ae le" href="https://www.amazon.com/Keras-Deep-Learning-Cookbook-implementing/dp/1788621751" rel="noopener ugc nofollow" target="_blank"> <em class="ka"> Keras深度学习食谱</em> </a> <em class="ka">利用深度学习和Keras的力量来开发更智能、更高效的数据模型。</em> <a class="ae le" href="https://www.packtpub.com/big-data-and-business-intelligence/keras-deep-learning-cookbook" rel="noopener ugc nofollow" target="_blank"> <em class="ka"> Keras深度学习食谱</em> </a> <em class="ka">向您展示了如何在广受欢迎的Keras库的帮助下，解决训练高效深度学习模型时遇到的不同问题。</em></p></div></div>    
</body>
</html>