<html>
<head>
<title>Deep Reinforcement Learning’s Generalization Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习的泛化问题</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/reinforcement-learnings-generalization-problem-414d276c4000?source=collection_archive---------15-----------------------#2018-12-20">https://medium.com/hackernoon/reinforcement-learnings-generalization-problem-414d276c4000?source=collection_archive---------15-----------------------#2018-12-20</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="54d6" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">Google Deepmind &amp; OpenAI最近强调的一个问题。</p><p id="0c44" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><em class="jp">本文研究深度强化学习代理的泛化能力——</em><em class="jp">一种最近被Google Deepmind &amp; OpenAI质疑的能力。</em></p><h1 id="f59a" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">死记硬背与一般化</h1><p id="2acb" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated"><strong class="it hv">我们人类有时依赖死记硬背。</strong>有没有参加过一个事后证明死记硬背的考试？应试者可能在没有真正理解材料的情况下鹦鹉学舌，取得了优异的成绩——从长远来看，这会损害他们概括概念以解决新问题的能力。</p><p id="0214" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">本文演示了这种现象的机器等价物。</strong>代理将通过死记硬背(相对于理解)来处理环境，导致以后难以推广到新环境。</p><h1 id="fe9e" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">概括测试</h1><p id="5945" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">最近，谷歌Deepmind &amp; OpenAI发布了旨在衡量代理归纳能力的环境——即使对现代深度强化学习来说，这也是一个基本挑战。</p><p id="eb56" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">一般化的需求无处不在<strong class="it hv">——例如，当一个代理在模拟器中接受训练，但随后被部署到现实世界时(这种差异也被称为现实差距)。</strong></p><p id="0dbf" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">然而，今天常见的基准测试使用相同的环境进行训练和测试— <strong class="it hv">这种实践对代理的概括能力提供的洞察相对较少</strong>。</p><p id="dc70" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">谷歌Deepmind &amp; OpenAI发布了三个环境，试图阐明或测量智能体的概括能力:“<em class="jp">分布转移</em>”、“<em class="jp">共润</em>”、&amp;“<em class="jp">刺猬索尼克</em>”。</p><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="fe ff kt"><img src="../Images/93ec4a76fddcac2b0418e7d382d6ac8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*m-Ta9gEpEkae6A9e"/></div></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">Distributional shift environment by GoogleDeepmind. <a class="ae lj" href="https://deepmind.com/blog/specifying-ai-safety-problems" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="fe ff lk"><img src="../Images/4549ac034bd6799aa2ef2b5306179c97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oRDhWhshJRJ1is1c"/></div></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">CoinRun environment by the authors of “<em class="ll">Quantifying Generalization in Reinforcement Learning”</em>. <a class="ae lj" href="https://blog.openai.com/quantifying-generalization-in-reinforcement-learning" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="fe ff lm"><img src="../Images/d887dd57f8fa44cee9911beb5774fe22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pMbJEvA3lCJK405u"/></div></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">Sonic the Hedgehog environment by OpenAI. Paper- “Gotta Learn Fast: A New Benchmark for Generalization in RL”. <a class="ae lj" href="https://blog.openai.com/retro-contest" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="b5a4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">观察RL代理在新游戏关卡(包含以前遇到的位置不同的对象的关卡)上的奇怪行为，可能会让人怀疑现代深度强化学习还不能创造出“真正”理解环境的代理。(这不是说近期不会有进展。)</p><p id="51e7" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">一些看似表现出色的代理人会不会仅仅是重复他们在训练中获得奖励的动作序列？</p><p id="188e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我尝试用分布转移实验来研究这个假设。</p><h1 id="4c22" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">谷歌Deepmind的分布式转变实验</h1><figure class="ku kv kw kx fq ky fe ff paragraph-image"><div class="fe ff ln"><img src="../Images/5825dd50cb48e78dd846c3682912e4be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/0*01vo_kWs-G-ov54l"/></div><figcaption class="lf lg fg fe ff lh li bd b be z ek">The distributional shift experiment set-up by AI Safety Gridworlds authors. Agent start position ‘A’. Lava ‘L’. Goal ‘G’. Wall ‘#’.</figcaption></figure><p id="d557" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">分布移位实验(见上图和开头的gif)<strong class="it hv">调查了一个代理适应新环境</strong>的能力，这些新环境包含来自不同位置的训练环境的对象(在本例中为lava-，goal-&amp;wall-tiles)——本质上是一个一般化的<strong class="it hv">测试。</strong></p><p id="7208" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">重要的是要注意，为了这个实验的缘故，代理人不应该在这个熔岩格子世界的许多不同变化上被训练。如果是这样的话，那么测试环境本质上将是“在训练有素的代理的分布的流形上”,因此不需要非常强的泛化能力来适应测试环境。</p><p id="ef83" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">本文的剩余部分讨论了使用现代深度强化学习代理运行该实验所获得的结果。</p><h1 id="a76d" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">培训和测试</h1><p id="43aa" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">一个深度强化学习代理——一个A2C(优势行动者-批评家)模型——在带有PPO(近似策略优化)的训练环境中被训练。</p><p id="e4fa" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">训练</strong> <br/>训练100万帧，归一化奖励:熔岩-1奖励，移动-0.02奖励，进球+1奖励。<br/> PPO参数(<em class="jp">与代码</em>并排读会更有意义):0.99折扣因子，7e-4学习率，0.95 gae-lambda，0.01熵系数，0.5价值损失系数，0.5梯度范数剪辑，0.2剪辑epsilon，4个历元，256批量。RMSprop参数:1e-5 alpha，0.99 alpha。<br/>输入格式:board (h x w x 1)。<br/>并行化:16个进程，每个进程128帧。<br/>简单的A2C模型。没有递归参数，因为模型没有RNN模块——保持简单。</p><p id="78c4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><em class="jp">代理可用</em> <a class="ae lj" href="https://github.com/davidleejy/ai-safety-gridworlds" rel="noopener ugc nofollow" target="_blank"> <em class="jp">此处</em> </a> <em class="jp">。</em></p><p id="6d9b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">测试</strong></p><p id="542c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">100集在训练环境上的结果:</strong> <br/>每集总回报:平均42.00标准差0.00分钟42.00最大42.00 <br/>每集帧数:平均8.0标准差0.0分钟8.0最大8.0 <br/> <strong class="it hv">观察:</strong> <br/> <strong class="it hv">所有100集的完美表现</strong>—达到可获得的最大奖励42(走8步，侧跨熔岩，进入球门)0次发作之间的差异。此代理的训练以策略分布的非零平均熵结束。</p><p id="bbed" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">测试环境100集的结果:</strong> <br/>每集总回报:平均-30.81标准差29.79分钟-100.00最大41.00 <br/>每集帧数:平均29.3标准差31.4分钟2.0最大100.0<br/>T5】观察: <br/>平均总回报-30.8是<strong class="it hv">与在列车环境上的评估相比明显较差的性能</strong>。每集帧数的变化显著增加。最小2帧来自代理自杀的情况(采取2步触摸熔岩)。由于代理过度探索，最多出现100帧(剧集设置为在100帧后终止)。最大奖励41可能来自于运行更简单的“测试环境v2”(但是这个环境的最大可获得奖励是46)。10次最糟糕的发作包括(i) 3次四处游荡躲避熔岩，产生R=-100，F=100，和(ii) 7次自杀，产生R=-52，F=2。</p><h1 id="782a" class="jq jr hu bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dt translated">结论</h1><p id="41ba" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">尽管这个深度强化学习代理在训练期间表现出色，但还不能说它“理解”了lava gridworld环境，<strong class="it hv">这意味着它的泛化能力相对较差。</strong></p><p id="2063" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">当被放置在测试环境中时，代理人要么被吸引到<strong class="it hv">自杀</strong> <em class="jp"> — </em>向右移动2格(-2奖励)，然后跳入熔岩(-50奖励)，以获得总共-52奖励，要么被吸引到<strong class="it hv">过度探索</strong> —在一集的最大允许长度(100帧)内四处游荡以避免熔岩，以获得总共-100奖励(-1奖励* 100步)。</p></div><div class="ab cl lo lp hc lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="hn ho hp hq hr"><h1 id="51da" class="jq jr hu bd js jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn dt translated">更多细节</h1><p id="d264" class="pw-post-body-paragraph ir is hu it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hn dt translated">我的代码可用<a class="ae lj" href="https://github.com/davidleejy/ai-safety-gridworlds" rel="noopener ugc nofollow" target="_blank">这里</a>。欢迎投稿。</p><p id="57fe" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">多亏了作者，这个分布转移实验在这里是<a class="ae lj" href="https://github.com/deepmind/ai-safety-gridworlds" rel="noopener ugc nofollow" target="_blank"/>。然而，他们的存储库不包含RL代理(截至2018年11月)，但将感兴趣的用户指向公共RL实现。</p><p id="e8b4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我使用的A2C建筑:</p><pre class="ku kv kw kx fq ma mb mc md aw me dt"><span id="37a6" class="mf jr hu mb b fv mg mh l mi mj">Model (no RNN modules):<br/>(0): Conv2d(1, 16, kernel_size=(2, 2), stride=(1, 1))<br/>(1): ReLU()<br/>(2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)<br/>(3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))<br/>(4): ReLU()<br/>)<br/>(actor): Sequential(<br/>(0): Linear(in_features=192, out_features=16, bias=True)<br/>(1): Tanh()<br/>(2): Linear(in_features=16, out_features=4, bias=True)<br/>)<br/>(critic): Sequential(<br/>(0): Linear(in_features=192, out_features=16, bias=True)<br/>(1): Tanh()<br/>(2): Linear(in_features=16, out_features=1, bias=True)<br/>)</span></pre></div><div class="ab cl lo lp hc lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="hn ho hp hq hr"><p id="7e43" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">感谢朋友和同事们分享他们关于强化学习和自主代理的观点和经验。没有他们，我对强化学习的理解就不会那么丰富。</p></div></div>    
</body>
</html>