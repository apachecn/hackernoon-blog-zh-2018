<html>
<head>
<title>Generating Lyrics Using Deep (Multi-Layer) LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度(多层)LSTM生成歌词</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/generating-lyrics-using-deep-multi-layer-lstm-b28ee8124936?source=collection_archive---------6-----------------------#2018-12-21">https://medium.com/hackernoon/generating-lyrics-using-deep-multi-layer-lstm-b28ee8124936?source=collection_archive---------6-----------------------#2018-12-21</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div class="fe ff ir"><img src="../Images/c932d32f7a27a6d861381e04c2f4111a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*NVgSB9ZKypjlhX8mtp8DZw.jpeg"/></div></figure><p id="c0c2" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><em class="jw">在这篇文章中，学习如何使用深度(多层)LSTM生成歌词，作者是Skejul的创始人兼首席执行官Matthew Lamons(人工智能平台，帮助人们管理他们的活动)，以及人工智能科学家、深度学习实践者和独立研究员Rahul Kumar。</em></p><p id="8793" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">本文将向您展示如何创建一个适合生成音乐歌词任务的深度LSTM模型。你的目标是:建立并训练一个模型，输出任意数量艺术家风格的全新原创歌词。这个练习可以参考在<code class="eh jx jy jz ka b">Lyrics-ai</code>(<a class="ae kb" href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter06/Lyrics-ai" rel="noopener ugc nofollow" target="_blank">https://github . com/packt publishing/Python-Deep-Learning-Projects/tree/master/chapter 06/Lyrics-ai</a>)找到的代码文件。</p><h1 id="c34d" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">数据预处理</h1><p id="762e" class="pw-post-body-paragraph iy iz hu ja b jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv hn dt translated">要构建一个可以生成歌词的模型，您需要大量的歌词数据，这些数据可以很容易地从各种来源提取出来。你可以在<a class="ae kb" href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter06/Lyrics-ai" rel="noopener ugc nofollow" target="_blank">https://github . com/packt publishing/Python-Deep-Learning-Projects/tree/master/chapter 06/Lyrics-ai</a>找到代码文件。这些文件包含一个名为<code class="eh jx jy jz ka b">lyrics_data.txt</code>的文本文件，其中包含了大约10，000首歌曲的<code class="eh jx jy jz ka b"> </code>歌词，并存储在。</p><p id="d318" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">现在您已经有了数据，将这个原始文本转换成一次性编码版本:</p><pre class="lf lg lh li fq lj ka lk ll aw lm dt"><span id="1973" class="ln kd hu ka b fv lo lp l lq lr">import numpy as np</span><span id="31af" class="ln kd hu ka b fv ls lp l lq lr">import codecs<br/></span><span id="14fd" class="ln kd hu ka b fv ls lp l lq lr"><strong class="ka hv"># Class to perform all preprocessing operations</strong></span><span id="1553" class="ln kd hu ka b fv ls lp l lq lr">class Preprocessing:</span><span id="28f6" class="ln kd hu ka b fv ls lp l lq lr">vocabulary = {}</span><span id="0961" class="ln kd hu ka b fv ls lp l lq lr">binary_vocabulary = {}</span><span id="c7f4" class="ln kd hu ka b fv ls lp l lq lr">char_lookup = {}</span><span id="5e59" class="ln kd hu ka b fv ls lp l lq lr">size = 0</span><span id="0321" class="ln kd hu ka b fv ls lp l lq lr">separator = '-&gt;'</span><span id="103b" class="ln kd hu ka b fv ls lp l lq lr"><strong class="ka hv"># This will take the data file and convert data into one hot encoding and dump the vocab into the file.</strong></span><span id="126d" class="ln kd hu ka b fv ls lp l lq lr">def generate(self, input_file_path):</span><span id="c5c1" class="ln kd hu ka b fv ls lp l lq lr">input_file = codecs.open(input_file_path, 'r', 'utf_8')</span><span id="710b" class="ln kd hu ka b fv ls lp l lq lr">index = 0</span><span id="b422" class="ln kd hu ka b fv ls lp l lq lr">for line in input_file:</span><span id="7a7b" class="ln kd hu ka b fv ls lp l lq lr">for char in line:</span><span id="f39f" class="ln kd hu ka b fv ls lp l lq lr">if char not in self.vocabulary:</span><span id="42a5" class="ln kd hu ka b fv ls lp l lq lr">self.vocabulary[char] = index</span><span id="aa92" class="ln kd hu ka b fv ls lp l lq lr">self.char_lookup[index] = char</span><span id="2f93" class="ln kd hu ka b fv ls lp l lq lr">index += 1</span><span id="c98e" class="ln kd hu ka b fv ls lp l lq lr">input_file.close()</span><span id="1fc8" class="ln kd hu ka b fv ls lp l lq lr">self.set_vocabulary_size()</span><span id="66d7" class="ln kd hu ka b fv ls lp l lq lr">self.create_binary_representation()</span><span id="b1eb" class="ln kd hu ka b fv ls lp l lq lr"><strong class="ka hv"># This method is to load the vocab into the memory</strong></span><span id="7171" class="ln kd hu ka b fv ls lp l lq lr">def retrieve(self, input_file_path):</span><span id="b67c" class="ln kd hu ka b fv ls lp l lq lr">input_file = codecs.open(input_file_path, 'r', 'utf_8')</span><span id="b14f" class="ln kd hu ka b fv ls lp l lq lr">buffer = ""</span><span id="d02d" class="ln kd hu ka b fv ls lp l lq lr">for line in input_file:</span><span id="fc06" class="ln kd hu ka b fv ls lp l lq lr">try:</span><span id="f406" class="ln kd hu ka b fv ls lp l lq lr">separator_position = len(buffer) + line.index(self.separator)</span><span id="c91e" class="ln kd hu ka b fv ls lp l lq lr">buffer += line</span><span id="7d4b" class="ln kd hu ka b fv ls lp l lq lr">key = buffer[:separator_position]</span><span id="7614" class="ln kd hu ka b fv ls lp l lq lr">value = buffer[separator_position + len(self.separator):]</span><span id="9a75" class="ln kd hu ka b fv ls lp l lq lr">value = np.fromstring(value, sep=',')</span><span id="9d1e" class="ln kd hu ka b fv ls lp l lq lr">self.binary_vocabulary[key] = value</span><span id="7b4c" class="ln kd hu ka b fv ls lp l lq lr">self.vocabulary[key] = np.where(value == 1)[0][0]</span><span id="7130" class="ln kd hu ka b fv ls lp l lq lr">self.char_lookup[np.where(value == 1)[0][0]] = key</span><span id="7cf0" class="ln kd hu ka b fv ls lp l lq lr">buffer = ""</span><span id="a669" class="ln kd hu ka b fv ls lp l lq lr">except ValueError:</span><span id="eded" class="ln kd hu ka b fv ls lp l lq lr">buffer += line</span><span id="d7c0" class="ln kd hu ka b fv ls lp l lq lr">input_file.close()</span><span id="1845" class="ln kd hu ka b fv ls lp l lq lr">self.set_vocabulary_size()</span><span id="3ab9" class="ln kd hu ka b fv ls lp l lq lr"><strong class="ka hv"># Below are some helper functions to perform pre-processing.</strong></span><span id="92c1" class="ln kd hu ka b fv ls lp l lq lr">def create_binary_representation(self):</span><span id="80f6" class="ln kd hu ka b fv ls lp l lq lr">for key, value in self.vocabulary.iteritems():</span><span id="3b25" class="ln kd hu ka b fv ls lp l lq lr">binary = np.zeros(self.size)</span><span id="568a" class="ln kd hu ka b fv ls lp l lq lr">binary[value] = 1</span><span id="dba4" class="ln kd hu ka b fv ls lp l lq lr">self.binary_vocabulary[key] = binary</span><span id="ac87" class="ln kd hu ka b fv ls lp l lq lr">def set_vocabulary_size(self):</span><span id="0b0b" class="ln kd hu ka b fv ls lp l lq lr">self.size = len(self.vocabulary)</span><span id="9b26" class="ln kd hu ka b fv ls lp l lq lr">print "Vocabulary size: {}".format(self.size)</span><span id="e681" class="ln kd hu ka b fv ls lp l lq lr">def get_serialized_binary_representation(self):</span><span id="2188" class="ln kd hu ka b fv ls lp l lq lr">string = ""</span><span id="b0cd" class="ln kd hu ka b fv ls lp l lq lr">np.set_printoptions(threshold='nan')</span><span id="3125" class="ln kd hu ka b fv ls lp l lq lr">for key, value in self.binary_vocabulary.iteritems():</span><span id="4b83" class="ln kd hu ka b fv ls lp l lq lr">array_as_string = np.array2string(value, separator=',', max_line_width=self.size * self.size)</span><span id="0180" class="ln kd hu ka b fv ls lp l lq lr">string += "{}{}{}\n".format(key.encode('utf-8'), self.separator, array_as_string[1:len(array_as_string) - 1])</span><span id="f907" class="ln kd hu ka b fv ls lp l lq lr">return string</span></pre><p id="4046" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">预处理模块的总体目标是将原始文本数据转换为一键编码，如下图所示:</p><figure class="lf lg lh li fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="fe ff lt"><img src="../Images/9f327fa0dbd2f07f622e4a5910083885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*og7gSsBOlEZunx6fxfqXlQ.png"/></div></div></figure><p id="1ab6" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">该图表示数据预处理部分。原始歌词数据被用于建立词汇映射，该词汇映射被进一步转换成热门编码。预处理模块成功执行后，二进制文件将被转储为<code class="eh jx jy jz ka b">{dataset_filename}.vocab</code>。这个<code class="eh jx jy jz ka b">vocab</code>文件是在训练过程中需要和数据集一起输入到模型中的强制文件之一。</p><h1 id="e09c" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">定义模型</h1><p id="4153" class="pw-post-body-paragraph iy iz hu ja b jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv hn dt translated">本文将采用Keras模型的方法，并使用TensorFlow从头开始编写每一层。TensorFlow使您能够更好地控制模型的架构。对于此模型，使用以下块中的代码创建两个占位符，用于存储输入和输出值:</p><pre class="lf lg lh li fq lj ka lk ll aw lm dt"><span id="bb28" class="ln kd hu ka b fv lo lp l lq lr">import tensorflow as tf</span><span id="172a" class="ln kd hu ka b fv ls lp l lq lr">import pickle</span><span id="3565" class="ln kd hu ka b fv ls lp l lq lr">from tensorflow.contrib import rnn</span><span id="a1fa" class="ln kd hu ka b fv ls lp l lq lr">def build(self, input_number, sequence_length, layers_number, units_number, output_number):</span><span id="4a73" class="ln kd hu ka b fv ls lp l lq lr">self.x = tf.placeholder("float", [None, sequence_length, input_number])</span><span id="c353" class="ln kd hu ka b fv ls lp l lq lr">self.y = tf.placeholder("float", [None, output_number])</span><span id="9c19" class="ln kd hu ka b fv ls lp l lq lr">self.sequence_length = sequence_length</span></pre><p id="2ae0" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">接下来，将权重和偏差存储在您创建的变量中:</p><pre class="lf lg lh li fq lj ka lk ll aw lm dt"><span id="fb72" class="ln kd hu ka b fv lo lp l lq lr">self.weights = {</span><span id="6a29" class="ln kd hu ka b fv ls lp l lq lr">'out': tf.Variable(tf.random_normal([units_number, output_number]))</span><span id="90fc" class="ln kd hu ka b fv ls lp l lq lr">}</span><span id="2f0f" class="ln kd hu ka b fv ls lp l lq lr">self.biases = {</span><span id="9b5c" class="ln kd hu ka b fv ls lp l lq lr">'out': tf.Variable(tf.random_normal([output_number]))</span><span id="75f2" class="ln kd hu ka b fv ls lp l lq lr">}</span><span id="0724" class="ln kd hu ka b fv ls lp l lq lr">x = tf.transpose(self.x, [1, 0, 2])</span><span id="2557" class="ln kd hu ka b fv ls lp l lq lr">x = tf.reshape(x, [-1, input_number])</span><span id="492e" class="ln kd hu ka b fv ls lp l lq lr">x = tf.split(x, sequence_length, 0)</span></pre><p id="6b80" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">您可以使用多个LSTM图层构建此模型，基本LSTM像元为每个图层分配指定数量的像元，如下图所示:</p><figure class="lf lg lh li fq iv fe ff paragraph-image"><div class="fe ff ly"><img src="../Images/caf05fbb325e8053cb9e6f4323f11024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*44B6KdjU2UNWeeIF1Bugmg.png"/></div><figcaption class="lz ma fg fe ff mb mc bd b be z ek">Tensorboard visualization of the LSTM architecture</figcaption></figure><p id="d5a7" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">以下是这方面的代码:</p><pre class="lf lg lh li fq lj ka lk ll aw lm dt"><span id="5544" class="ln kd hu ka b fv lo lp l lq lr">lstm_layers = []</span><span id="614b" class="ln kd hu ka b fv ls lp l lq lr">for i in range(0, layers_number):</span><span id="f274" class="ln kd hu ka b fv ls lp l lq lr">lstm_layer = rnn.BasicLSTMCell(units_number)</span><span id="da89" class="ln kd hu ka b fv ls lp l lq lr">lstm_layers.append(lstm_layer)</span><span id="f4e9" class="ln kd hu ka b fv ls lp l lq lr">deep_lstm = rnn.MultiRNNCell(lstm_layers)</span><span id="d8b0" class="ln kd hu ka b fv ls lp l lq lr">self.outputs, states = rnn.static_rnn(deep_lstm, x, dtype=tf.float32)</span><span id="21e8" class="ln kd hu ka b fv ls lp l lq lr">print "Build model with input_number: {}, sequence_length: {}, layers_number: {}, " \</span><span id="f84f" class="ln kd hu ka b fv ls lp l lq lr">"units_number: {}, output_number: {}".format(input_number, sequence_length, layers_number,</span><span id="36dd" class="ln kd hu ka b fv ls lp l lq lr">units_number, output_number)</span><span id="8a9e" class="ln kd hu ka b fv ls lp l lq lr"><strong class="ka hv"># This method is using to dump the model configurations</strong></span><span id="83c6" class="ln kd hu ka b fv ls lp l lq lr">self.save(input_number, sequence_length, layers_number, units_number, output_number)</span></pre><h1 id="c7ee" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">训练基于深度张量流的LSTM模型</h1><p id="0d4e" class="pw-post-body-paragraph iy iz hu ja b jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv hn dt translated">既然已经有了强制输入，也就是数据集文件路径、<code class="eh jx jy jz ka b">vocab</code>文件路径和模型名称，就可以开始训练过程了。定义模型的所有超参数:</p><pre class="lf lg lh li fq lj ka lk ll aw lm dt"><span id="be99" class="ln kd hu ka b fv lo lp l lq lr">import os</span><span id="c31a" class="ln kd hu ka b fv ls lp l lq lr">import argparse</span><span id="c866" class="ln kd hu ka b fv ls lp l lq lr">from modules.Model import *</span><span id="0842" class="ln kd hu ka b fv ls lp l lq lr">from modules.Batch import *</span><span id="889c" class="ln kd hu ka b fv ls lp l lq lr">def main():</span><span id="a7f0" class="ln kd hu ka b fv ls lp l lq lr">parser = argparse.ArgumentParser()</span><span id="fb09" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--training_file', type=str, required=True)</span><span id="6a54" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--vocabulary_file', type=str, required=True)</span><span id="e44d" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--model_name', type=str, required=True)</span><span id="70c2" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--epoch', type=int, default=200)</span><span id="b328" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--batch_size', type=int, default=50)</span><span id="6bd1" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--sequence_length', type=int, default=50)</span><span id="e86b" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--log_frequency', type=int, default=100)</span><span id="6f94" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--learning_rate', type=int, default=0.002)</span><span id="2873" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--units_number', type=int, default=128)</span><span id="bcc4" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--layers_number', type=int, default=2)</span><span id="abdf" class="ln kd hu ka b fv ls lp l lq lr">args = parser.parse_args()</span></pre><p id="5d89" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">由于这是一个训练模型的批处理，使用<code class="eh jx jy jz ka b">Batch</code>模块将数据集分成定义的<code class="eh jx jy jz ka b">batch_size</code>的批处理:</p><pre class="lf lg lh li fq lj ka lk ll aw lm dt"><span id="0bc6" class="ln kd hu ka b fv lo lp l lq lr">batch = Batch(training_file, vocabulary_file, batch_size, sequence_length)</span></pre><p id="57b2" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">每批将返回两个数组。一个是输入序列的输入向量，其形状为[ <code class="eh jx jy jz ka b">batch_size</code>、<code class="eh jx jy jz ka b">sequence_length</code>、<code class="eh jx jy jz ka b">vocab_size</code> ]，另一个数组保存标签向量，其形状为[ <code class="eh jx jy jz ka b">batch_size</code>、<code class="eh jx jy jz ka b">vocab_size</code> ]。</p><p id="ebd3" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">现在，初始化您的模型并创建优化器函数。在这个模型中，您使用了<code class="eh jx jy jz ka b">Adam</code>优化器。然后，您将训练您的模型并对每一批执行优化:</p><pre class="lf lg lh li fq lj ka lk ll aw lm dt"><span id="3aaa" class="ln kd hu ka b fv lo lp l lq lr"><strong class="ka hv"># Building model instance and classifier</strong></span><span id="8b49" class="ln kd hu ka b fv ls lp l lq lr">model = Model(model_name)</span><span id="cea6" class="ln kd hu ka b fv ls lp l lq lr">model.build(input_number, sequence_length, layers_number, units_number, classes_number)</span><span id="d9b5" class="ln kd hu ka b fv ls lp l lq lr">classifier = model.get_classifier()</span><span id="87ad" class="ln kd hu ka b fv ls lp l lq lr"><strong class="ka hv"><br/># Building cost functions</strong></span><span id="b5ce" class="ln kd hu ka b fv ls lp l lq lr">cost = tf.reduce_mean(tf.square(classifier - model.y))</span><span id="ff0f" class="ln kd hu ka b fv ls lp l lq lr">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><span id="bdc8" class="ln kd hu ka b fv ls lp l lq lr"><strong class="ka hv"># Computing the accuracy metrics</strong></span><span id="b0fc" class="ln kd hu ka b fv ls lp l lq lr">expected_prediction = tf.equal(tf.argmax(classifier, 1), tf.argmax(model.y, 1))</span><span id="f592" class="ln kd hu ka b fv ls lp l lq lr">accuracy = tf.reduce_mean(tf.cast(expected_prediction, tf.float32))</span><span id="a582" class="ln kd hu ka b fv ls lp l lq lr"><strong class="ka hv"># Preparing logs for Tensorboard</strong></span><span id="6e87" class="ln kd hu ka b fv ls lp l lq lr">loss_summary = tf.summary.scalar("loss", cost)</span><span id="ebbc" class="ln kd hu ka b fv ls lp l lq lr">acc_summary = tf.summary.scalar("accuracy", accuracy)</span><span id="113f" class="ln kd hu ka b fv ls lp l lq lr">train_summary_op = tf.summary.merge_all()</span><span id="9fe6" class="ln kd hu ka b fv ls lp l lq lr">out_dir = "{}/{}".format(model_name, model_name)</span><span id="ae49" class="ln kd hu ka b fv ls lp l lq lr">train_summary_dir = os.path.join(out_dir, "summaries")</span><span id="5527" class="ln kd hu ka b fv ls lp l lq lr">##</span><span id="5c5a" class="ln kd hu ka b fv ls lp l lq lr"># Initializing the session and executing the training</span><span id="f91b" class="ln kd hu ka b fv ls lp l lq lr">init = tf.global_variables_initializer()</span><span id="e402" class="ln kd hu ka b fv ls lp l lq lr">with tf.Session() as sess:</span><span id="5198" class="ln kd hu ka b fv ls lp l lq lr">sess.run(init)</span><span id="fb64" class="ln kd hu ka b fv ls lp l lq lr">iteration = 0</span><span id="4763" class="ln kd hu ka b fv ls lp l lq lr">while batch.dataset_full_passes &lt; epoch:</span><span id="35e9" class="ln kd hu ka b fv ls lp l lq lr">iteration += 1</span><span id="39f2" class="ln kd hu ka b fv ls lp l lq lr">batch_x, batch_y = batch.get_next_batch()</span><span id="399a" class="ln kd hu ka b fv ls lp l lq lr">batch_x = batch_x.reshape((batch_size, sequence_length, input_number))</span><span id="d597" class="ln kd hu ka b fv ls lp l lq lr">sess.run(optimizer, feed_dict={model.x: batch_x, model.y: batch_y})</span><span id="7753" class="ln kd hu ka b fv ls lp l lq lr">if iteration % log_frequency == 0:</span><span id="d6c8" class="ln kd hu ka b fv ls lp l lq lr">acc = sess.run(accuracy, feed_dict={model.x: batch_x, model.y: batch_y})</span><span id="cbe1" class="ln kd hu ka b fv ls lp l lq lr">loss = sess.run(cost, feed_dict={model.x: batch_x, model.y: batch_y})</span><span id="7112" class="ln kd hu ka b fv ls lp l lq lr">print("Iteration {}, batch loss: {:.6f}, training accuracy: {:.5f}".format(iteration * batch_size,</span><span id="521d" class="ln kd hu ka b fv ls lp l lq lr">loss, acc))</span><span id="fce8" class="ln kd hu ka b fv ls lp l lq lr">batch.clean()</span></pre><p id="ab67" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">一旦模型完成其训练，就存储检查点。您可以稍后使用它们进行推理。以下是在训练过程中发生的准确度和损失的图表:</p><figure class="lf lg lh li fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="fe ff md"><img src="../Images/dd1855e3699c8db10a580fea2444c56a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*80Woz9x3qFtu64xm5i4WFA.png"/></div></div><figcaption class="lz ma fg fe ff mb mc bd b be z ek">The accuracy (top) and the loss (bottom) plot with respect to the time</figcaption></figure><p id="411e" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">可以看到，随着时间的推移，精度在提高，损耗在降低。</p><h1 id="82e2" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">推理</h1><p id="ed3f" class="pw-post-body-paragraph iy iz hu ja b jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv hn dt translated">现在模型已经准备好了，您可以使用它来进行预测。首先定义所有参数。在构建推理时，您需要提供一些种子文本，就像您在前面的模型中所做的那样。除此之外，您还应该提供<code class="eh jx jy jz ka b">vocab</code>文件的路径和输出文件，您将在其中存储生成的歌词。另外，提供您需要生成的文本的长度:</p><pre class="lf lg lh li fq lj ka lk ll aw lm dt"><span id="1e88" class="ln kd hu ka b fv lo lp l lq lr">import argparse</span><span id="0383" class="ln kd hu ka b fv ls lp l lq lr">import codecs</span><span id="7b37" class="ln kd hu ka b fv ls lp l lq lr">from modules.Model import *</span><span id="7dfc" class="ln kd hu ka b fv ls lp l lq lr">from modules.Preprocessing import *</span><span id="4da3" class="ln kd hu ka b fv ls lp l lq lr">from collections import deque</span><span id="1164" class="ln kd hu ka b fv ls lp l lq lr">def main():</span><span id="b479" class="ln kd hu ka b fv ls lp l lq lr">parser = argparse.ArgumentParser()</span><span id="3e2b" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--model_name', type=str, required=True)</span><span id="4b02" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--vocabulary_file', type=str, required=True)</span><span id="720a" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--output_file', type=str, required=True)</span><span id="386c" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--seed', type=str, default="Yeah, oho ")</span><span id="8d27" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--sample_length', type=int, default=1500)</span><span id="d4a9" class="ln kd hu ka b fv ls lp l lq lr">parser.add_argument('--log_frequency', type=int, default=100)</span></pre><p id="b147" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">接下来，通过提供您在前面代码的培训步骤中使用的模型名称来加载模型，并从文件中恢复词汇:</p><pre class="lf lg lh li fq lj ka lk ll aw lm dt"><span id="15c8" class="ln kd hu ka b fv lo lp l lq lr">model = Model(model_name)</span><span id="ded5" class="ln kd hu ka b fv ls lp l lq lr">model.restore()</span><span id="bb30" class="ln kd hu ka b fv ls lp l lq lr">classifier = model.get_classifier()</span><span id="2d23" class="ln kd hu ka b fv ls lp l lq lr">vocabulary = Preprocessing()</span><span id="1ac3" class="ln kd hu ka b fv ls lp l lq lr">vocabulary.retrieve(vocabulary_file)</span></pre><p id="b2d3" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">使用stack方法存储生成的字符，追加堆栈，然后使用同一个堆栈以交互方式将其输入到模型中:</p><pre class="lf lg lh li fq lj ka lk ll aw lm dt"><span id="4cf0" class="ln kd hu ka b fv lo lp l lq lr"><strong class="ka hv"># Preparing the raw input data</strong></span><span id="13d6" class="ln kd hu ka b fv ls lp l lq lr">for char in seed:</span><span id="7b83" class="ln kd hu ka b fv ls lp l lq lr">if char not in vocabulary.vocabulary:</span><span id="460a" class="ln kd hu ka b fv ls lp l lq lr">print char,"is not in vocabulary file"</span><span id="5438" class="ln kd hu ka b fv ls lp l lq lr">char = u' '</span><span id="f924" class="ln kd hu ka b fv ls lp l lq lr">stack.append(char)</span><span id="abc5" class="ln kd hu ka b fv ls lp l lq lr">sample_file.write(char)</span><span id="bd7d" class="ln kd hu ka b fv ls lp l lq lr"><strong class="ka hv"># Restoring the models and making inferences</strong></span><span id="ef7d" class="ln kd hu ka b fv ls lp l lq lr">with tf.Session() as sess:</span><span id="8a12" class="ln kd hu ka b fv ls lp l lq lr">tf.global_variables_initializer().run()</span><span id="3f8b" class="ln kd hu ka b fv ls lp l lq lr">saver = tf.train.Saver(tf.global_variables())</span><span id="5fa6" class="ln kd hu ka b fv ls lp l lq lr">ckpt = tf.train.get_checkpoint_state(model_name)</span><span id="cf58" class="ln kd hu ka b fv ls lp l lq lr">if ckpt and ckpt.model_checkpoint_path:</span><span id="df51" class="ln kd hu ka b fv ls lp l lq lr">saver.restore(sess, ckpt.model_checkpoint_path)</span><span id="9c8f" class="ln kd hu ka b fv ls lp l lq lr">for i in range(0, sample_length):</span><span id="49e0" class="ln kd hu ka b fv ls lp l lq lr">vector = []</span><span id="ae24" class="ln kd hu ka b fv ls lp l lq lr">for char in stack:</span><span id="287c" class="ln kd hu ka b fv ls lp l lq lr">vector.append(vocabulary.binary_vocabulary[char])</span><span id="794a" class="ln kd hu ka b fv ls lp l lq lr">vector = np.array([vector])</span><span id="1676" class="ln kd hu ka b fv ls lp l lq lr">prediction = sess.run(classifier, feed_dict={model.x: vector})</span><span id="36bc" class="ln kd hu ka b fv ls lp l lq lr">predicted_char = vocabulary.char_lookup[np.argmax(prediction)]</span><span id="48d3" class="ln kd hu ka b fv ls lp l lq lr">stack.popleft()</span><span id="817d" class="ln kd hu ka b fv ls lp l lq lr">stack.append(predicted_char)</span><span id="527c" class="ln kd hu ka b fv ls lp l lq lr">sample_file.write(predicted_char)</span><span id="ccfa" class="ln kd hu ka b fv ls lp l lq lr">if i % log_frequency == 0:</span><span id="1d2d" class="ln kd hu ka b fv ls lp l lq lr">print "Progress: {}%".format((i * 100) / sample_length)</span><span id="23e2" class="ln kd hu ka b fv ls lp l lq lr">sample_file.close()</span><span id="ab7e" class="ln kd hu ka b fv ls lp l lq lr">print "Sample saved in {}".format(output_file)</span></pre><h1 id="b218" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">输出</h1><p id="3dcc" class="pw-post-body-paragraph iy iz hu ja b jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv hn dt translated">成功执行后，你将获得自己新创作的、人工智能生成的歌词，并被审核和发布。以下是这类歌词的一个例子。修改了一些拼写，使句子更有意义:</p><pre class="lf lg lh li fq lj ka lk ll aw lm dt"><span id="15c4" class="ln kd hu ka b fv lo lp l lq lr">Yeah, oho once upon a time, on ir intasd</span><span id="a03d" class="ln kd hu ka b fv ls lp l lq lr">I got monk that wear your good</span><span id="a7ad" class="ln kd hu ka b fv ls lp l lq lr">So heard me down in my clipp</span><span id="9225" class="ln kd hu ka b fv ls lp l lq lr">Cure me out brick</span><span id="e6dd" class="ln kd hu ka b fv ls lp l lq lr">Coway got baby, I wanna sheart in faic</span><span id="9c49" class="ln kd hu ka b fv ls lp l lq lr">I could sink awlrook and heart your all feeling in the firing of to the still hild, gavelly mind, have before you, their lead</span><span id="d4b6" class="ln kd hu ka b fv ls lp l lq lr">Oh, oh shor,s sheld be you und make</span><span id="c26b" class="ln kd hu ka b fv ls lp l lq lr">Oh, fseh where sufl gone for the runtome</span><span id="350c" class="ln kd hu ka b fv ls lp l lq lr">Weaaabe the ligavus I feed themust of hear</span></pre><p id="2b68" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">在这里，您可以看到模型已经学会了生成具有适当间距的段落和句子的方式。它仍然缺乏完美，也没有意义。</p><p id="0cfc" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><strong class="ja hv"> <em class="jw">看到成功的迹象— </em> </strong> <em class="jw">第一个任务是创建一个可以学习的模型，然后第二个用来在那个模型上改进。这可以通过用更大的训练数据集和更长的训练持续时间来训练模型来获得。</em></p><p id="2302" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><em class="jw">如果你觉得这篇文章有趣，可以探索</em> <a class="ae kb" href="https://www.amazon.com/Python-Deep-Learning-Projects-demystifying/dp/1788997093" rel="noopener ugc nofollow" target="_blank"> <em class="jw"> Python深度学习项目</em> </a> <em class="jw">掌握使用Python和Keras的深度学习和神经网络架构。</em> <a class="ae kb" href="https://www.packtpub.com/big-data-and-business-intelligence/python-deep-learning-projects" rel="noopener ugc nofollow" target="_blank"> <em class="jw"> Python深度学习项目</em> </a> <em class="jw">传授在计算语言学和计算机视觉领域实现复杂深度学习项目所需的所有知识。</em></p></div></div>    
</body>
</html>