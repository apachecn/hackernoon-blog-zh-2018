<html>
<head>
<title>Who Has the Best Prices for Tech’s Top 100 Products of the Year? A Machine Learning Analysis.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谁的年度科技100强产品价格最优惠？机器学习分析。</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/who-has-the-best-prices-for-techs-top-100-products-of-the-year-a-machine-learning-analysis-885d9dd9400a?source=collection_archive---------9-----------------------#2018-12-27">https://medium.com/hackernoon/who-has-the-best-prices-for-techs-top-100-products-of-the-year-a-machine-learning-analysis-885d9dd9400a?source=collection_archive---------9-----------------------#2018-12-27</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="4de9" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果你还没有阅读第一部分我朋友的帖子，请在这里阅读，我们将继续讨论项目的剩余部分。</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff jq"><img src="../Images/79a488a9c0833413761e04875d2cd961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wnc1Hq_NHt72qCQkntF4cQ.jpeg"/></div></div></figure><h1 id="de5f" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated"><strong class="ak">延续分析</strong></h1><p id="7a80" class="pw-post-body-paragraph ir is hu it b iu la iw ix iy lb ja jb jc lc je jf jg ld ji jj jk le jm jn jo hn dt translated">我们想预测iprice的每个搜索结果是否都符合100个最酷的电子产品之一。</p><p id="bcdc" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们考虑包括的功能有:</p><p id="d63d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><code class="eh lf lg lh li b">dist_jw</code>:Jaro-Winkler距离</p><p id="d26c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><code class="eh lf lg lh li b">price_diff_ratio</code> : ( <code class="eh lf lg lh li b">price</code> — <code class="eh lf lg lh li b">refer_price</code> ) / <code class="eh lf lg lh li b">refer_price</code></p><p id="aa8f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><code class="eh lf lg lh li b">discount</code>:折扣百分比</p><h1 id="7b8e" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated"><strong class="ak"> Jaro-Wrinkler距离</strong></h1><blockquote class="lj lk ll"><p id="c6ec" class="ir is lm it b iu iv iw ix iy iz ja jb ln jd je jf lo jh ji jj lp jl jm jn jo hn dt translated"><em class="hu">“在计算机科学和统计学中，Jaro-Winkler距离是用于测量两个序列之间的编辑距离的字符串度量。</em></p><p id="5cd8" class="ir is lm it b iu iv iw ix iy iz ja jb ln jd je jf lo jh ji jj lp jl jm jn jo hn dt translated"><em class="hu">非正式地说，两个单词之间的Jaro距离是将一个单词变为另一个单词所需的单字符换位的最小次数。</em></p><p id="c00d" class="ir is lm it b iu iv iw ix iy iz ja jb ln jd je jf lo jh ji jj lp jl jm jn jo hn dt translated"><em class="hu">Jaro-Winkler距离使用前缀标度，该标度对从开始就匹配设定前缀长度的字符串给予更有利的评级"</em></p><p id="f026" class="ir is lm it b iu iv iw ix iy iz ja jb ln jd je jf lo jh ji jj lp jl jm jn jo hn dt translated"><em class="hu"> —来源:</em> <a class="ae jp" href="https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance" rel="noopener ugc nofollow" target="_blank"> <em class="hu">维基百科</em> </a> <em class="hu">。</em></p></blockquote><p id="f1d4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们将钻研数学，这样我们就更容易完全理解了！</p><p id="660f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">Jaro距离定义为:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff lq"><img src="../Images/b853fdbdd103a29e6ff81812047bd024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wWTgzm7_HvNFoRQpLhtiig.png"/></div></div></figure><p id="0689" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">哇，这看起来很复杂…我更喜欢睡觉…不，我保证在这几个例子之后你会完全理解。</p><p id="81ca" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">但是首先，我们需要理解这些术语的含义。</p><blockquote class="lj lk ll"><p id="63ec" class="ir is lm it b iu iv iw ix iy iz ja jb ln jd je jf lo jh ji jj lp jl jm jn jo hn dt translated"><strong class="it hv"> dj: </strong> <em class="hu"> Jaro距离<br/> </em> <strong class="it hv"> m: </strong> <em class="hu">出现在</em> s1 <em class="hu">和</em> s2中的匹配字符数。<em class="hu"> <br/> </em> <strong class="it hv"> t </strong> <em class="hu">是换位次数的一半(比较s1的第I个字符和</em> s2的第I个字符<em class="hu">除以</em>2<em class="hu">)<br/></em><strong class="it hv">| S1 |</strong><em class="hu">是第一串<br/> </em> <strong class="it hv"> |s2| </strong> <em class="hu">是第二串的长度</em></p></blockquote><p id="6a5f" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">让我们用一个例子来解释数学。</p><p id="44a4" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如何计算<strong class="it hv">脸书</strong>和<strong class="it hv">火书之间的jaro距离？</strong></p><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="d9b8" class="lv kd hu li b fv lw lx l ly lz">matching characters       : Febook   -&gt; 6 characters -&gt; m = 6<br/>no transposition needed   : t=0<br/>length of the 1st string  : Facebook -&gt; 8 characters -&gt; |s1| = 8<br/>length of the 2nd string  : Firebook -&gt; 8 characters -&gt; |s2| = 8</span><span id="73a5" class="lv kd hu li b fv ma lx l ly lz">dj = (1/3)*( (6/8) + (6/8) + ((6-0)/6)) )<br/>dj ~= 0.83</span><span id="ae57" class="lv kd hu li b fv ma lx l ly lz">Jaro distance = 83%</span></pre><p id="68e9" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">知道了如何计算Jaro距离之后，就该明白如何计算<em class="lm"> Jaro-Winkler距离了！</em></p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff mb"><img src="../Images/672945daa1c74cb275f42b94ca88841a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gsnr_YBHwql30yNbRprPIg.png"/></div></div></figure><p id="9c55" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv"> <em class="lm"> l: </em> </strong>字符串开头的常用前缀长度最多为4个字符。</p><p id="d3dd" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv"> p </strong>:常量比例因子，表示分数因具有共同前缀而向上调整了多少。通常我们使用p=0.1。</p><p id="785b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">继续前面的案例，比如《火书》和《脸书》</p><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="c90c" class="lv kd hu li b fv lw lx l ly lz">dj    : 0.83<br/>prefix: character F -&gt; 1 character -&gt; <em class="lm">l</em>=1<br/>p     : 0.1</span><span id="f3db" class="lv kd hu li b fv ma lx l ly lz">dw = 0.83 + 1 * 0.1 * (1-0.83)<br/>dw = 0.847</span><span id="392c" class="lv kd hu li b fv ma lx l ly lz"><em class="lm">Jaro-Winkler distance</em> = 84.7%</span></pre><h1 id="3482" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated"><strong class="ak">差价率</strong></h1><p id="6527" class="pw-post-body-paragraph ir is hu it b iu la iw ix iy lb ja jb jc lc je jf jg ld ji jj jk le jm jn jo hn dt translated">创建这个功能的直觉是，我们认为如果产品的价格比<em class="lm">前100名最酷产品</em> ( <strong class="it hv">关键字</strong>)的价格高或低很多，那么这个产品与我们想要找到的关键字不匹配。</p><p id="d07e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">例如，摘自我们的一个关键词:<strong class="it hv">苹果Ipad Pro </strong></p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff mc"><img src="../Images/b5d1854182417385de10e9226c6f4500.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*5JOgeZSD2h26g-tmLTvGgw.png"/></div><figcaption class="md me fg fe ff mf mg bd b be z ek">Product that <strong class="bd mh">MATCH</strong> our keyword (<strong class="bd mh">Apple Ipad Pro</strong>)</figcaption></figure><figure class="jr js jt ju fq jv fe ff paragraph-image"><div class="fe ff mi"><img src="../Images/7af4b678ddd1415f79e03768b5849d66.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*OVy4MiVrqVcyIWkufITQrQ.png"/></div><figcaption class="md me fg fe ff mf mg bd b be z ek">Product that <strong class="bd mh">DO NOT</strong> <strong class="bd mh">MATCH</strong> our keyword (<strong class="bd mh">Apple Ipad Pro</strong>)</figcaption></figure><p id="3293" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><code class="eh lf lg lh li b">refer_price</code>苹果Ipad Pro的等于1081新币左右(使用汇率1美元= 1.37新币)。那么，我们可以根据差价率=(51–1081)/1081 ~ =-0.95得出结论。</p><p id="786c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">换句话说--&gt; 95%的价格差异<code class="eh lf lg lh li b">refer_price</code>和关键字的价格- &gt;在这种情况下，产品与我们的关键字不匹配的可能性很高- &gt; Apple Ipad Pro。</p><h1 id="f5ff" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated"><strong class="ak">计算<em class="mj"> Jaro-Winkler距离和价差比率</em> </strong></h1><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="315c" class="lv kd hu li b fv lw lx l ly lz">for index,row in data.iterrows():<br/>    data.loc[index,'dist_jw'] = L.jaro_winkler(row['name'], row['refer_name'])<br/>    <br/>data['price_diff_ratio'] = (data['price']-data['refer_price'])/data['refer_price']</span></pre><h1 id="e428" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated"><strong class="ak">可视化的<em class="mj"> Jaro-Winkler距离和价格差异比率</em> </strong></h1><p id="566c" class="pw-post-body-paragraph ir is hu it b iu la iw ix iy lb ja jb jc lc je jf jg ld ji jj jk le jm jn jo hn dt translated">使用下面的代码，您应该能够重现我们的结果。</p><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="c6dc" class="lv kd hu li b fv lw lx l ly lz">sns.scatterplot(data=data, x='dist_jw',y='price_diff_ratio', hue='status').set_title("Relationship between jaro-wrinkle distance and price difference ratio")</span></pre><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff mk"><img src="../Images/888de6f8f093fa0f3cf885fcb4f92feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r5KZ1uPEYLfjtGLQ5_7d6g.png"/></div></div></figure><p id="a126" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">哇！似乎应该存在一个能够区分匹配或不匹配关键字的搜索产品的界限(<strong class="it hv">状态= 0或1 </strong>)。</p><p id="f222" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">使用下面的代码，我们能够找到区分status =0和1的最佳水平线，并将其可视化。</p><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="d595" class="lv kd hu li b fv lw lx l ly lz">count_dict = {}<br/>x = min(data['price_diff_ratio'])</span><span id="22f6" class="lv kd hu li b fv ma lx l ly lz">while x&lt;2:<br/>  temp_data = data<br/>  temp_data['guess'] = [1 if price&gt;=x else 0 for price in       data['price_diff_ratio'] ]<br/>  correct = len(data[temp_data['status'] == temp_data['guess']])<br/>  count_dict[x] = correct<br/>  x = x+0.001</span><span id="a736" class="lv kd hu li b fv ma lx l ly lz">boundary_const = [max(count_dict, key=lambda x: count_dict[x])][0]</span><span id="d47d" class="lv kd hu li b fv ma lx l ly lz">ax = sns.scatterplot(<br/>    data=data[data['price_diff_ratio']&lt;=1], <br/>    x='dist_jw',y='price_diff_ratio', hue='status')</span><span id="4640" class="lv kd hu li b fv ma lx l ly lz">plt.axhline(y=boundary_const, color='r', linestyle='-')<br/>plt.show()</span></pre><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff ml"><img src="../Images/55c5d8f142d529c736bbb265a2584564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_f8X_V-WiO-QqBAk2ovZw.png"/></div></div></figure><p id="7127" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">从上图中，我们可以看到最好的水平状态分离线大约等于-0.55。</p><p id="d062" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">分类规则是:</p><ol class=""><li id="ca80" class="mm mn hu it b iu iv iy iz jc mo jg mp jk mq jo mr ms mt mu dt translated">低于-0.55将被归类为状态= 0。</li><li id="5ed9" class="mm mn hu it b iu mv iy mw jc mx jg my jk mz jo mr ms mt mu dt translated">高于0.55将被归类为状态= 1。</li></ol><p id="a1fb" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">基于以上分类规则，我们将能够获得大约94%的准确率！我们不应该把这个数字看得太重，因为我们实际上应该只对训练数据应用这个规则，以避免数据泄漏问题。所以这个数字是为了我们后面对机器学习模型预测有个大概的概念。</p><p id="eafa" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这种观察给了我们直觉，让我们创建一个机器学习模型来找到最佳边界，这样我们就能够拥有预测能力！</p><h1 id="2dc8" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated"><strong class="ak">为机器学习模型选择特征</strong></h1><p id="955a" class="pw-post-body-paragraph ir is hu it b iu la iw ix iy lb ja jb jc lc je jf jg ld ji jj jk le jm jn jo hn dt translated">有几种方法来选择要包括在我们的模型中的特征，对于我们的例子，我们使用<strong class="it hv"> <em class="lm"> p值</em> </strong>来选择合适的特征。</p><p id="0779" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">什么是p值？</p><blockquote class="na"><p id="346d" class="nb nc hu bd nd ne nf ng nh ni nj jo ek translated">假设<strong class="ak">零假设</strong>为真，找到更多极值的概率。</p></blockquote><p id="7599" class="pw-post-body-paragraph ir is hu it b iu nk iw ix iy nl ja jb jc nm je jf jg nn ji jj jk no jm jn jo hn dt translated">如果您想了解更多关于<strong class="it hv"> p值</strong>的信息，请访问以下链接:</p><ol class=""><li id="c1ac" class="mm mn hu it b iu iv iy iz jc mo jg mp jk mq jo mr ms mt mu dt translated"><a class="ae jp" href="https://en.wikipedia.org/wiki/P-value" rel="noopener ugc nofollow" target="_blank">维基</a></li><li id="a351" class="mm mn hu it b iu mv iy mw jc mx jg my jk mz jo mr ms mt mu dt translated"><a class="ae jp" href="https://www.statsdirect.com/help/basics/p_values.htm" rel="noopener ugc nofollow" target="_blank"> StatsDirect </a></li><li id="8cc7" class="mm mn hu it b iu mv iy mw jc mx jg my jk mz jo mr ms mt mu dt translated"><a class="ae jp" href="http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-correctly-interpret-p-values" rel="noopener ugc nofollow" target="_blank">博客</a></li></ol><p id="6eb0" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">如果变量的<strong class="it hv"> p值</strong>小于显著值，则该变量具有统计显著性，反之亦然。我们选择0.05作为我们的显著性水平。</p><p id="2eef" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">首先，我们对<code class="eh lf lg lh li b">dist_jw</code>、<code class="eh lf lg lh li b">price_diff_ratio</code>和<code class="eh lf lg lh li b">discount</code>变量运行<strong class="it hv"> logit </strong>模型。关于<strong class="it hv"> Logit型号</strong>的<strong class="it hv">细节</strong>解释，请参考以下链接。</p><ol class=""><li id="9f46" class="mm mn hu it b iu iv iy iz jc mo jg mp jk mq jo mr ms mt mu dt translated"><a class="ae jp" href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">机器学习掌握</a></li><li id="d2d2" class="mm mn hu it b iu mv iy mw jc mx jg my jk mz jo mr ms mt mu dt translated"><a class="ae jp" rel="noopener" href="/datadriveninvestor/machine-learning-model-logistic-regression-5fa4ffde5773">博客</a> @ <a class="ae jp" rel="noopener" href="/@amitabhadey?source=post_header_lockup">阿弥陀佛Dey </a></li></ol><p id="adf1" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">运行下面的代码开始吧！</p><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="b19d" class="lv kd hu li b fv lw lx l ly lz">logit_model=sm.Logit(data['status'],data[["dist_jw","price_diff_ratio", "discount"]])<br/>result=logit_model.fit()<br/>print(result.summary2())</span></pre><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff np"><img src="../Images/ebd54877ad9511bbece258b70d83117f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ixmOf0aQlO8tTIcZVEgQ-A.png"/></div></div></figure><p id="6a3e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们可以看到<code class="eh lf lg lh li b">discount</code> (0.8549)的p值<strong class="it hv">大于0.05，</strong>因此<strong class="it hv">贴现变量</strong>在统计上<strong class="it hv">不显著</strong>。其他变量如<code class="eh lf lg lh li b">dist_jw</code>和<code class="eh lf lg lh li b">price_diff_ratio</code> p值为小于0.05 的<strong class="it hv">，在统计上<strong class="it hv">显著</strong>，因此它们是<strong class="it hv">在我们后面的机器学习建模中将包含</strong>的变量。</strong></p><h1 id="7b07" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated"><strong class="ak">训练</strong>机器学习模型</h1><p id="0995" class="pw-post-body-paragraph ir is hu it b iu la iw ix iy lb ja jb jc lc je jf jg ld ji jj jk le jm jn jo hn dt translated">我们将使用三种基本的机器学习模型:</p><ol class=""><li id="2e91" class="mm mn hu it b iu iv iy iz jc mo jg mp jk mq jo mr ms mt mu dt translated"><a class="ae jp" href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">用于分类的逻辑回归</a></li><li id="bfa7" class="mm mn hu it b iu mv iy mw jc mx jg my jk mz jo mr ms mt mu dt translated"><a class="ae jp" href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47" rel="noopener" target="_blank">支持向量机进行分类</a> @ <a class="ae jp" href="https://towardsdatascience.com/@grohith327?source=post_header_lockup" rel="noopener" target="_blank">甘地</a></li><li id="6eb9" class="mm mn hu it b iu mv iy mw jc mx jg my jk mz jo mr ms mt mu dt translated"><a class="ae jp" href="https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd" rel="noopener" target="_blank">随机森林分类</a> @ <a class="ae jp" href="https://towardsdatascience.com/@n.donges?source=post_header_lockup" rel="noopener" target="_blank">尼克拉斯东格斯</a></li></ol><p id="05ef" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">等等…在训练我们的模型之前，我们仍然需要将我们的数据分成训练数据和测试数据。对于我们的例子，我们将把<strong class="it hv"> 80% </strong>的数据分割为训练数据，而<strong class="it hv"> 20% </strong>的数据分割为测试数据。</p><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="2f33" class="lv kd hu li b fv lw lx l ly lz">X_train, X_test, y_train, y_test = train_test_split(data[["dist_jw","price_ratio"]], data['status'], test_size=0.2, random_state=0)</span></pre><p id="a59c" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">让我们开始训练和预测我们的第一个机器学习模型。</p><p id="df45" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">用于分类的逻辑回归</strong></p><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="6772" class="lv kd hu li b fv lw lx l ly lz">logreg = LogisticRegression()<br/>logreg.fit(X_train, y_train)</span><span id="214d" class="lv kd hu li b fv ma lx l ly lz">y_pred = logreg.predict(X_test)<br/>print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))</span></pre><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff nq"><img src="../Images/f4d6bb0394422e5d3c28a2f84e7a9e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k27kP0zQEdZ7u9PpQwIBgA.png"/></div></div></figure><p id="d2f8" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">哇！！！仅使用两个特征，我们就能够获得92%的准确率，而无需微调我们的机器学习模型。好东西！92%的准确率将作为我们机器学习模型的基准。</p><p id="b0af" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">为了更形象化，我们绘制了通过逻辑回归计算边界！</p><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="bb3d" class="lv kd hu li b fv lw lx l ly lz">xx, yy = np.mgrid[-5:5:.01, -5:5:.01]<br/>grid = np.c_[xx.ravel(), yy.ravel()]<br/>probs = logreg.predict_proba(grid)[:, 1].reshape(xx.shape)</span><span id="c9ac" class="lv kd hu li b fv ma lx l ly lz">f, ax = plt.subplots(figsize=(8, 6))<br/>contour = ax.contourf(xx, yy, probs, 25, cmap="RdBu",<br/>                      vmin=0, vmax=1)<br/>ax_c = f.colorbar(contour)<br/>ax_c.set_label("$P(y = 1)$")<br/>ax_c.set_ticks([0, .25, .5, .75, 1])</span><span id="10e2" class="lv kd hu li b fv ma lx l ly lz">ax.scatter(X_train.iloc[:,0], X_train.iloc[:,1], c=y_train, s=50,<br/>           cmap="RdBu", vmin=-.2, vmax=1.2,<br/>           edgecolor="white", linewidth=1)</span><span id="16be" class="lv kd hu li b fv ma lx l ly lz">ax.set(aspect="equal",<br/>       xlim=(-5, 5), ylim=(-5, 5),<br/>       xlabel="$X_1$", ylabel="$X_2$")</span></pre><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff nr"><img src="../Images/04d78009b36db0256c97946dc470ae10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tAiklBv6BSGpT4eMOzts2g.png"/></div></div></figure><p id="33d7" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在可视化边界之后，我们继续绘制<a class="ae jp" href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a>。我参考这个<a class="ae jp" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html" rel="noopener ugc nofollow" target="_blank">链接</a>来绘制我们的混淆矩阵。</p><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="fb14" class="lv kd hu li b fv lw lx l ly lz">def plot_confusion_matrix(cm, classes,<br/>                          normalize=False,<br/>                          title='Confusion matrix',<br/>                          cmap=plt.cm.Blues):<br/>    """<br/>    This function prints and plots the confusion matrix.<br/>    Normalization can be applied by setting `normalize=True`.<br/>    """<br/>    if normalize:<br/>        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]<br/>        print("Normalized confusion matrix")<br/>    else:<br/>        print('Confusion matrix, without normalization')</span><span id="2ef0" class="lv kd hu li b fv ma lx l ly lz">    print(cm)</span><span id="26bd" class="lv kd hu li b fv ma lx l ly lz">    plt.imshow(cm, interpolation='nearest', cmap=cmap)<br/>    plt.title(title)<br/>    plt.colorbar()<br/>    tick_marks = np.arange(len(classes))<br/>    plt.xticks(tick_marks, classes, rotation=45)<br/>    plt.yticks(tick_marks, classes)</span><span id="d9bb" class="lv kd hu li b fv ma lx l ly lz">    fmt = '.2f' if normalize else 'd'<br/>    thresh = cm.max() / 2.<br/>    for i, j in itertools.product(range(cm.shape[0]),   range(cm.shape[1])):<br/>        plt.text(j, i, format(cm[i, j], fmt),<br/>                 horizontalalignment="center",<br/>                 color="white" if cm[i, j] &gt; thresh else "black")</span><span id="9f72" class="lv kd hu li b fv ma lx l ly lz">    plt.ylabel('True label')<br/>    plt.xlabel('Predicted label')<br/>    plt.tight_layout()</span><span id="92d7" class="lv kd hu li b fv ma lx l ly lz">confusion_mat = confusion_matrix(y_test, y_pred)<br/># Plot non-normalized confusion matrix<br/>plt.figure()<br/>plot_confusion_matrix(confusion_mat, classes=['Incorrect', 'Correct'], title='Confusion matrix, without normalization')</span></pre><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff ns"><img src="../Images/196395c192cfe54b7c75f361c7bf7890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TvT-lvJZjKoxKMjN56rhYg.png"/></div></div></figure><p id="5240" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">总之，239个测试数据中有220个预测正确。没有特定变量具有更高的错误预测率。</p><p id="af16" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">让我们进一步可视化<a class="ae jp" href="https://www.medcalc.org/manual/roc-curves.php" rel="noopener ugc nofollow" target="_blank"> ROC曲线</a>。</p><blockquote class="lj lk ll"><p id="1b7a" class="ir is lm it b iu iv iw ix iy iz ja jb ln jd je jf lo jh ji jj lp jl jm jn jo hn dt translated">在受试者工作特征(ROC)曲线中，真阳性率(灵敏度)被绘制成不同截止点的假阳性率(100特异性)的函数。</p><p id="0443" class="ir is lm it b iu iv iw ix iy iz ja jb ln jd je jf lo jh ji jj lp jl jm jn jo hn dt translated">ROC曲线上的每个点代表对应于特定决策阈值的灵敏度/特异性对。</p><p id="f663" class="ir is lm it b iu iv iw ix iy iz ja jb ln jd je jf lo jh ji jj lp jl jm jn jo hn dt translated">具有完美区分度(两个分布中没有重叠)的测试具有通过左上角的ROC曲线(100%灵敏度，100%特异性)。</p><p id="d736" class="ir is lm it b iu iv iw ix iy iz ja jb ln jd je jf lo jh ji jj lp jl jm jn jo hn dt translated">因此，ROC曲线越靠近左上角，测试的总体准确性越高(Zweig &amp; Campbell，1993)。</p><p id="d522" class="ir is lm it b iu iv iw ix iy iz ja jb ln jd je jf lo jh ji jj lp jl jm jn jo hn dt translated">——作者<a class="ae jp" href="https://www.medcalc.org/manual/roc-curves.php" rel="noopener ugc nofollow" target="_blank">https://www.medcalc.org/manual/roc-curves.php</a></p></blockquote><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="a9f6" class="lv kd hu li b fv lw lx l ly lz">logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))<br/>fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])<br/>plt.figure()<br/>plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)<br/>plt.plot([0, 1], [0, 1],'r--')<br/>plt.xlim([0.0, 1.0])<br/>plt.ylim([0.0, 1.05])<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.title('Receiver operating characteristic')<br/>plt.legend(loc="lower right")<br/>plt.savefig('Log_ROC')<br/>plt.show()</span></pre><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff nt"><img src="../Images/960c121111273f1549e9cdb792c58625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vlur90i241fmH8tvzQIW1g.png"/></div></div></figure><p id="5d9d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">我们可以观察到，对于逻辑回归，ROC曲线的面积(0.92)接近于1，这意味着逻辑回归模型的准确性很高！</p><p id="aaa7" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">支持向量机用于分类</strong></p><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="d809" class="lv kd hu li b fv lw lx l ly lz">clf = SVC(kernel='rbf')<br/>clf.fit(X_train, y_train)</span><span id="0502" class="lv kd hu li b fv ma lx l ly lz">y_pred = clf.predict(X_test)<br/>print('Accuracy of SVM classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))</span></pre><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff nu"><img src="../Images/9c3b1fa982deb6ed73730dc04174fdb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eNwOI9Ov41A__uWZ5XOkoQ.png"/></div></div></figure><p id="8584" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">用于分类的支持向量机以2%的优势击败了我们的基准92%!</p><p id="0098" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><strong class="it hv">随机森林进行分类</strong></p><pre class="jr js jt ju fq lr li ls lt aw lu dt"><span id="6fc7" class="lv kd hu li b fv lw lx l ly lz">clf = RandomForestClassifier(random_state=0)<br/>clf.fit(X_train, y_train)</span><span id="724e" class="lv kd hu li b fv ma lx l ly lz">y_pred = clf.predict(X_test)<br/>print('Accuracy of Random Forest classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))</span></pre><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff nv"><img src="../Images/9d07b2a219ad5d45e8325d1cb80468dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KSTJ4Kn2nfFYSvJK77X5vA.png"/></div></div></figure><p id="2733" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">随机森林分类以4%的优势击败了我们的基准92%，是我们测试的三个模型中最好的机器学习模型。我们可以做到<strong class="it hv"> 96% </strong>不需要任何机器学习模型的微调。<strong class="it hv">意思是如果我们能够创建一个好的特征，我们实际上不需要花费很多时间来微调我们的模型以达到理想的精度！</strong></p><h1 id="bae8" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated"><strong class="ak">进一步改进</strong></h1><ol class=""><li id="35c0" class="mm mn hu it b iu la iy lb jc nw jg nx jk ny jo mr ms mt mu dt translated">包括更多变量，例如每个关键词的喜欢、评论和评级。</li><li id="f42f" class="mm mn hu it b iu mv iy mw jc mx jg my jk mz jo mr ms mt mu dt translated">对每个关键字执行更多的字符串操作，为我们的分析和建模获得更多的搜索结果。</li></ol><h2 id="4dbf" class="lv kd hu bd ke nz oa ob ki oc od oe km jc of og kq jg oh oi ku jk oj ok ky ol dt translated">也阅读</h2><div class="om on fm fo oo op"><a href="https://hackernoon.com/who-carries-techs-top-100-products-of-the-year-a-machine-learning-analysis-11415d4cd746" rel="noopener  ugc nofollow" target="_blank"><div class="oq ab ej"><div class="or ab os cl cj ot"><h2 class="bd hv fv z el ou eo ep ov er et ht dt translated">谁拥有Tech年度100强产品？机器学习分析。</h2><div class="ow l"><h3 class="bd b fv z el ou eo ep ov er et ek translated">作为一名初级数据科学家，大多数时候训练数据都已经为我训练模型做好了准备(通过访问…</h3></div><div class="ox l"><p class="bd b gc z el ou eo ep ov er et ek translated">hackernoon.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd ka op"/></div></div></a></div><h2 id="e7b5" class="lv kd hu bd ke nz oa ob ki oc od oe km jc of og kq jg oh oi ku jk oj ok ky ol dt translated">编码愉快，欢迎在下面评论。</h2><p id="f778" class="pw-post-body-paragraph ir is hu it b iu la iw ix iy lb ja jb jc lc je jf jg ld ji jj jk le jm jn jo hn dt translated">如果你想让我们微调我们的机器学习模型，请在下面评论让我们知道！</p><p id="1161" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">代码链接:<a class="ae jp" href="https://github.com/KeXin95/top100gadget" rel="noopener ugc nofollow" target="_blank">百强小工具</a></p><p id="4470" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">欢迎<a class="ae jp" href="http://lowweihong.strikingly.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="it hv">联系我</strong> </a>也是:)</p><p id="8aca" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">请继续关注我的下一篇文章！</p></div></div>    
</body>
</html>