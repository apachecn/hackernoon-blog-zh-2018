<html>
<head>
<title>Deep Dream with TensorFlow: A Practical guide to build your first Deep Dream Experience</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow深度梦境:建立你的第一次深度梦境体验的实用指南</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/deep-dream-with-tensorflow-a-practical-guide-to-build-your-first-deep-dream-experience-f91df601f479?source=collection_archive---------1-----------------------#2018-12-29">https://medium.com/hackernoon/deep-dream-with-tensorflow-a-practical-guide-to-build-your-first-deep-dream-experience-f91df601f479?source=collection_archive---------1-----------------------#2018-12-29</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><blockquote class="ir"><p id="3e87" class="is it hu bd iu iv iw ix iy iz ja jb ek translated">想象力比知识更重要。因为<strong class="ak">的知识</strong>是有限的，而<strong class="ak">的想象力</strong>包容了整个世界，刺激了进步，催生了进化。”</p></blockquote><p id="6635" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jb hn dt jz translated">无论是机器学习工程师，还是深度学习专业人士，都会聚集在一些聚会或会议上，他们讨论的深度学习最常见的应用范围从对象检测，人脸识别，自然语言处理和语音识别，主要是由于自动驾驶汽车，亚马逊-Alexa或聊天机器人，但还有其他类型的应用不同于这些标准应用，这些应用不仅在人工智能领域，也在艺术领域引起了巨大的轰动。</p><p id="9f25" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">一个这样的应用是“<strong class="je hv">深梦”</strong>，它赋予了艺术家力量，反过来增强了我们的创造性启示，扩大了我们可以想象的空间。</p><p id="c1e1" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated"><strong class="je hv">深梦</strong>是一个<a class="ae kn" href="https://en.wikipedia.org/wiki/Computer_vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>程序，由<a class="ae kn" href="https://en.wikipedia.org/wiki/Google" rel="noopener ugc nofollow" target="_blank">谷歌</a>工程师<a class="ae kn" href="https://twitter.com/zzznah" rel="noopener ugc nofollow" target="_blank"> Alex Mordvintsev </a>创建，它使用<a class="ae kn" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a>通过<a class="ae kn" href="https://en.wikipedia.org/wiki/Algorithm" rel="noopener ugc nofollow" target="_blank">算法</a> <a class="ae kn" href="https://en.wikipedia.org/wiki/Pareidolia" rel="noopener ugc nofollow" target="_blank">幻觉</a>在的<a class="ae kn" href="https://en.wikipedia.org/wiki/Image" rel="noopener ugc nofollow" target="_blank">图像中寻找并增强模式，从而在图像中创建一个类似于</a>的<a class="ae kn" href="https://en.wikipedia.org/wiki/Hallucinogenic" rel="noopener ugc nofollow" target="_blank">致幻</a>外观</p><p id="e6f6" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">谷歌的计划推广了术语(深度)“做梦”，指的是在经过训练的<a class="ae kn" href="https://en.wikipedia.org/wiki/Deep_neural_network" rel="noopener ugc nofollow" target="_blank">深度网络</a>中产生期望的<a class="ae kn" href="https://en.wikipedia.org/wiki/Activation_(neural_network)" rel="noopener ugc nofollow" target="_blank">激活</a>的图像生成，该术语现在指的是相关方法的集合。</p><h2 id="17e4" class="ko kp hu bd kq kr ks kt ku kv kw kx ky jn kz la lb jr lc ld le jv lf lg lh li dt translated"><strong class="ak">从理论到实践</strong></h2><p id="2a29" class="pw-post-body-paragraph jc jd hu je b jf lj jh ji jj lk jl jm jn ll jp jq jr lm jt ju jv ln jx jy jb hn dt translated">接下来是我最喜欢的部分，在自学了<a class="ae kn" href="https://en.wikipedia.org/wiki/Google" rel="noopener ugc nofollow" target="_blank">谷歌</a>深度梦之后，是时候从<strong class="je hv">阅读器</strong>模式切换到<strong class="je hv">编码器</strong>模式，因为从这一点开始，我将只谈论代码，这与了解任何深度学习应用背后的概念同等重要。</p><p id="6a82" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">遵循这个一步一步的实践指南来创建你的第一个深度梦体验，但是在开始我的编码之旅之前，先看看我的深度梦图像，这些图像非常迷幻。</p><figure class="lp lq lr ls fq lt fe ff paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="fe ff lo"><img src="../Images/73d8290c57d68118dcff184099ab75ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h1DL2aTZd9xJb-AhviX0gg.png"/></div></div><figcaption class="ma mb fg fe ff mc md bd b be z ek">Image 1: Welcome to the trippy world of hallucinogenic images</figcaption></figure><p id="085e" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated"><strong class="je hv">免责声明</strong>:在开始本编码教程之前，请确保您在一个文件夹中有两个python文件，即download.py和inception5h.py，您可以从我在参考资料部分提到的GitHub资源库中获得，否则您会发现自己被“没有找到模块”错误所困扰，这无疑是一种痛苦</p><p id="1d4a" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">那么，让我们开始吧</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="ea63" class="ko kp hu mf b fv mj mk l ml mm">#This was developed using Python 3.6.3 (Anaconda) <br/>#Important library to import</span><span id="13b0" class="ko kp hu mf b fv mn mk l ml mm">%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>import tensorflow as tf<br/>import numpy as np<br/>import random<br/>import math</span><span id="408e" class="ko kp hu mf b fv mn mk l ml mm"># Image manipulation.<br/>from PIL import Image<br/>from scipy.ndimage.filters import gaussian_filter</span></pre><h1 id="9cf7" class="mo kp hu bd kq mp mq mr ku ms mt mu ky mv mw mx lb my mz na le nb nc nd lh ne dt translated">初始模型</h1><p id="9bd7" class="pw-post-body-paragraph jc jd hu je b jf lj jh ji jj lk jl jm jn ll jp jq jr lm jt ju jv ln jx jy jb hn dt translated">使用Inception 5h模型是因为它更容易使用:它接受任何大小的输入图像，并且它似乎比Inception v3模型创建了更漂亮的图片。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="aeb9" class="ko kp hu mf b fv mj mk l ml mm">import inception5h</span></pre><p id="492a" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">下载初始模型的数据。它的大小是50兆字节</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="849e" class="ko kp hu mf b fv mj mk l ml mm">inception5h.maybe_download()</span><span id="9ab1" class="ko kp hu mf b fv mn mk l ml mm">Downloading Inception 5h Model ...<br/>Data has apparently already been downloaded and unpacked.</span></pre><p id="eb0a" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">加载先启模型，这样它就可以使用了。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="b432" class="ko kp hu mf b fv mj mk l ml mm">model = inception5h.Inception5h()</span></pre><p id="1241" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">Inception 5h模型有许多层，可以用于深度做梦。但是为了便于参考，我们将只使用12个最常用的层。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="4eab" class="ko kp hu mf b fv mj mk l ml mm">len(model.layer_tensors)</span></pre><p id="359a" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">要了解inception 5h模型中的不同层</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="12d8" class="ko kp hu mf b fv mj mk l ml mm">def printTensors(pb_file):</span><span id="6946" class="ko kp hu mf b fv mn mk l ml mm"># read pb into graph_def<br/>    with tf.gfile.GFile(pb_file, "rb") as f:<br/>        graph_def = tf.GraphDef()<br/>        graph_def.ParseFromString(f.read())</span><span id="8de7" class="ko kp hu mf b fv mn mk l ml mm"># import graph_def<br/>    with tf.Graph().as_default() as graph:<br/>        tf.import_graph_def(graph_def)</span><span id="1346" class="ko kp hu mf b fv mn mk l ml mm"># print operations<br/>    for op in graph.get_operations():<br/>        print(op.name)</span><span id="ee7c" class="ko kp hu mf b fv mn mk l ml mm">printTensors("inception/5h/tensorflow_inception_graph.pb")</span></pre><h1 id="6504" class="mo kp hu bd kq mp mq mr ku ms mt mu ky mv mw mx lb my mz na le nb nc nd lh ne dt translated">图像处理的辅助功能</h1><p id="234f" class="pw-post-body-paragraph jc jd hu je b jf lj jh ji jj lk jl jm jn ll jp jq jr lm jt ju jv ln jx jy jb hn dt translated">这个函数加载一个图像，并以浮点numpy数组的形式返回。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="21f7" class="ko kp hu mf b fv mj mk l ml mm">def load_image(filename):<br/>    try:<br/>        original = Image.open(filename)<br/>        print("the size of the image is :")<br/>        print(original.format,original.size)<br/>    except:<br/>        print ("Unable to load image")</span><span id="020d" class="ko kp hu mf b fv mn mk l ml mm">return np.float32(original)</span></pre><p id="07af" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">将图像保存为jpeg文件。图像以numpy数组的形式给出，像素值在0到255之间。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="001d" class="ko kp hu mf b fv mj mk l ml mm">def save_image(image, filename):<br/>    # Ensure the pixel-values are between 0 and 255.<br/>    image = np.clip(image, 0.0, 255.0)<br/>    <br/>    # Convert to bytes.<br/>    image = image.astype(np.uint8)<br/>    <br/>    # Write the image-file in jpeg-format.<br/>    with open(filename, 'wb') as file:<br/>        Image.fromarray(image).save(file, 'jpeg')</span></pre><p id="8d0a" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">该函数绘制图像。使用matplotlib会产生低分辨率图像。使用PIL给出了漂亮的图片。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="bd34" class="ko kp hu mf b fv mj mk l ml mm">def plot_image(image):<br/>    # Assume the pixel-values are scaled between 0 and 255.<br/>    <br/>    if False:<br/>        # Convert the pixel-values to the range between 0.0 and 1.0<br/>        image = np.clip(image/255.0, 0.0, 1.0)<br/>        <br/>        # Plot using matplotlib.<br/>        plt.imshow(image, interpolation='lanczos')<br/>        plt.show()<br/>    else:<br/>        # Ensure the pixel-values are between 0 and 255.<br/>        image = np.clip(image, 0.0, 255.0)<br/>        <br/>        # Convert pixels to bytes.<br/>        image = image.astype(np.uint8)</span><span id="5c2d" class="ko kp hu mf b fv mn mk l ml mm"># Convert to a PIL-image and display it.<br/>        display(Image.fromarray(image))</span></pre><p id="f52b" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">归一化图像，使其值介于0.0和1.0之间。这对于绘制梯度很有用。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="9ad6" class="ko kp hu mf b fv mj mk l ml mm">def normalize_image(x):<br/>    # Get the min and max values for all pixels in the input.<br/>    x_min = x.min()<br/>    x_max = x.max()</span><span id="1db2" class="ko kp hu mf b fv mn mk l ml mm"># Normalize so all values are between 0.0 and 1.0<br/>    x_norm = (x - x_min) / (x_max - x_min)<br/>    <br/>    return x_norm</span></pre><p id="c745" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">该函数绘制归一化后的梯度</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="1727" class="ko kp hu mf b fv mj mk l ml mm">def plot_gradient(gradient):<br/>    # Normalize the gradient so it is between 0.0 and 1.0<br/>    gradient_normalized = normalize_image(gradient)<br/>    <br/>    # Plot the normalized gradient.<br/>    plt.imshow(gradient_normalized, interpolation='bilinear')<br/>    plt.show()</span></pre><p id="241c" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">这个函数调整图像的大小。它可以接受一个大小参数，你可以给它你想要的图像的像素大小，例如(100，200)。或者它可以带一个因子参数，你可以给它一个你想要的缩放因子，比如0.5，用来在每个维度上把图像的大小减半。</p><p id="5134" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">这是用PIL实现的，有点长，因为我们处理的是像素是浮点值的numpy数组。PIL不支持这一点，因此必须将图像转换为8位字节，同时确保像素值在适当的范围内。然后调整图像的大小并转换回浮点值。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="6176" class="ko kp hu mf b fv mj mk l ml mm">def resize_image(image, size=None, factor=None):<br/>    # If a rescaling-factor is provided then use it.<br/>    if factor is not None:<br/>        # Scale the numpy array's shape for height and width.<br/>        size = np.array(image.shape[0:2]) * factor<br/>        <br/>        # The size is floating-point because it was scaled.<br/>        # PIL requires the size to be integers.<br/>        size = size.astype(int)<br/>    else:<br/>        # Ensure the size has length 2.<br/>        size = size[0:2]<br/>    <br/>    # The height and width is reversed in numpy vs. PIL.<br/>    size = tuple(reversed(size))</span><span id="1d27" class="ko kp hu mf b fv mn mk l ml mm"># Ensure the pixel-values are between 0 and 255.<br/>    img = np.clip(image, 0.0, 255.0)<br/>    <br/>    # Convert the pixels to 8-bit bytes.<br/>    img = img.astype(np.uint8)<br/>    <br/>    # Create PIL-object from numpy array.<br/>    img = Image.fromarray(img)<br/>    <br/>    # Resize the image.<br/>    img_resized = img.resize(size, Image.LANCZOS)<br/>    <br/>    # Convert 8-bit pixel values back to floating-point.<br/>    img_resized = np.float32(img_resized)</span><span id="9c04" class="ko kp hu mf b fv mn mk l ml mm">return img_resized</span></pre><h1 id="9fe9" class="mo kp hu bd kq mp mq mr ku ms mt mu ky mv mw mx lb my mz na le nb nc nd lh ne dt translated">DeepDream算法</h1><h1 id="b0d3" class="mo kp hu bd kq mp mq mr ku ms mt mu ky mv mw mx lb my mz na le nb nc nd lh ne dt translated">梯度</h1><p id="bd0a" class="pw-post-body-paragraph jc jd hu je b jf lj jh ji jj lk jl jm jn ll jp jq jr lm jt ju jv ln jx jy jb hn dt translated">以下辅助函数计算输入图像的渐变，用于DeepDream算法。Inception 5h模型可以接受任何大小的图像，但是非常大的图像可能会使用许多千兆字节的RAM。为了保持较低的RAM使用率，我们将把输入图像分割成较小的图像块，并计算每个图像块的梯度。</p><p id="9a01" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">但是，这可能会导致DeepDream算法生成的最终图像中出现可见线条。因此，我们随机选择瓷砖，所以瓷砖的位置总是不同的。这使得瓷砖之间的接缝在最终的DeepDream图像中不可见。</p><p id="f1e9" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">这是一个帮助函数，用于确定合适的图块大小。期望的图块尺寸例如是400×400像素，但是实际的图块尺寸将取决于图像尺寸。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="0383" class="ko kp hu mf b fv mj mk l ml mm">def get_tile_size(num_pixels, tile_size=400):<br/>    """<br/>    num_pixels is the number of pixels in a dimension of the image.<br/>    tile_size is the desired tile-size.<br/>    """</span><span id="1a90" class="ko kp hu mf b fv mn mk l ml mm"># How many times can we repeat a tile of the desired size.<br/>    num_tiles = int(round(num_pixels / tile_size))<br/>    <br/>    # Ensure that there is at least 1 tile.<br/>    num_tiles = max(1, num_tiles)<br/>    <br/>    # The actual tile-size.<br/>    actual_tile_size = math.ceil(num_pixels / num_tiles)<br/>    <br/>    return actual_tile_size</span></pre><p id="7021" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">这个辅助函数计算输入图像的梯度。将图像分割成小块，并计算每个小块的梯度。瓷砖是随机选择的，以避免在最终的DeepDream图像中出现可见的接缝/线条。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="5cd8" class="ko kp hu mf b fv mj mk l ml mm">def tiled_gradient(gradient, image, tile_size=400):<br/>    # Allocate an array for the gradient of the entire image.<br/>    grad = np.zeros_like(image)</span><span id="9210" class="ko kp hu mf b fv mn mk l ml mm"># Number of pixels for the x- and y-axes.<br/>    x_max, y_max, _ = image.shape</span><span id="2d5f" class="ko kp hu mf b fv mn mk l ml mm"># Tile-size for the x-axis.<br/>    x_tile_size = get_tile_size(num_pixels=x_max, tile_size=tile_size)<br/>    # 1/4 of the tile-size.<br/>    x_tile_size4 = x_tile_size // 4</span><span id="0f3d" class="ko kp hu mf b fv mn mk l ml mm"># Tile-size for the y-axis.<br/>    y_tile_size = get_tile_size(num_pixels=y_max, tile_size=tile_size)<br/>    # 1/4 of the tile-size<br/>    y_tile_size4 = y_tile_size // 4</span><span id="8b1f" class="ko kp hu mf b fv mn mk l ml mm"># Random start-position for the tiles on the x-axis.<br/>    # The random value is between -3/4 and -1/4 of the tile-size.<br/>    # This is so the border-tiles are at least 1/4 of the tile-size,<br/>    # otherwise the tiles may be too small which creates noisy gradients.<br/>    x_start = random.randint(-3*x_tile_size4, -x_tile_size4)</span><span id="e44c" class="ko kp hu mf b fv mn mk l ml mm">while x_start &lt; x_max:<br/>        # End-position for the current tile.<br/>        x_end = x_start + x_tile_size<br/>        <br/>        # Ensure the tile's start- and end-positions are valid.<br/>        x_start_lim = max(x_start, 0)<br/>        x_end_lim = min(x_end, x_max)</span><span id="d8bc" class="ko kp hu mf b fv mn mk l ml mm"># Random start-position for the tiles on the y-axis.<br/>        # The random value is between -3/4 and -1/4 of the tile-size.<br/>        y_start = random.randint(-3*y_tile_size4, -y_tile_size4)</span><span id="7adf" class="ko kp hu mf b fv mn mk l ml mm">while y_start &lt; y_max:<br/>            # End-position for the current tile.<br/>            y_end = y_start + y_tile_size</span><span id="2b31" class="ko kp hu mf b fv mn mk l ml mm"># Ensure the tile's start- and end-positions are valid.<br/>            y_start_lim = max(y_start, 0)<br/>            y_end_lim = min(y_end, y_max)</span><span id="7075" class="ko kp hu mf b fv mn mk l ml mm"># Get the image-tile.<br/>            img_tile = image[x_start_lim:x_end_lim,<br/>                             y_start_lim:y_end_lim, :]</span><span id="f572" class="ko kp hu mf b fv mn mk l ml mm"># Create a feed-dict with the image-tile.<br/>            feed_dict = model.create_feed_dict(image=img_tile)</span><span id="b8cd" class="ko kp hu mf b fv mn mk l ml mm"># Use TensorFlow to calculate the gradient-value.<br/>            g = session.run(gradient, feed_dict=feed_dict)</span><span id="b1a4" class="ko kp hu mf b fv mn mk l ml mm"># Normalize the gradient for the tile. This is<br/>            # necessary because the tiles may have very different<br/>            # values. Normalizing gives a more coherent gradient.<br/>            g /= (np.std(g) + 1e-8)</span><span id="3f80" class="ko kp hu mf b fv mn mk l ml mm"># Store the tile's gradient at the appropriate location.<br/>            grad[x_start_lim:x_end_lim,<br/>                 y_start_lim:y_end_lim, :] = g<br/>            <br/>            # Advance the start-position for the y-axis.<br/>            y_start = y_end</span><span id="97af" class="ko kp hu mf b fv mn mk l ml mm"># Advance the start-position for the x-axis.<br/>        x_start = x_end</span><span id="99a0" class="ko kp hu mf b fv mn mk l ml mm">return grad</span></pre><h1 id="0efb" class="mo kp hu bd kq mp mq mr ku ms mt mu ky mv mw mx lb my mz na le nb nc nd lh ne dt translated">优化图像</h1><p id="4afa" class="pw-post-body-paragraph jc jd hu je b jf lj jh ji jj lk jl jm jn ll jp jq jr lm jt ju jv ln jx jy jb hn dt translated">这个函数是DeepDream算法的主要优化循环。它计算初始模型的给定层相对于输入图像的梯度。然后将梯度添加到输入图像，从而增加层张量的平均值。这个过程被重复多次，并且放大了初始模型在输入图像中看到的任何模式。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="03c3" class="ko kp hu mf b fv mj mk l ml mm"><strong class="mf hv">def</strong> optimize_image(layer_tensor, image,<br/>                   num_iterations=10, step_size=3.0, tile_size=400,<br/>                   show_gradient=<strong class="mf hv">False</strong>):<br/>    <em class="nf">"""</em><br/><em class="nf">    Use gradient ascent to optimize an image so it maximizes the</em><br/><em class="nf">    mean value of the given layer_tensor.</em><br/><em class="nf">    </em><br/><em class="nf">    Parameters:</em><br/><em class="nf">    layer_tensor: Reference to a tensor that will be maximized.</em><br/><em class="nf">    image: Input image used as the starting point.</em><br/><em class="nf">    num_iterations: Number of optimization iterations to perform.</em><br/><em class="nf">    step_size: Scale for each step of the gradient ascent.</em><br/><em class="nf">    tile_size: Size of the tiles when calculating the gradient.</em><br/><em class="nf">    show_gradient: Plot the gradient in each iteration.</em><br/><em class="nf">    """</em><br/><br/>    <em class="nf"># Copy the image so we don't overwrite the original image.</em><br/>    img = image.copy()<br/>    <br/>    print("Image before:")<br/>    plot_image(img)<br/><br/>    print("Processing image: ", end="")<br/><br/>    <em class="nf"># Use TensorFlow to get the mathematical function for the</em><br/>    <em class="nf"># gradient of the given layer-tensor with regard to the</em><br/>    <em class="nf"># input image. This may cause TensorFlow to add the same</em><br/>    <em class="nf"># math-expressions to the graph each time this function is called.</em><br/>    <em class="nf"># It may use a lot of RAM and could be moved outside the function.</em><br/>    gradient = model.get_gradient(layer_tensor)<br/>    <br/>    <strong class="mf hv">for</strong> i <strong class="mf hv">in</strong> range(num_iterations):<br/>        <em class="nf"># Calculate the value of the gradient.</em><br/>        <em class="nf"># This tells us how to change the image so as to</em><br/>        <em class="nf"># maximize the mean of the given layer-tensor.</em><br/>        grad = tiled_gradient(gradient=gradient, image=img)<br/>        <br/>        <em class="nf"># Blur the gradient with different amounts and add</em><br/>        <em class="nf"># them together. The blur amount is also increased</em><br/>        <em class="nf"># during the optimization. This was found to give</em><br/>        <em class="nf"># nice, smooth images. You can try and change the formulas.</em><br/>        <em class="nf"># The blur-amount is called sigma (0=no blur, 1=low blur, etc.)</em><br/>        <em class="nf"># We could call gaussian_filter(grad, sigma=(sigma, sigma, 0.0))</em><br/>        <em class="nf"># which would not blur the colour-channel. This tends to</em><br/>        <em class="nf"># give psychadelic / pastel colours in the resulting images.</em><br/>        <em class="nf"># When the colour-channel is also blurred the colours of the</em><br/>        <em class="nf"># input image are mostly retained in the output image.</em><br/>        sigma = (i * 4.0) / num_iterations + 0.5<br/>        grad_smooth1 = gaussian_filter(grad, sigma=sigma)<br/>        grad_smooth2 = gaussian_filter(grad, sigma=sigma*2)<br/>        grad_smooth3 = gaussian_filter(grad, sigma=sigma*0.5)<br/>        grad = (grad_smooth1 + grad_smooth2 + grad_smooth3)<br/><br/>        <em class="nf"># Scale the step-size according to the gradient-values.</em><br/>        <em class="nf"># This may not be necessary because the tiled-gradient</em><br/>        <em class="nf"># is already normalized.</em><br/>        step_size_scaled = step_size / (np.std(grad) + 1e-8)<br/><br/>        <em class="nf"># Update the image by following the gradient.</em><br/>        img += grad * step_size_scaled<br/><br/>        <strong class="mf hv">if</strong> show_gradient:<br/>            <em class="nf"># Print statistics for the gradient.</em><br/>            msg = "Gradient min: <strong class="mf hv">{0:&gt;9.6f}</strong>, max: <strong class="mf hv">{1:&gt;9.6f}</strong>, stepsize: <strong class="mf hv">{2:&gt;9.2f}</strong>"<br/>            print(msg.format(grad.min(), grad.max(), step_size_scaled))<br/><br/>            <em class="nf"># Plot the gradient.</em><br/>            plot_gradient(grad)<br/>        <strong class="mf hv">else</strong>:<br/>            <em class="nf"># Otherwise show a little progress-indicator.</em><br/>            print(". ", end="")<br/><br/>    print()<br/>    print("Image after:")<br/>    plot_image(img)<br/>    <br/>    <strong class="mf hv">return</strong> img</span></pre><h1 id="37c1" class="mo kp hu bd kq mp mq mr ku ms mt mu ky mv mw mx lb my mz na le nb nc nd lh ne dt translated">递归图像优化</h1><p id="9f47" class="pw-post-body-paragraph jc jd hu je b jf lj jh ji jj lk jl jm jn ll jp jq jr lm jt ju jv ln jx jy jb hn dt translated">初始模型是在相当小的图像上训练的。确切的大小还不清楚，但每个维度可能有200-300个像素。如果我们使用更大的图像，比如1920x1080像素，那么上面的<code class="eh ng nh ni mf b">optimize_image()</code>函数会给图像添加许多小图案。</p><p id="9048" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">这个辅助函数将输入图像缩小几次，并通过上面的<code class="eh ng nh ni mf b">optimize_image()</code>函数运行每个缩小的版本。这导致最终图像中的图案更大。这也加快了计算速度。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="169a" class="ko kp hu mf b fv mj mk l ml mm">def recursive_optimize(layer_tensor, image,<br/>                       num_repeats=4, rescale_factor=0.7, blend=0.2,<br/>                       num_iterations=10, step_size=3.0,<br/>                       tile_size=400):<br/>    """<br/>    Recursively blur and downscale the input image.<br/>    Each downscaled image is run through the optimize_image()<br/>    function to amplify the patterns that the Inception model sees.</span><span id="90ea" class="ko kp hu mf b fv mn mk l ml mm">Parameters:<br/>    image: Input image used as the starting point.<br/>    rescale_factor: Downscaling factor for the image.<br/>    num_repeats: Number of times to downscale the image.<br/>    blend: Factor for blending the original and processed images.</span><span id="1b96" class="ko kp hu mf b fv mn mk l ml mm">Parameters passed to optimize_image():<br/>    layer_tensor: Reference to a tensor that will be maximized.<br/>    num_iterations: Number of optimization iterations to perform.<br/>    step_size: Scale for each step of the gradient ascent.<br/>    tile_size: Size of the tiles when calculating the gradient.<br/>    """</span><span id="3c14" class="ko kp hu mf b fv mn mk l ml mm"># Do a recursive step?<br/>    if num_repeats&gt;0:<br/>        # Blur the input image to prevent artifacts when downscaling.<br/>        # The blur amount is controlled by sigma. Note that the<br/>        # colour-channel is not blurred as it would make the image gray.<br/>        sigma = 0.5<br/>        img_blur = gaussian_filter(image, sigma=(sigma, sigma, 0.0))</span><span id="bd45" class="ko kp hu mf b fv mn mk l ml mm"># Downscale the image.<br/>        img_downscaled = resize_image(image=img_blur,<br/>                                      factor=rescale_factor)<br/>            <br/>        # Recursive call to this function.<br/>        # Subtract one from num_repeats and use the downscaled image.<br/>        img_result = recursive_optimize(layer_tensor=layer_tensor,<br/>                                        image=img_downscaled,<br/>                                        num_repeats=num_repeats-1,<br/>                                        rescale_factor=rescale_factor,<br/>                                        blend=blend,<br/>                                        num_iterations=num_iterations,<br/>                                        step_size=step_size,<br/>                                        tile_size=tile_size)<br/>        <br/>        # Upscale the resulting image back to its original size.<br/>        img_upscaled = resize_image(image=img_result, size=image.shape)</span><span id="a8c9" class="ko kp hu mf b fv mn mk l ml mm"># Blend the original and processed images.<br/>        image = blend * image + (1.0 - blend) * img_upscaled</span><span id="b13a" class="ko kp hu mf b fv mn mk l ml mm">print("Recursive level:", num_repeats)</span><span id="8326" class="ko kp hu mf b fv mn mk l ml mm"># Process the image using the DeepDream algorithm.<br/>    img_result = optimize_image(layer_tensor=layer_tensor,<br/>                                image=image,<br/>                                num_iterations=num_iterations,<br/>                                step_size=step_size,<br/>                                tile_size=tile_size)<br/>    <br/>    return img_result</span></pre><h1 id="433a" class="mo kp hu bd kq mp mq mr ku ms mt mu ky mv mw mx lb my mz na le nb nc nd lh ne dt translated">张量流会话</h1><p id="5298" class="pw-post-body-paragraph jc jd hu je b jf lj jh ji jj lk jl jm jn ll jp jq jr lm jt ju jv ln jx jy jb hn dt translated">我们需要一个TensorFlow会话来执行图表。这是一个互动的会议，所以我们可以继续添加梯度函数到计算图。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="ab62" class="ko kp hu mf b fv mj mk l ml mm">session = tf.InteractiveSession(graph=model.graph)</span></pre><p id="bf21" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">是时候运行算法了</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="87b5" class="ko kp hu mf b fv mj mk l ml mm">#load the image which you want to process<br/>image=load_image(filename='test_output/test_output_11.jpg')<br/>plot_image(image)</span><span id="7b86" class="ko kp hu mf b fv mn mk l ml mm"># the size of the image is :<br/># JPEG (780, 1040)</span></pre><figure class="lp lq lr ls fq lt fe ff paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="fe ff nj"><img src="../Images/50067c29dc5b4fae95eeb0bd8f676064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*333UZR6Lmc_-xjnBSVtf1A.jpeg"/></div></div><figcaption class="ma mb fg fe ff mc md bd b be z ek">Image 2 : That’s me few years back</figcaption></figure><p id="f838" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">首先，我们需要在盗梦模型中引用张量，我们将在DeepDream优化算法中最大化它。在这种情况下，我们选择初始模型的整个第三层(层索引2)。它有192个频道，我们将尝试最大化所有这些频道的平均值。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="1334" class="ko kp hu mf b fv mj mk l ml mm">layer_tensor = model.layer_tensors[2]<br/>layer_tensor</span><span id="bd73" class="ko kp hu mf b fv mn mk l ml mm"># &lt;tf.Tensor 'conv2d2:0' shape=(?, ?, ?, 192) dtype=float32&gt;</span></pre><p id="e404" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">递归应用深度梦算法。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="8d36" class="ko kp hu mf b fv mj mk l ml mm">img_result = recursive_optimize(layer_tensor=layer_tensor, image=image,<br/>                 num_iterations=10, step_size=3.0, rescale_factor=0.7,<br/>                 num_repeats=4, blend=0.2)</span></pre><figure class="lp lq lr ls fq lt fe ff paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="fe ff nj"><img src="../Images/a75eb4b99a85984fe5d8879b7c74f6e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6V_LrSlnSrbQKTv-dxrQFw.jpeg"/></div></div><figcaption class="ma mb fg fe ff mc md bd b be z ek">Image 3: After applying Deep Dream to my image</figcaption></figure><p id="7876" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">现在我们将在初始模型中最大化一个更高层。在这种情况下，它是第7层(索引6)。该层识别输入图像中更复杂的形状，因此DeepDream算法将产生更复杂的图像。这一层似乎在识别狗脸和皮毛，因此DeepDream算法将它们添加到了图像中。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="d515" class="ko kp hu mf b fv mj mk l ml mm">layer_tensor = model.layer_tensors[6]<br/>img_result = recursive_optimize(layer_tensor=layer_tensor, image=image,<br/>                 num_iterations=10, step_size=3.0, rescale_factor=0.7,<br/>                 num_repeats=4, blend=0.2)</span></pre><figure class="lp lq lr ls fq lt fe ff paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="fe ff nk"><img src="../Images/2352cba9b1de53152364cd1c56dfa5eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yIvD7-4rYPa5uxGovtpGFw.jpeg"/></div></div><figcaption class="ma mb fg fe ff mc md bd b be z ek">Image 4: After applying Deep Dream Algorithm</figcaption></figure><p id="9934" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">这是一个使用DeepDream算法仅最大化图层要素通道子集的示例。在这种情况下，索引为10的层和只有其前3个特征通道被最大化。</p><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="1916" class="ko kp hu mf b fv mj mk l ml mm">layer_tensor = model.layer_tensors[10][:,:,:,0:3]<br/>img_result = recursive_optimize(layer_tensor=layer_tensor, image=image,<br/>                 num_iterations=10, step_size=3.0, rescale_factor=0.7,<br/>                 num_repeats=4, blend=0.2)</span></pre><figure class="lp lq lr ls fq lt fe ff paragraph-image"><div class="fe ff nl"><img src="../Images/7c2276d69d28aa978cd966e0bd6e28f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*IwugQk8YCw5ynt5lOC_Zuw.jpeg"/></div><figcaption class="ma mb fg fe ff mc md bd b be z ek">Image 5: After applying Deep Dream Algorithm</figcaption></figure><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="ff6a" class="ko kp hu mf b fv mj mk l ml mm">layer_tensor = model.layer_tensors[4]<br/>img_result = recursive_optimize(layer_tensor=layer_tensor, image=image,<br/>                 num_iterations=10, step_size=3.0, rescale_factor=0.7,<br/>                 num_repeats=4, blend=0.2)</span></pre><figure class="lp lq lr ls fq lt fe ff paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="fe ff nm"><img src="../Images/78fa52338713d33d44f2d4aa65795887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QcvBkMFn-IO2yZvwwiw64w.jpeg"/></div></div><figcaption class="ma mb fg fe ff mc md bd b be z ek">Image 6: After applying Deep Dream Algorithm</figcaption></figure><pre class="lp lq lr ls fq me mf mg mh aw mi dt"><span id="278a" class="ko kp hu mf b fv mj mk l ml mm"># To save the final Output</span><span id="fca9" class="ko kp hu mf b fv mn mk l ml mm">image_save=save_image(img_result,"test_output/test_output_12.jpg")</span></pre><p id="87bd" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">如果这还不够，我在YouTube上上传了一个视频，它将进一步扩展你的迷幻体验。</p><figure class="lp lq lr ls fq lt"><div class="bz el l di"><div class="nn no l"/></div></figure><p id="533d" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">结论:就是这样，这篇文章向你展示了如何使用张量流和一些概念，你也可以自己创造一个深刻的梦境体验。</p><p id="9eaf" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">特别说明:如果没有Magnus Erik Hvass Pedersen通过他著名的TensorFlow教程给出的指导，这篇文章是不可能完成的。GitHub库可以在这里找到<a class="ae kn" href="https://github.com/Hvass-Labs" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="498f" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">资源:</p><ol class=""><li id="83c2" class="np nq hu je b jf ki jj kj jn nr jr ns jv nt jb nu nv nw nx dt translated">对于<strong class="je hv"> GitHub </strong>库点击<a class="ae kn" href="https://github.com/ElephantHunters/Deep-Dream-using-Tensorflow" rel="noopener ugc nofollow" target="_blank">这里</a>。</li><li id="1be4" class="np nq hu je b jf ny jj nz jn oa jr ob jv oc jb nu nv nw nx dt translated">为了增加对深层梦想的理解，请阅读谷歌研究博客文章。</li></ol><figure class="lp lq lr ls fq lt fe ff paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="fe ff od"><img src="../Images/4f3fa3da2f3b53eea7f6967651ff8176.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S0Yy7saEymxsZADJsrhBVw.jpeg"/></div></div></figure><p id="26ca" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated"><strong class="je hv">感谢您的关注</strong></p><p id="4bda" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">你用<em class="nf">你的</em>时间来阅读<em class="nf">我的</em>工作对我来说意味着一切。我完全是这个意思。</p><p id="89c9" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">如果你喜欢这个故事，疯狂鼓掌吧👏<strong class="je hv"> ) </strong>按钮！这将有助于其他人找到我的工作。</p><p id="5f85" class="pw-post-body-paragraph jc jd hu je b jf ki jh ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jb hn dt translated">此外，如果你愿意，可以在Medium、LinkedIn或Twitter上关注我！我很乐意。</p><div class="oe of fm fo og oh"><a rel="noopener follow" target="_blank" href="/@naveenmanwani"><div class="oi ab ej"><div class="oj ab ok cl cj ol"><h2 class="bd hv fv z el om eo ep on er et ht dt translated">纳文·曼瓦尼培养基</h2><div class="oo l"><h3 class="bd b fv z el om eo ep on er et ek translated">阅读纳文·曼瓦尼在媒介上的作品。一个机器学习工程师，一个深度学习爱好者|谷歌印度…</h3></div><div class="op l"><p class="bd b gc z el om eo ep on er et ek translated">medium.com</p></div></div><div class="oq l"><div class="or l os ot ou oq ov ly oh"/></div></div></a></div><div class="oe of fm fo og oh"><a href="https://www.linkedin.com/in/naveen-manwani-65491678/" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab ej"><div class="oj ab ok cl cj ol"><h2 class="bd hv fv z el om eo ep on er et ht dt translated">Naveen Manwani -机器学习工程师- AIMonk Labs Private Ltd | LinkedIn</h2><div class="oo l"><h3 class="bd b fv z el om eo ep on er et ek translated">查看纳文·曼瓦尼在全球最大的职业社区LinkedIn上的个人资料。Naveen有一份工作列在他们的…</h3></div><div class="op l"><p class="bd b gc z el om eo ep on er et ek translated">www.linkedin.com</p></div></div><div class="oq l"><div class="ow l os ot ou oq ov ly oh"/></div></div></a></div><div class="oe of fm fo og oh"><a href="https://twitter.com/NaveenManwani17" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab ej"><div class="oj ab ok cl cj ol"><h2 class="bd hv fv z el om eo ep on er et ht dt translated">纳文·曼瓦尼(@纳文·曼瓦尼17) |推特</h2><div class="oo l"><h3 class="bd b fv z el om eo ep on er et ek translated">纳文·曼瓦尼的最新推文(@纳文·曼瓦尼17)。机器学习工程师@ AIMONK Labs Pvt ltd，深…</h3></div><div class="op l"><p class="bd b gc z el om eo ep on er et ek translated">twitter.com</p></div></div><div class="oq l"><div class="ox l os ot ou oq ov ly oh"/></div></div></a></div></div></div>    
</body>
</html>