# 人工智能介入降低自杀率

> 原文：<https://medium.com/hackernoon/ai-intervenes-to-bring-down-suicide-rates-33b771c098c8>

![](img/15acf11be76eedc315f2c230c1e02237.png)

Photo by [Oscar Keys](https://unsplash.com/photos/AmPRUnRb6N0?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/search/photos/suicide?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

虽然从医学文献中找不到准确识别自杀患者的可靠方法，但研究人员仍在不断努力寻找人工智能解决方案来预测和防止自杀企图。

根据[疾病控制和预防中心(CDC)的数据，自杀被列为第十大死亡原因，也是呈上升趋势的三大原因之一。](https://www.cdc.gov/media/releases/2018/p0607-suicide-prevention.html)

人工智能工具正在努力部署，以解决不断上升的自杀风险。

**用于识别自杀想法的机器学习**

在最近的一项研究中，范德比尔特大学的研究人员通过查看成年患者的电子健康记录，应用机器学习(ML)来克服预测自杀企图的传统方法的局限性。结果显示，ML 准确地预测了未来的自杀企图，在自杀事件发生一周内的准确率为 84%到 92%。

2017 年，来自[卡耐基梅隆大学](https://www.cmu.edu/dietrich/news/news-stories/2017/october/brain-imaging-science-identifies-suicidal-thoughts.html)的(CMU) Marcel Just 和匹兹堡大学的 David Brent 的研究人员开发了一种有望帮助识别个人自杀意图的方法。这项研究部分由国家心理健康研究所资助，研究分析了大脑对死亡和生命相关概念的区分和反应的变化，如“死亡”、“残忍”、“麻烦”、“无忧无虑”、“好”和“赞美”。

他们的研究发表在[自然人类行为](https://www.nature.com/articles/s41562-017-0234-y)上，依赖于机器学习算法(**高斯朴素贝叶斯**)。研究的中心点是通过调查他们对死亡相关话题的想法来识别自杀想法。

**NLP 识别社交媒体行为**

识别用户社交媒体上的语言模式也可以导致早期干预和阻止自杀企图。

在人们在脸书现场直播上广播他们的自杀事件后，脸书开始通过帖子寻找自杀想法的迹象。利用人工智能，[脸书](https://www.facebook.com/zuck/posts/10104242660091961)扫描用户的帖子，寻找自杀行为的模式。然后，它会将帖子标记给人类版主，版主会通过向用户发送心理健康资源来做出回应。在紧急情况下，公司会联系第一反应人员，他们会试图找到该人。此外，Save.org 和国家自杀预防生命线等 80 个地方合作伙伴与脸书合作，制定关于自残和自杀相关内容的政策。

但是，脸书并不是唯一一个帮助最脆弱人群的国家。

在搜索“自杀方法”或“自杀想法”时，谷歌搜索会立即将用户带到一个页面，提供 24*7 自杀预防生命线的信息，以及帮助克服自杀感觉的在线聊天。对于谷歌搜索自杀或自残的方式，“自动完成”是禁用的。通常的谷歌搜索依赖于预测分析和自动完成，因此用户可以跳过在搜索空间键入整个句子。

根据谷歌的自动完成政策指南，对自杀搜索的预测属于有害或危险行为，因此不会产生预期的结果。

**识别焦虑和抑郁的聊天机器人**

根据世界卫生组织(世卫组织)的数据，估计 60%的自杀者患有严重的抑郁症。不同版本的自杀预防人工智能工具正在被部署，以解决可能导致自杀的抑郁症等症状。

Woebot 是一款对话聊天机器人，旨在识别青少年的焦虑和抑郁症状。这个有着古怪幽默感的聊天机器人通过图表跟踪情绪，并使用认知行为疗法(CBT)每周显示进展。换句话说，它为“所有使用他的人创造了治疗性对话的体验。”

[危机短信热线](https://www.crisistextline.org/blog/ava2)，一个免费的 24/7 短信热线提供了一个检测自杀风险的人工智能解决方案。在深度神经网络和自然语言处理(NLP)的支持下，危机短信热线可以在 5 分钟内为 94%的高危短信用户提供服务。该模型的本质在于其通过“*阅读句子中的可变性并理解上下文*”来预测风险的能力，对数千个高风险单词和单词组合进行分类。模型预测和来自危机顾问的实时反馈回路的结合是重新训练模型的关键。

**AI 可能的拦路虎**

一个更大的争论是关于人工智能解决方案在复杂的道德和隐私领域的合规性。公司经常发现自己在隐私问题和自杀预防之间难以找到平衡。在这种情况下，让用户信任创新的应用程序仍然至关重要。

2014 年，英国慈善机构[撒马利亚人](https://www.theguardian.com/society/2014/nov/07/samaritans-radar-app-suicide-watch-privacy-twitter-users)暂停了其自杀预防雷达应用程序，原因是担心隐私及其可能被网络恶霸滥用。在处理案件之前，何时以及如何干预也需要一定程度的确定性。通过识别自杀想法进行干预可能会增加[假阳性](https://www.scientificamerican.com/article/ai-algorithms-to-prevent-suicide-gain-traction/)的风险。

但是考虑到大规模的不懈研究努力，临床医生可能很快就会发现人工智能在识别自杀意图方面很有用，这将意味着拯救生命。

***如果你在美国面临痛苦或自杀危机，你可以立即与国家自杀预防热线(800–273–8255，***[***suicidepreventionlifeline.org***](https://suicidepreventionlifeline.org/)***)或危机短信热线(Text HOME to 741–741)的人交谈。***