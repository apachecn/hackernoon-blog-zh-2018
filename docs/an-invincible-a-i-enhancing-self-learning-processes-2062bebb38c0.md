# 一个不可战胜的人工智能:加强自我学习过程

> 原文：<https://medium.com/hackernoon/an-invincible-a-i-enhancing-self-learning-processes-2062bebb38c0>

![](img/98af3acb7abb75ae25ec1bfe6c0a3e0f.png)

Photo by [Franck V.](https://unsplash.com/photos/rDxP1tF3CmA?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/search/photos/artificial-intelligence?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

构建一个原始的环境来培育一个不断增长的、强大的、能够将数据转化为见解的学习实体是一项不可回避的挑战性任务。有必要认识到，自动化迫使我们失去对人工智能挖掘的海量数据的控制和管理。上述相关问题的根源在于数据的易变性，因为偏见和腐败的定义会随着情况和环境条件的变化而变化。

我们观察到这个问题在人类儿童的成长过程中提出了同样的分支，人类儿童是一个能够积极参与环境的实体，即从经验和事件中提取数据，以逐渐塑造他们的信念、判断、预测等。通过训练一个孩子相信 2 + 2 = 5 或者地球是平的，我们可以改变他们的感知来产生不利的结果(例如，相信刻板印象)。然而，决定对错和真假的解决方案源于对背景和结果的逻辑探索。

正如文章《人工智能有偏见吗？([https://iq.intel.com/is-artificial-intelligence-prejudiced/](https://iq.intel.com/is-artificial-intelligence-prejudiced/))，当人工智能只接触到一部分数据时，偏差就会被隐含地添加到结果中。正如文章中所讨论的，解决这一漏洞的方法是让人工智能解释其方法和结论，并要求更多的证据。因此，我认为限制人工智能对虚假数据的脆弱性的关键是识别和推导逻辑推理/结论，以处理虚假数据的含义。想象一下突然的时间流逝，人工智能在面对可能有偏见的数据时，可以通过增强的计算能力来研究数据的未来后果，这种计算能力声称是错误的信息，并观察环境的特征是否与我们当前的世界和理想相符。

将人工智能暴露在不相关、过时、有偏见的数据中，使得[算法](https://hackernoon.com/tagged/algorithms)能够利用“嘈杂的信号”并质疑证据。如果我们要向一个自我学习的人工智能提供一个数据集，其中包含错误的数学计算或个人错误地自我报告的医疗诊断，那么通过实验(类似于人类的学习模式)探索似乎错误的场景的背景和后果将被证明是一个强大的解决方案:例如，如果个人自我报告他们患有某种疾病，他们根据某些症状的出现诊断出这种疾病，人工智能应该能够检查疾病和症状之间的相关性，以及解决可能影响分析的后果和潜在因素的场景。

我认为，作为一个自我学习的实体(类似于人类)，人工智能需要使用几个关键要素来实现网络安全，并防止与未知来源和非常规数据相关的操纵/偏见/谬误。这些元素的灵感来自于人类行为的心理学视角，以及噪音/错误/无关数据可以提高自然神经网络性能的理论。在这里，我认为虚假数据的定义范围从未经准备的数据到包含偏见/成见的数据。

**1。情景意识:**理解情景之间的异同。虚假数据可能被用来得出不正确的偏差和预测。例如，人工智能算法可能会根据“30/100 的人在 X 医疗手术后死亡”的数据预测使用 X 医疗手术是危险的，而没有考虑到 70/100 的人在 X 手术后仍然活着。情境意识对人工智能的重要性反映在这个问题发生在人工智能的人类智能对手身上的可能性很高:框架使我们在一个场景中放弃另一个场景的价值；例如，如果我们被告知，如果我们不使用节能方法，我们将损失 500 美元，那么我们更有可能使用节能方法，而不是被告知，如果我们使用这些方法，我们将获得相同的金额。在推荐系统、诸如自然语言处理的应用等中，可以为诸如跟踪虚假新闻的传播之类的任务实现上下文感知的分层递归/前馈神经网络。

**2。避免错误的模式识别/关联:**当人工智能算法处理数据时，最常见的偏差来源之一是在数据中找到实际上由随机波动引起的不存在的模式；换句话说，相关性不是因果关系。在 ML 算法的初始开发和训练中，目标是开发一个具有足够预测能力和/或能够发现数据模式的模型。因此，我们可以观察到与神经网络的最大优势之一(即，强大的模式识别技能)相关联的明显劣势。这就是为什么深度神经网络可以很容易地被愚弄，以做出图像中的对象是可识别的(实际上，并不是不可识别的)的高置信度预测，正如论文“深度神经网络很容易被愚弄:对不可识别图像的高置信度预测”(【http://www.evolvingai.org/fooling】)中所发现的。人工智能受到错误关联影响的一个有意义的例子是包含刻板印象的数据:“女性”和“接待员”单词嵌入之间的高度相关性导致人工智能算法旨在识别潜在的求职者以形成关联，并且它呈现了女性简历中刻板印象角色的增加(例子取自[“人工智能有偏见吗？”](https://iq.intel.com/is-artificial-intelligence-prejudiced/)文章)。避免这些问题的关键是对假设进行替代测试——浏览证据和相关的逻辑推理，此外还要探索影响关联的替代假设/因素:例如，认为一个篮球运动员如果连续几次投篮命中，她下一次投篮的可能性更大是不正确的。人工智能应该能够理解，玩家不出手/会出手的可能性是相等的，因此应该能够以怀疑的态度对待支持这种“热手”理论的理论。类似地，人工智能也应该能够通过更新其模型和与模式的成功或错误预测相关的环境视角来适应不稳定的数据。

**3。谨慎对待虚假数据的可用性:**正如我在这篇文章中谈到的，提供更多的证据/数据(除了虚假数据之外)可以帮助人工智能形成正确的假设，并限制其对虚假数据的脆弱性。错误/过时/损坏数据的更高频率/可用性可能会暴露人工智能神经网络的漏洞。举个例子(摘自[《人工智能有偏见吗](https://iq.intel.com/is-artificial-intelligence-prejudiced/)文章)，这是一个基于应用的程序，允许用户报告波士顿市的坑洞，收集的数据被输入到人工智能技术中。由于频繁报告坑洞和频繁使用智能手机，人工智能错误地预测了中高收入社区的更多坑洞。因此，频繁暴露于可用的错误数据会导致预测的偏差和不利结果。如果我们在数据集上训练人工智能，其中包含比医疗问题(如心脏病发作)更频繁的自然灾害死亡案例，以通过某种方法预测死亡概率，我们可以看到易受过时/虚假数据影响的人工智能可能会预测自然灾害死亡的更高可能性。我们可以防止这一问题的解决方案是训练自动人工智能了解虚假数据的来源，并利用更多“代表性”数据提供当前的真实世界视角，以消除对更高/更低可能性的评估。

**4。在自动化和规则之间找到平衡:**正如在这篇文章中所讨论的，我们可以想到人工智能对错误数据的脆弱性可以通过两种主要方式暴露出来:人类程序员的偏见，或者人工智能在学习过程中做出的假设。这最后一个要素试图解决这样一个问题:当提供给人工智能的信息有偏差时，人工智能也会有偏差。或许，可以通过在自动化和预编程偏差之间进行平衡来找到解决方案，因为人工智能遵循的经验法则可能会导致系统性偏差和漏洞。例如，人类判断中的一个谬误(可以反映在人工智能中)是，我们从一个锚开始——一个思维过程的起点——并倾向于相应地调整我们的思维。如果我们向人工智能输入或暴露有偏见的数据，它将开始围绕这种偏见做出决定，并将这种思维模式/观点应用于其预测、对人类行为和行动的分析。确定我们应该通过这些“锚”在多大程度上影响人工智能的学习过程是程序员的责任，但限制人工智能对虚假数据的脆弱性的一个关键解决方案是减少程序员可能产生负面结果的偏见的参与/潜在投入。

这些元素作为人工智能技术的指南，用于创造安全的人工智能技术，能够挖掘和处理虚假数据，而不会产生负面结果。这些元素可以被纳入人工智能的治理，以增强网络安全，并试图解开这些强大技术的黑匣子。

因此，不管由于薄弱的网络安全或人类程序员的偏见，人工智能可能会将有偏见、有缺陷和损坏的数据作为输入，我们都可以通过将算法暴露于整体环境(包括虚假和当前的真实数据)来限制人工智能对这些数据的脆弱性。如上所述，另一种解决方案是通过更新与成功或错误预测相关的环境模型和视角来(在人工智能中)结合对易变数据的适应。

然而，最终的问题是关于“虚假”数据的定义。正如我上面提到的，这可能意味着数据是没有准备好的/过时的，或者数据是有偏见的/刻板的。人工智能的进化是为了应对一个充满易变数据的丰富环境，这一点至关重要。

我认为，如果我们想在构建人工智能方面融入对虚假与真实的终极直觉和内在理解，以作为沉浸式环境中的积极参与者，我们应该从与梦有关的人类行为中获得灵感。类似于《黑客帝国》,我们在做梦时被输入“错误的”数据/环境信号，这些信号改变了我们的感知。特别是，我们应该模拟和反映与发现虚构的梦结构不是真实的相关联的实现，以发展人工智能需要从真相中确定虚假数据/场景/暗示或虚假环境的视角。

我同意，允许更大的“数据流”将有助于限制人工智能对虚假数据的脆弱性，此外还有同样重要的“批判性思维”技能的实施。人类参与的增加和合作肯定会提高人工智能可以有效过滤的数据的代表性。因此，数据的可靠性/准确性和完整性之间似乎存在差距；这个问题反映在人工智能和[机器学习](https://hackernoon.com/tagged/machine-learning)中，其中一致的“数据流”需要整合准确性和完整性。

面对塑造人工智能锚的问题，决策者的一个想法/指导方针是限制人类参与人工智能的自我学习过程，并帮助建立人工智能从业者可以用来建立技术的指导方针/技术，通过更新与成功或错误预测相关的环境模型和视角来构建锚。因此，我认为锚应该由人工智能根据环境因素、个人和对象的参与以及政策制定者、机器学习和人工智能从业者以及教育组织和机构之间的合作产生的特定需求/最佳实践来构建。人工智能应分析数据元素，以得出逻辑推断/结论，从而处理虚假数据的含义，并探索背景和后果。例如，在由 A.I .驱动的儿童保育机器人的应用中，由政策制定者、儿童保育和健康组织和机构等编程的声称“儿童不得受到伤害”的潜在主播。当然不会适用于所有场景。预计人工智能机器人应该能够理解决策者和儿童保育组织所做的背景和结论/假设；即，机器人不应该阻止儿童学习如何行走，因为他/她在绊倒和跌倒时会受到暂时的伤害。

因此，我认为，数据流的准确性(和其他特征)只是问题的一部分，当然不能想当然，这一理论强调了将“批判性思维”融入人工智能的重要性。解决人工智能可以识别数据的错误子集的方法至关重要:在鼓励包含贡献者的数据挖掘环境中，优化的机器学习算法如何区分人类和伪装成可靠数据源的黑客技术？在这种情况下，我们可以假设我们无法控制人工智能在虚假新闻检测或医疗保健等应用中基于环境参数的“锚”/经验法则的自我生成，在这些应用中，可能缺乏关于数据的可用性/完整性、来源的可靠性和准确性的知识，并且“虚假”数据的锚和易变定义会根据应用、上下文和特定需求而改变。由此可见，在人工智能的治理方面有一个相关的关切，这与将自学过程的功能方面留给人工智能的黑箱性质有关，以及一些事件，如微软的聊天机器人在与推特用户互动后变成种族主义、刻板的巨魔([https://www . independent . co . uk/life-style/gadgets-and-tech/news/ai-robots-artificial-intelligence-racism-predict-bias-language-learn-from-humans-a 7683161)](https://www.independent.co.uk/life-style/gadgets-and-tech/news/ai-robots-artificial-intelligence-racism-sexism-prejudice-bias-language-learn-from-humans-a7683161.html)

新兴的非常规数据源挑战了人工智能在复杂环境中的操作能力，并证明了人工智能必须能够比人类更有效地理解和避免更高和更低抽象层次的偏见以及我们相关的认知偏见。此外，这个问题似乎包括数据的特征(准确性、完整性等)。)以及同样重要的过滤/处理数据的任务。因此，随着我们继续开发越来越复杂的人工智能系统，能够挖掘和过滤大数据的各个维度——数量、速度、多样性、准确性和价值，我们应该仔细平衡人类参与自学过程的数量。

感谢您的阅读！

**引用注释:**一些例子摘自理查德·h·泰勒(Richard H. Thaler)和卡斯·r·桑斯坦(Cass R. Sunstein)的小说《轻推:改善关于健康、财富和幸福的决定》，以从心理学角度说明与人类思维模式相关的关键现实世界偏见/谬误。我已经将这些偏见的例子和它们的解决方案应用到人工智能技术中。

*原载于 2018 年 8 月 29 日*[*【demystifymachinelearning.wordpress.com*](https://demystifymachinelearning.wordpress.com/2018/08/29/an-invincible-a-i-enhancing-self-learning-processes/)*。*