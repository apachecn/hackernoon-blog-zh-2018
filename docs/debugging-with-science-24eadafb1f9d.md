# 用科学调试！

> 原文：<https://medium.com/hackernoon/debugging-with-science-24eadafb1f9d>

![](img/2307af0658c1392597c9cf1575a58f79.png)

关于你的 bug 的根本原因，你是在欺骗自己吗？

在我的上一家公司成立之初，我们有一个不为人知的错误，它会周期性地让一个大型流行网站的整个数据中心瘫痪。在已经成功处理了数十亿次交易的代码中，这个错误很难被发现，而且只发生在晚上。更糟糕的是，它偶尔会导致全面停电。我们知道我们必须在第一时间找到并解决问题，这样我们才不会失去一个重要的早期客户。

以下是我们如何找到漏洞并拯救公司的。

## 科学的方法

为了挽救我们与这位客户的机会，也可能是公司本身的机会，我们只有一次机会来修复这个 bug。这意味着没有猜测。我们需要一种严格的科学方法。

这种方法的第一部分是查看我们的证据:核心转储、回溯、日志消息、数据包数据等。，并制定了一个关于我们的错误来源的假设。这将是我们的*根本原因假设*。

科学中的一个关键概念是[可证伪性](https://en.wikipedia.org/wiki/Falsifiability)。对于某些类型的陈述，比如“所有的天鹅都是白色的”，一个假说的一个反证就足以推翻它。就这样，就一个。如果我看到一只黑天鹅，我需要重新设定我的假设。

对于我们的 bug，如果只有一个数据没有被我们的根本原因假设所解释，我们可能没有找到正确的罪魁祸首…或者至少不是所有的*罪魁祸首。这意味着更多的潜在停电。我们要么证明无法解释的数据是伪造的，要么改进我们的根本原因假说来解释它。*

![](img/a596056c2064dc8ffe2e2cc9eb2c1bae.png)

## 假设

我们的公司 LineRate Systems 构建了高性能的纯软件负载平衡器。在对这个受欢迎的网站进行概念验证的过程中，事情进展得非常顺利。

每天早上，客户的运营团队会将负载平衡器投入生产，并开始将其扩展到越来越多的数据中心，直到我们在数百台 web 服务器前以超过 10 Gb/s 的生产流量运行。令人兴奋的东西。我们最终与客户建立了足够的信心，是时候进行一些连续测试了，通宵运行。

这就是问题开始的时候。

一天晚上，所有负载平衡进程都卡住了，消耗了 100%的 CPU。重大停电、愤怒的客户和令人沮丧的挫折。

我们又回到了只有白天的测试。我们对系统进行了测试，以记录故障发生的时间，并添加了一个看门狗来转储违规进程的核心，以便我们可以分析故障。与此同时，启动了一个新的进程来保持系统运行。

![](img/dc92bad945019f6d32f1320857061add.png)

## 只在夜间活动的虫子

奇怪的是，在任何白天的测试中，我们都没有看到失败。所以，有了新的看门狗，我们又开始了夜间测试。(耐心和宽容的早期客户价值连城。)果然，我们开始看到进程在 100% CPU 挂起后被看门狗杀死。至少这一次，这仅仅导致了一些连接丢失，但是没有大的中断。

在收集了核心转储和数百 Gb 的数据包捕获之后(记住，在 10 Gb/s 的速度下，每秒钟可以获得大约 1 GB 的数据包数据)，我们开始了我们的分析。核心转储表明我们陷入了 HTTP 解析器内部的无限循环。然而，这段代码此时已经成功处理了数十亿个 HTTP 请求，所以问题是，“为什么现在失败了，为什么只在晚上失败？”

现在，艰苦的工作开始了。我们构建了一个工具来查找在某个时间窗口内没有响应的 HTTP 请求，因为这是一个进程可能已经挂起并且没有响应的线索。几个小时后，该工具开始吐出数据包序列…而且，它们看起来都一样。

找到了。我们希望找到确凿的证据。原来，每次负载平衡器无法响应传入的 HTTP 请求时，HTTP 头都相当大，并且有几十个额外的`NOKIA-`头。在通过解析器重放这些头之后，我们能够确定大的头是通过慢速网络链接进入的，因此很可能在正确的位置出现缓冲区中断，从而触发解析器错误，无法推进缓冲区指针。这导致解析器一遍又一遍地查看同一个字节，导致 100%的 CPU 使用率。

这成了我们的根本原因假说。

![](img/829bc60ed6fa59ebfc320a59d593bf4d.png)

## 该死的科学

这一切都很有希望，但我们无法解释为什么这种情况只发生在夜间，而不是白天。请记住，我们必须解释*所有*的数据，否则我们可能无法正确地解决问题，并且我们会以一个非常愤怒的客户和信誉的巨大损失而告终。

幸运的是，在与分配到我们项目的客户运营团队进行头脑风暴后，我们提出了一个令人满意的解释。所有失败的请求都来自连接到诺基亚基站的手机，我们可以从他们添加的 HTTP 报头膨胀中推断出来。此外，对于失败的情况，连接延迟和分组间延迟非常高。

在美国，很少有诺基亚基站，如果有的话。然而，早在 2011 年，亚洲的主要运营商都在使用诺基亚基站。快速检查违规 HTTP 请求的来源 IP 地址确实都来自亚洲，尤其是印度。

就在那里。如果所有的违规流量都来自印度，这就可以解释当时(以及今天)的互联网状况下的高延迟和可变的数据包到达间隔时间。这也解释了为什么我们只在晚上看到令人不快的请求——在印度是白天。

对我们的数据包分析产生的所有违规请求的时间戳的快速查看显示，它们发生在印度和东南亚的使用高峰期。并且时间也对应于 IP 地址位置估计。我们解释了所有的数据。科学！

## 结论

好消息是，如果你在运营一项由你的公司控制运营的服务，你也许可以不用解释所有深奥的数据。但是你应该小心，因为你的修复可能只会掩盖真正的问题，使它不太可能发生。因此，它会静静地潜伏着，等待着造成网站范围的中断。

在我们的情况下，我们别无选择。在公司早期，我们只有几次机会接近客户。我们的软件绝对是关键任务，我们无法控制部署时间表。

幸运的是，调试失败的好的科学方法和好的工程实践会带来好的结果。我们甚至在客户站点结束了产品，而客户忘记了他们已经在生产中安装了产品。当他们发现疏忽时，系统正常运行时间超过一年，没有监控、操作员干预或停机。哦，那是该产品的测试版。

这就是科学的力量！

*原载于*[](https://unbounded.systems/blog/debugging-with-science/)**。**