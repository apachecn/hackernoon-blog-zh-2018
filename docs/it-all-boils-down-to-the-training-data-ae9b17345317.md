# 这一切都归结于训练数据

> 原文：<https://medium.com/hackernoon/it-all-boils-down-to-the-training-data-ae9b17345317>

![](img/b35329bebe97877ff8ad153110f437e5.png)

ML Training Data Pipeline

你的模型表现不好吗？试着挖掘你的数据。与其通过搜索最先进的模型来提高性能，不如通过提高数据质量来大幅提高模型的准确性。

由于大多数数据科学家正在将现成的算法应用于特定的业务应用，因此数据科学家今天面临的最大挑战之一是创建一个持续的工作流，将高质量的训练数据一致地输入到他们的算法中。同时，您的模型正在学习，您希望能够利用这一智能模型来标记数据集的其余部分。构建基础设施来进行与模型集成的注释并管理工作流是机器学习中最具挑战性的部分。

**迭代= >准确度&一致性**

*垃圾进垃圾出*的公理可以在训练中被掩盖。即使输入随机噪声，如随机标签或无结构像素，某些模型也能够过度训练，达到 0%的训练误差([理解深度学习需要重新思考泛化](https://arxiv.org/pdf/1611.03530.pdf))。这是因为最近的高容量模型，如深度神经网络，甚至可以记忆海量数据集。虽然这些模型在训练过程中不会出错，但在测试时，它们的表现并不比随机猜测好。

因此，迭代和严格的 QA/QC 流程对于正确的数据标注工作流程至关重要。“质量评估方法可以分为三大类:(一)自动，(二)通过直接检查工作提供者，(三)使用人群本身作为评估者的方法”([使用聚合函数在众包平台中确定工人排名](https://upcommons.upc.edu/bitstream/handle/2117/26147/owa_wcci.pdf))。由于在大多数情况下，没有人工输入的自动评估要么是不可能的，要么只能保证最低的质量，因此我们将讨论如何实施后一类的 QA/QC 方法，以帮助提高对训练数据质量的信心。

1.  测试问题
2.  直接检查
3.  共识；一致

测试问题和直接检查是 QA/QC 方法，属于工作提供者或数据科学家直接负责评估质量的类别(ii)。测试问题是公司间的标准技术。它指的是由数据科学家正确标记的一组数据，然后在标记器之间随机分配，以测试其准确性。直接检查是目视检查标签数据以衡量准确性的过程。

![](img/9871d9718e65339ec493c518e5dbcb69.png)

视觉筛选是一项基本功能，每个人都应该有预处理数据和标签后审查的准确性。在文章中，*你为什么需要改进你的训练数据，以及如何做，*皮特·沃顿建议随机浏览你的数据。这个基本的实践可以揭示关于你的数据集的有价值的信息，例如“不同类别中不平衡的例子数量，损坏的数据(例如标有 JPG 文件扩展名的 png)，不正确的标签，或者只是令人惊讶的组合。”关于提高数据质量的更多实用技巧，请阅读他的文章[点击这里](https://petewarden.com/2018/05/28/why-you-need-to-improve-your-training-data-and-how-to-do-it/)。虽然大多数开源工具不提供这一基本功能，但 Labelbox 是一个标记数据的存储库，您可以在一个地方直观地浏览和管理您的数据。

虽然类别(ii)的 QA/QC 方法非常有用，但它们有两个固有的缺点。首先，它们本质上是不可扩展的，因为工作提供者或数据科学家评估众包标签准确性的资源是有限的。第二，为了执行这些方法，正确的答案必须是已知的。

另一方面，当不知道正确答案时，共识既有内在的可伸缩性，又很有用。共识要求多个不同的注释者为同一段数据提供标签。有了这些信息，consensus 计算并交集(IOU ),以平均出标签的特性，并获得更好的信号衰减。换句话说，对同一问题的答案进行比较，以确定一致率。高一致性表明数据集质量高，而低一致性通常表明数据质量差，但也可能表明实例不明确。Labelbox 提供了一个内置的共识工具，因此您可以实时监控您的质量指标。点击阅读更多关于标签箱共识工具如何工作的信息[。](https://support.labelbox.com/docs/how-consensus-works)

![](img/b88aa88c03048fbb953f3df959d3e76b.png)

Consensus

**边际收益递减**

谷歌发表的一项研究表明，即使当你认为你已经有足够的数据时，增加更多的数据也可以使你的模型表现得更好([数据的不合理有效性](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf))。然而，答案比越多越好更复杂。

要问的核心问题不是你是否有足够的数据，而是你是否达到了标记的边际成本超过模型性能的边际收益的有效边界。为了形象化这一点，绘制模型在一段时间内的性能图。例如，从 1000 个样本开始训练您的模型，并对 200 个样本进行评估，以测量您的起始准确度。然后再收集 1000 个样本，用第二套重复实验。该模型有望在 2000 个例子中做得更好，因为它正在学习看到数据中的自然变化，并过滤掉特质，同时更好地衰减信号。

**工作流程透明度**

通常的做法是使用标记服务，外包数据并获得标记数据作为回报。然而，如果你外包你的数据标签，但没有办法衡量标签服务的质量，你基本上是在拿你的投资赌博。

外包贴标服务可能是基本对象分类的一个很好的途径，比如给汽车或服装贴标签。如果你需要在一个特定的主题上产生一个大的标签任务组，有不同的业务流程外包(BPO)公司可以容纳特定的专业知识类别。通过[标签盒](https://labelbox.com/)，您可以与我们的合作伙伴 BPOs 联系，监控您外包的数据标签服务的质量，并在一个统一的平台上创建和管理您自己的工作流程。

**总结一下，清理一下**

你的模型和你的训练数据一样好。既然您已经知道如何确保您的训练数据足够一致、标记准确并且大小足够，那么就去清理它吧！

访问[www.labelbox.com](http://labelbox.com/)免费探索 Labelbox 或[与我们的团队成员之一](http://labelbox.com/enterprise)谈论适合您业务的企业解决方案。

*原载于 2018 年 11 月 12 日*[*medium.com*](/labelbox/it-all-boils-down-to-the-training-data-393376f24e6a)*。*