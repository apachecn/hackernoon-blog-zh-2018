# 深度学习的经验法则

> 原文：<https://medium.com/hackernoon/rules-of-thumb-for-deep-learning-5a3b6d4b0138>

凭借我几年培训和使用各种可用开源模型[网络](https://hackernoon.com/tagged/networks)的经验，我已经学会了设置各种超级参数并有效使用它们的艰难方法。我已经失去了我收集信息的来源，但是，这似乎对我很有用。所以，今天，我想分享我记得的那些。

没有一个可能在开箱后对你有效，但是，大多数时候对我有效。大多数观察结果不是我的直觉，而是从我经历的各种资源中学到的。

## **嵌入维度**

如果你试图为单词(任何东西)创建你自己的向量，嵌入的维度要稍微复杂一点，并且很难知道什么可行，什么不可行。这个特别起草的公式在大多数情况下对我有效。

> 要使用的嵌入维度:总单词的第 4 个根(你的内容的字典)

## **输出特征地图分辨率**

在图像**分类**场景中，如果你计划建立自己的网络，我对各种网络的观察表明，你最终特征地图的分辨率应该是

> 最终特征图的空间分辨率=原始图像分辨率的 1/34(图像分类)

同样的观察对**语义分割**有一个略有不同的公式变化

> 编码器输出特征图的空间分辨率=原始图像分辨率的 1/16(语义分割)

## **学习率**

最大的话题是[学习](https://hackernoon.com/tagged/learning)率，我没有一个固定的公式，但是，粗略的想法是从高开始，随着你的进一步发展而降低，如果你是从你自己随机初始化的权重开始，那么，使它变高并四处移动，如果权重是从预训练网络初始化的，那么保持它稍微低一些，并根据你的验证分数或围绕它的各种公式继续降低。

> 公式是，随着训练的进展，总是降低学习率

## **重量初始化**

> 总是用预先训练好的网络权重初始化(Imagenet 或任何其他东西)
> 从资源的角度来看，随机初始化权重的问题比任何人能处理的都要糟糕。

## **LSTM 健忘**

> LSTMS:将遗忘门偏差初始化为更高的值，如果不是，它只是作为你的输入的一个 s 形(当初始化的权重变小时)(现在大多数主要的库默认这样做)

## **辍学**

> 坚定的肯定(应该总是使用)

## **批量标准化:**

> 是啊！(论文上说，协变转移，我说，在训练阶段更好更快)

## **数据预处理**

> 零均值和单位方差(你看到的大多数预训练网络都使用这种方法，而且很有效)

## **小批量**

> 32–128 更好(大部分情况下，较大的批量可能不会产生更好的结果),即使较小的批量也足够好，但是，可能需要更长的时间来压缩

## **一群模特**

> 是啊！，并且通常比单一模型更好。根据问题的复杂程度做出决定。

我能想到的就只有这些了。当我了解更多的时候，我会继续更新。我再说一遍，这些只是一些技巧，对我有用，对你可能有用也可能没用。但是，这是大多数成功的预训练网络的格式。