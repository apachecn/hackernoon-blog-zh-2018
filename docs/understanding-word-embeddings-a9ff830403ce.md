# 理解单词嵌入

> 原文：<https://medium.com/hackernoon/understanding-word-embeddings-a9ff830403ce>

对于我们人类来说，理解不同的单词如何根据它们被使用的上下文相关联是一件容易的事情。如果我们以文章为例，那么我们会阅读大量不同主题的文章。在几乎所有的文章中，当作者试图教你一个新概念时，作者会试图用你已经知道的例子来教你任何新概念。类似地，计算机也需要一种方法来学习一个主题，并且理解不同的单词之间的关系。

让我从语言的概念开始，这些令人惊奇的不同语言。我们用它来互相交流，分享不同的想法。
但是我们如何更好地解释一种语言呢？一段时间以前，我在读《智人》这本书，作者回顾并质疑了很多事情，在这本书里，他对语言的解释很好:

> 我们可以将有限数量的声音和符号连接起来，产生无限数量的句子，每个句子都有不同的含义。因此，我们可以摄取、储存和交流大量关于周围世界的信息。

因为语言，这一切都是可能的。正如他所写的，我们可以产生无限多的句子，这意味着单词将根据使用的上下文有不同的含义。因此，我们不是对每个单词都有固定的定义，而是根据它们的上下文来定义它们。这就是为什么在编程中我们不使用 WordNet 来找出不同单词之间的关系。WordNet 是一个图书馆，在这里你可以找到每个单词的同义词集合。此外，它还包含上位词(“is a”)关系。它忽略了细微差别，例如单词“good”有许多同义词，但上下文却没有。WordNet 的另一个问题是，它需要人工来创建和更新。

我们需要一种方法来比较单词的相似性。为了进行比较，我们需要用向量的形式来表示单词。我们可以用离散符号的形式来表示它们。将它们表示为一个热点向量，例如:

```
hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]
```

因此，向量的维数将等于词汇表中的单词总数。尽管如此，我们无法通过这种方式找到不同单词之间的相似之处，因为每个单词都有不同的表示方式，也没有办法进行比较。解决这个问题的一个方法是使用 WordNet 的同义词列表来获得相似性，或者如果我们可以在向量本身中添加相似性呢？

这里的想法是为每个单词创建一个密集向量，选择的方式是它与出现在相似上下文中的单词向量相似。Word2Vec 就是我们要用来创建这样一个密集向量的东西。

## Word2Vec

假设我们正在阅读一本书，我们的任务是学习不同的单词是如何相互联系的。我们在这里要做的是，我们将拿出一个段落，并仔细阅读每个单词。一次一个词，让我们称它为中心词。然后我们有邻近词，假设一个句子中有五个词，中心词是中间的，那么左边的两个词和右边的两个词是邻近词。这些也称为上下文单词。这里我们取两个，因为我们有一个大小为 2 的窗口，它是可以改变的。

这里我们将计算给定中心词的上下文词的概率，反之亦然。基于这个概率，我们将调整单词向量，并尝试最大化概率。这样我们将得到单词向量，通过它我们将知道任何两个单词是否相似。

Word2Vec 是由 Mikolov 等人于 2013 年在谷歌推出的，旨在实现单词的高效矢量表示。这是论文的摘要:

> 我们提出了两种新的模型架构，用于从非常大的数据集计算单词的连续向量表示。在单词相似性任务中测量这些表示的质量，并且将结果与先前基于不同类型的神经网络的最佳执行技术进行比较。我们观察到，在低得多的计算成本下，准确性有了很大的提高，也就是说，从 16 亿个单词的数据集学习高质量的单词向量只需要不到一天的时间。此外，我们表明，这些向量在我们的测试集上为测量句法和语义单词相似性提供了最先进的性能。

## 模型

让我们进入细节。我们将创建一个只有一个隐藏层的神经网络。在输入中，我们将传递一个代表单词的热向量，在输出中，我们将得到该单词靠近中心单词的概率。这是一种神经网络，其实际目标不是我们得到输出，在这种情况下，我们感兴趣的是从隐藏层的权重中学习。需要输出层来降低损耗。

![](img/67106ca043049f22629690b809ec15f3.png)

Figure 1: Skip-gram predicts surrounding words given the current word.

从图 1 中，我们可以得到跳格模型中不同层次的基本概念。因此，有两种架构被提出:一种是跳格模型，另一种是连续词袋模型。在 CBOW 的情况下，我们基于上下文单词预测中心单词，而在 Skip-gram 的情况下，情况相反。此外，它被称为词袋模型，因为词在历史中的顺序不会影响投影。在本帖中，我们将讨论跳格模型。

用数学术语来说，它的目标是最大化平均对数概率。假设训练单词的数量是 T，那么这是平均对数概率的表示:

![](img/86917249b41b90aae9241772b3f90b84.png)![](img/075700288837bec3acc8b1f912b55519.png)![](img/8d74e35cd54498c582645fcdfab23b3c.png)![](img/bf54feb1814495e51b9239bf96c43e8e.png)

## 隐蔽层

![](img/a163c53cc7a5a05e5a7375f9e9ab206f.png)

在上面的神经网络架构中，您可以看到我们有三层，在这一层中，我们已经知道了输入和输出层。让我们讨论一下上面架构中的隐藏层。

这个神经网络中的隐藏层充当查找表。因此，假设我们有一个 1000 行的热点向量作为输入，隐藏层包含 30 个特征，这意味着一个 1000 行 30 列的矩阵。当我们将它们相乘时，输出将是对应于 1 的矩阵行。

此外，隐藏层中的功能可以被视为超参数，您可以在您的应用程序中调整它。没有限制，可以尝试不同的值。

隐藏层的输出就是我们所说的单词向量。我们根据输出层的结果更新它们。

## 开源项目

如果你想使用一些预建的工具来创建单词表示，那么脸书的 [fastText](https://github.com/facebookresearch/fastText) 就是其中之一。
和[这个](https://code.google.com/archive/p/word2vec/)是谷歌对 Word2Vec 的实现，他们使用谷歌新闻数据来训练模型。

喜欢这个故事吗？点击那个拍手按钮，跟着我上 [*中*](/@MukeshThawani) *。感谢阅读！本文原载于* [*Applozic 博客*](https://www.applozic.com/blog/understanding-word-embeddings-tech-talks-at-applozic/) *。*